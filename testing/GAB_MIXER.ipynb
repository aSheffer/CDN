{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import retriever\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset_file = '../data/training/order_train_data.bin'\n",
    "testset_file = '../data/training/order_test_data.bin'\n",
    "vocab_file =  '../data/metadata/w2v_vocab.json'\n",
    "params_dir_tmp = '../data/training/models/All/'\n",
    "embed_path =  '../data/metadata/w2v.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Each entry in this list has the following structure:</h3>\n",
    "<ul>\n",
    "<li>entry[0]: query indexes </li>\n",
    "<li>entry[1:n]: n items where each item is [bounding box vector, bounding box spaital features]. Note that different enteries might have different 'n' </li>\n",
    "<li>entry[n+1]: integer, entry[ 1 + entry[n+1]] is the ture bbox </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset barches #: 297\n",
      "test barches #: 297\n",
      "val barches #: 149\n"
     ]
    }
   ],
   "source": [
    "trainset = np.load(open(trainset_file, 'rb'))\n",
    "trainset = [item for item in trainset if len(item)>2 and len(item[0])>0]\n",
    "print('trainset barches #:', len(trainset))\n",
    "\n",
    "test_N_val_set = np.load(open(testset_file, 'rb'))\n",
    "test_N_val_set = [item for item in test_N_val_set if len(item)>2 and len(item[0])>0]\n",
    "\n",
    "np.random.shuffle(test_N_val_set)\n",
    "np.random.shuffle(test_N_val_set)\n",
    "\n",
    "testset = test_N_val_set[:int(0.5*len(test_N_val_set))]\n",
    "valset = test_N_val_set[int(0.5*len(test_N_val_set)):]\n",
    "\n",
    "print('test barches #:', len(test_N_val_set))\n",
    "print('val barches #:', len(valset))\n",
    "\n",
    "with open(vocab_file, 'r') as f:\n",
    "    vocab = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab['<unk>'] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8242, 8241)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), vocab['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_vecs = np.load(open(embed_path, 'rb')).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> augment_data function </h3>\n",
    "<br>We try sevral regularization methods. One of the things I've tried is to add data points where for each data I pick a query from a random data point and a set of bbox from a different random point. We build the labels (bboxes) distribution by giving an equal probability to each label. <br><br>\n",
    "The augment_data function does just that but the label of each added poind is writen as -1*number of bboxes. When we build the batch it self (function build_data in class Model), if we see a negative label index, we know its an added point and we know the number of bboxes so we can build the correct distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_data(data, ratio=0.5, addNoise=False):\n",
    "        '''\n",
    "        The function add data points. \n",
    "        We pick a query from a random data point,\n",
    "        and a set of bbox from a different random point and we join them\n",
    "        to build a new data point. The label distribution of the new data point will \n",
    "        uniform, that is, all labels will have equal probability. \n",
    "        \n",
    "        \n",
    "        Params:\n",
    "            data: a list of data entries\n",
    "                                                \n",
    "        Returns: a list of augmented data\n",
    "            \n",
    "                        \n",
    "        '''\n",
    "                          \n",
    "        q_idx = np.random.choice(range(len(data)), int(len(data)*ratio), replace=False)\n",
    "        im_idx = np.random.choice([i for i in range(len(data)) if i not in q_idx], int(len(data)*ratio))\n",
    "        for i in range(len(q_idx)):\n",
    "            q, im = data[q_idx[i]][0], data[im_idx[i]][1:-1]\n",
    "            item = [q]\n",
    "            for im_tmp in im:\n",
    "                item.append(im_tmp)\n",
    "            item.append(-len(im))\n",
    "            data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stats(test, train, ephocs=100, title=None, params=[50, 100, 150, 200]):\n",
    "    '''\n",
    "    Plot metrics graphs and print some stats.\n",
    "    \n",
    "    Params:\n",
    "        test: list. \n",
    "              Each item is a tuple, [test accuracy, test IOU, test loss]\n",
    "        \n",
    "        train: list. \n",
    "               Each item is a tuple, [train accuracy, train IOU, train loss, 0]\n",
    "               For now we can ignore the last part in the tuple (zero)\n",
    "               \n",
    "        params: The hyper-parameters to iterate over, defult to number of rnn's hidden units.\n",
    "    '''\n",
    "    \n",
    "    ephocs = range(ephocs)\n",
    "    test_res = np.array(test)\n",
    "    train_res = np.array(train)\n",
    "    test_Glabels = ['test accuracy', 'test IOU', 'test loss']\n",
    "    train_Glabels = ['train accuracy', 'train IOU', 'train loss']\n",
    "\n",
    "    for j, param in enumerate(params):\n",
    "        print('num_hidden:', param)\n",
    "        print('='*(len('num_hidden:')+3))\n",
    "        for i in range(len(train_Glabels)):\n",
    "            plt.plot(ephocs, test_res[j][:,i])\n",
    "            plt.plot(ephocs, train_res[j][:,i])\n",
    "            plt.legend([test_Glabels[i], train_Glabels[i]], loc='upper left')\n",
    "            if title is not None:\n",
    "                plt.title('%s: %d'%(title, param))\n",
    "            plt.show()\n",
    "\n",
    "            metric = ''.join(train_Glabels[i][len('train')+1:])\n",
    "            if metric=='loss':\n",
    "                print('Train min %s:%.3f'%(metric, min(train_res[j][:,i])))\n",
    "                print('Test min %s:%.3f'%(metric, min(test_res[j][:,i])))\n",
    "            else:\n",
    "                print('Train max %s:%.3f'%(metric, max(train_res[j][:,i])))\n",
    "                print('Test max %s:%.3f'%(metric, max(test_res[j][:,i])))\n",
    "        print('-'*100,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALSTM\n",
    "\n",
    "This RNN cell has two LSTM cells, Bcell (for player B) and Acell (for Player A) and work as follow:\n",
    "<ol> \n",
    "<li>We run B's cells with the true query input word, getting the un-edited state. If use_worsAttn=True we add attention on the images vectors for each word.</li>\n",
    "<li>We run B's cells with the edited input word - 'unk', getting the edited state. If use_worsAttn=True we add attention on the images vectors for each word.</li>\n",
    "<li>We feed the un-edited state to A's cell.</li>\n",
    "<li>We run A's cells. A's input are:\n",
    "<ul><li>B's un-edited state</li><li>The reward for editing a word</li><li>B's loss having no edited words.</li></ul></li>\n",
    "<li>A's output is then goes throw a transformation which yields two values, one for editing a word and another for not.<br>If the the value for edit the word is higher, we pass B's edited state to the next time step, else we pass B's un_edit state.</li><br>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ALSTM(tf.nn.rnn_cell.LSTMCell):\n",
    "    def __init__(self, \n",
    "                 batch_size, \n",
    "                 num_units, \n",
    "                 \n",
    "                 # Size of A's attention vector.\n",
    "                 words_attn_dim, \n",
    "                 # B's outputs (for each time step).\n",
    "                 words_attn_states, \n",
    "                 # Inicates whether attentionvec is a padding (0) or not (1).\n",
    "                 words_attn_idx, \n",
    "                 \n",
    "                 # Size of B's attention vector ([image vector, spital features] size) .\n",
    "                 img_attn_dim, \n",
    "                 # B's outputs (for each time step).\n",
    "                 img_attn_states, \n",
    "                 \n",
    "                 \n",
    "                 # Inicates whether attentionvec is a padding (0) or not (1).\n",
    "                 img_attn_idx, \n",
    "                 \n",
    "                 unk, #'unk' word vector\n",
    "                 \n",
    "                 # Whehter to edit the query or not.\n",
    "                 isEdit, \n",
    "                 \n",
    "                 # Whether B uses words levlel attention or not.\n",
    "                 use_wordAttn,\n",
    "                 \n",
    "                 # Probabilty for choosing an action (edit or not) randomly, \n",
    "                 editRandomlyProb, \n",
    "                 # If the action is chosen randomly. edit with probability of rnn_editProb\n",
    "                 rnn_editProb,\n",
    "                 \n",
    "                 # Dropin and dropout ratio.\n",
    "                 dropout_in, \n",
    "                 dropout_out,\n",
    "                 \n",
    "                 # qeuries_length[i]=length of the i'th query in the batch\n",
    "                 qeuries_length,\n",
    "                 \n",
    "                 # If (qeuries_length[i]-1)-current_timestamp>delta\n",
    "                 # don't edit.\n",
    "                 delta,\n",
    "                 \n",
    "                 # If True add noise instead of using 'unk'\n",
    "                 editByNoise=False,\n",
    "                 \n",
    "                 # If useNoise is true word vec = alpha*word_vector+(1-alpha)*noise\n",
    "                 useNoise=False,\n",
    "                 alpha=.3,\n",
    "                 \n",
    "                 #this holds A's rewards and B's losses to\n",
    "                 # be add to A's feature vectors.\n",
    "                 reward_loss=None,\n",
    "                 state_is_tuple=True\n",
    "                ):\n",
    "        \n",
    "        \n",
    "        # When useing A, the cell state will contain the concatenation \n",
    "        # of both B and A states. Therefore we set the unit number to be\n",
    "        # 2*(A and B unit size).\n",
    "        super().__init__(2*num_units+1, state_is_tuple=state_is_tuple)\n",
    "    \n",
    "        self.words_attn_states = words_attn_states\n",
    "        self.words_attn_idx = words_attn_idx\n",
    "        self.words_attn_dim = words_attn_dim\n",
    "        \n",
    "        self.img_attn_states = img_attn_states\n",
    "        self.img_attn_idx = img_attn_idx\n",
    "        self.img_attn_dim = img_attn_dim\n",
    "        \n",
    "        self.num_units = num_units\n",
    "        self.batch_size = batch_size\n",
    "        self.unk = unk \n",
    "        self.isEdit = isEdit\n",
    "        self.use_wordAttn=use_wordAttn\n",
    "        \n",
    "        self.Bcell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.LSTMCell(self.num_units, state_is_tuple=True), input_keep_prob=dropout_in, output_keep_prob=dropout_out)\n",
    "        \n",
    "        self.Acell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                tf.nn.rnn_cell.LSTMCell(self.num_units, state_is_tuple=True), output_keep_prob=dropout_out)\n",
    "        \n",
    "        \n",
    "        self.editRandomlyProb = editRandomlyProb\n",
    "        self.rnn_editProb = rnn_editProb\n",
    "        self.reward_loss = reward_loss\n",
    "        self.useNoise=useNoise\n",
    "        self.alpha=alpha\n",
    "        self.qeuries_length=qeuries_length\n",
    "        self.delta=delta\n",
    "        \n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        '''\n",
    "        Params:\n",
    "            inputs: word embadding.\n",
    "            state:  [B's state form privious state, A's state form privious state]\n",
    "        '''\n",
    "        # takse B's state from state[:self.num_units]\n",
    "        Bstate_c = tf.slice(state[0], [0, 0], [-1, self.num_units])\n",
    "        Bstate_h = tf.slice(state[1], [0, 0], [-1, self.num_units])\n",
    "        self.Bstate =  tf.nn.rnn_cell.LSTMStateTuple(c=Bstate_c, h=Bstate_h)\n",
    "        \n",
    "        # state[2*self.num_units] will hold the current time step\n",
    "        self.time_step = tf.slice(state[0], [0, 2*self.num_units], [-1, 1])\n",
    "        # If timestep_delta[i]=0, don't edit the i'th word in the batch\n",
    "        self.timestep_delta = tf.cast(tf.less_equal(\n",
    "                tf.expand_dims(self.qeuries_length-1, 1)-tf.cast(self.time_step, tf.int32), self.delta), tf.float32)\n",
    "        \n",
    "         \n",
    "        # If B's cell uses attention\n",
    "        if self.use_wordAttn:\n",
    "            words_attn = self.attention(Bstate_h, self.img_attn_states, self.img_attn_dim, self.img_attn_idx)\n",
    "            new_input = tf.concat([inputs, words_attn], -1)\n",
    "            Boutputs, Bnew_state =  self.Bcell(new_input, self.Bstate, 'Bcell')\n",
    "        else:\n",
    "            Boutputs, Bnew_state =  self.Bcell(inputs, self.Bstate, 'Bcell')\n",
    "\n",
    "         \n",
    "        def f1(): # If isEdit==True\n",
    "            # takse A's state from state[self.num_units: 2*self.num_units]\n",
    "            Astate_c = tf.slice(state[0], [0, self.num_units], [-1, self.num_units])\n",
    "            Astate_h = tf.slice(state[1], [0, self.num_units], [-1, self.num_units])\n",
    "            self.Astate =  tf.nn.rnn_cell.LSTMStateTuple(c=Astate_c, h=Astate_h)\n",
    "            \n",
    "            if self.useNoise: # just add noise to the edited words\n",
    "                new_unk_vecs = self.alpha*inputs + (1-alpha)*tf.random_normal(shape=inputs.get_shape(), stddev=0.1)\n",
    "            else: # change the edited words by 'unk'\n",
    "                unk_vecs = tf.concat([self.unk for _ in range(self.batch_size)], 0) # shape: self.batch_size x 1 x embed_size\n",
    "                new_unk_vecs = tf.squeeze(unk_vecs) # shape: self.batch_size x embed_size\n",
    "            \n",
    "            # run B's cell with unk_batch\n",
    "            if self.use_wordAttn:\n",
    "                new_unk = tf.concat([new_unk_vecs, words_attn], -1)\n",
    "            else:\n",
    "                new_unk = new_unk_vecs\n",
    "            edit_output, edit_new_state = self.Bcell(new_unk, self.Bstate, 'Bcell')  \n",
    "            out1, state1 = self.runCell(Boutputs, Bnew_state, edit_new_state)\n",
    "            return out1, state1\n",
    "        \n",
    "        def f2(): \n",
    "            outs = tf.concat([Boutputs, tf.zeros((self.batch_size, self.num_units+1))], 1)\n",
    "            new_state_c = tf.concat([Bnew_state[0], tf.zeros_like(Bnew_state[0]), self.time_step+1], 1)\n",
    "            new_state_h = tf.concat([Bnew_state[1], tf.zeros_like(Bnew_state[1]), self.time_step+1], 1)\n",
    "            return outs, tf.nn.rnn_cell.LSTMStateTuple(c=new_state_c, h=new_state_h)\n",
    "        \n",
    "        new_output, new_state = tf.cond(self.isEdit, f1, f2)\n",
    "        \n",
    "        return new_output, new_state\n",
    "    \n",
    "    def runCell(self, Boutputs, Bnew_state, edit_new_state):\n",
    "        '''\n",
    "        Run B's cell after editing.\n",
    "        \n",
    "        params:\n",
    "            Boutputs: output vector without editing.\n",
    "            Bnew_state: state vector without editing.\n",
    "            edit_new_state: state vector after editing the input word to 'unk'. \n",
    "        '''\n",
    "        \n",
    "        with tf.variable_scope('runcell'):\n",
    "            # get action values according to B's hidden state\n",
    "            Aout, Anew_state, actions_vals = self.action_vals(Boutputs) \n",
    "            \n",
    "            # We can't use a tf.cond with batch_size  conditions so....\n",
    "            \n",
    "            # A choose to edit a word if actions_vals[0]<actions_vals[1].\n",
    "            # Note: if we edit the word cond=1, else cond=0.\n",
    "            a1, a2 = tf.split(value=actions_vals, num_or_size_splits=2, axis=1)\n",
    "            A_choice = tf.cast(tf.less(a1, a2), tf.float32)\n",
    "            \n",
    "            # If action is chosen randomly, edit word with probability self.rnn_editProb\n",
    "            # Note: if we edit the word cond=1, else cond=0.\n",
    "            rand = tf.multinomial(tf.log([[self.rnn_editProb, 1-self.rnn_editProb]]), self.batch_size)\n",
    "            rand_choice = tf.cast(tf.less(tf.transpose(rand), 1), tf.float32) \n",
    "            \n",
    "            # Choose whether to edit a word randomly with probability of editRandomlyProb\n",
    "            editRandomly = tf.multinomial(tf.log([[self.editRandomlyProb, 1-self.editRandomlyProb]]), self.batch_size)\n",
    "            editRandomly_choice = tf.cast(tf.less(tf.transpose(editRandomly), 1), tf.float32)\n",
    "            \n",
    "            # A list of decisions for each word in the batch. actions_idx[i] = 1->edit word in query i (at this time step), 0->do not edit.\n",
    "            actions_idx = self.timestep_delta*(editRandomly_choice*rand_choice + (1-editRandomly_choice)*A_choice)\n",
    "            \n",
    "            # We'd like to know the action values and decision for each word,\n",
    "            # therefore theses info are placed on the first 3 dimensions of the \n",
    "            # output vector. Note that this vector is not passed to the \n",
    "            # next tiee step so it won't affect the model. \n",
    "            outs = tf.concat([actions_vals,  actions_idx], 1)\n",
    "\n",
    "            # B's i state is replaced by the edited state if actions_idx[i]=1 (i =1, 2, ..., batch_size)\n",
    "            new_edit_state_c = (1-actions_idx)*Bnew_state[0] + actions_idx*edit_new_state[0]\n",
    "            new_edit_state_h = (1-actions_idx)*Bnew_state[1] + actions_idx*edit_new_state[1]\n",
    "\n",
    "\n",
    "            new_outs = tf.concat([outs, tf.zeros((self.batch_size, 2*self.num_units-2))], 1)\n",
    "            new_state_c = tf.concat([new_edit_state_c, Anew_state[0], self.time_step+1], 1)\n",
    "            new_state_h = tf.concat([new_edit_state_h, Anew_state[1], self.time_step+1], 1)\n",
    "            return new_outs, tf.nn.rnn_cell.LSTMStateTuple(c=new_state_c, h=new_state_h)\n",
    "    \n",
    "    \n",
    "    def action_vals(self, Boutputs):\n",
    "        '''\n",
    "        Get values for editing/not editing the input word.\n",
    "        \n",
    "        Params:\n",
    "            Boutputs: output vector without editing.\n",
    "            \n",
    "        Returns vals where:\n",
    "            Aout: A's cell output\n",
    "            Anew_state: A's cell state\n",
    "            vals: Tensor where vals[0] is the value for not editing the word \n",
    "                    and vals[1] is the value for editing the word.\n",
    "        '''\n",
    "        \n",
    "        with tf.variable_scope('action_vals') as scope:\n",
    "            Aattn = self.attention(self.Astate[1], self.words_attn_states, self.words_attn_dim, self.words_attn_idx)\n",
    "            \n",
    "            # A's input: [input, B's output, attntion state, reward, B's loss with no edits]\n",
    "            Anew_inputs = tf.concat([Boutputs, Aattn, self.reward_loss], 1)\n",
    "            \n",
    "            Aout, Anew_state = self.Acell(Anew_inputs, self.Astate, 'Acell')\n",
    "            vals = tf.nn.relu(self.linear(Aout, 2))\n",
    "\n",
    "            # Save the variables that only A uses. \n",
    "            # These variables will be trained separately from B's model.\n",
    "            self.Avars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)\n",
    "            \n",
    "            return Aout, Anew_state, vals\n",
    "            \n",
    "        \n",
    "    def linear(self, inputs, output_dim, scope='linear', bias=True, reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=False):\n",
    "            W = tf.get_variable('W', initializer=tf.random_uniform_initializer(maxval=1., minval=-1.),\n",
    "                                shape=(inputs.get_shape()[-1], output_dim))\n",
    "            if bias:\n",
    "                b = tf.get_variable('b', initializer=tf.constant_initializer(0.1),\n",
    "                               shape=[1, output_dim])\n",
    "                return tf.matmul(inputs, W) + b\n",
    "\n",
    "        return tf.matmul(inputs, W)\n",
    "    \n",
    "    \n",
    "    def attention(self, state, attn_states, attn_dim, attn_idx, relu=False):\n",
    "        '''\n",
    "        Attention mechanism (see https://arxiv.org/pdf/1409.0473.pdf)\n",
    "        \n",
    "        state: State from previous time step.\n",
    "        attn_states: Attetntion states. \n",
    "                     Tensor of shape (batch_size x max([len(attention_vectors[i]) for i in range(bach_size)]) x attn_dim)\n",
    "        attn_dim: Attention vector size.\n",
    "        attn_idx,: Tensor used for masking of shape (batch_size x max([len(attention_vectors[i]) for i in range(bach_size)]). \n",
    "                   attn_idx[i, j]=1 if the j's attention vcctior of sample i  is not padding, else its equat to 0.\n",
    "        '''\n",
    "        \n",
    "        self.attn_length = tf.shape(attn_states)[1]  \n",
    "        \n",
    "        # Computing... hidden_attn = W*v_att (use tf.nn.conv2d for efficiency)\n",
    "        attn_vecs = tf.reshape(attn_states, [self.batch_size, self.attn_length, 1, attn_dim])\n",
    "        W = tf.get_variable(\"attn_W\", [1, 1, attn_dim, self.num_units])\n",
    "        hidden_attn = tf.nn.conv2d(attn_vecs, W, [1, 1, 1, 1], \"SAME\")\n",
    "\n",
    "        # Computing... hidden_s = U*v_state\n",
    "        hidden_s = tf.reshape(\n",
    "            self.linear(\n",
    "                tf.cast(state, tf.float32), output_dim=self.num_units, scope='hidden_s_linear'), [-1, 1, 1,  self.num_units], name='hidden_s')\n",
    "\n",
    "        # Computing alpha\n",
    "        v = tf.get_variable(\"attn_v\", [self.num_units])\n",
    "        if relu:\n",
    "            logits = tf.reduce_sum(v * tf.nn.relu(hidden_attn + hidden_s), [2, 3])\n",
    "        else:\n",
    "            logits = tf.reduce_sum(v * tf.nn.tanh(hidden_attn + hidden_s), [2, 3])\n",
    "\n",
    "        # Masked softmax\n",
    "        max_logits = tf.reduce_max(logits, axis=-1)\n",
    "        masked_logits = tf.exp(logits-tf.expand_dims(max_logits, axis=1))*attn_idx\n",
    "        alpha = masked_logits/tf.reduce_sum(masked_logits, axis=-1, keep_dims=True)\n",
    "\n",
    "        a = tf.reduce_sum(tf.reshape(alpha, [-1, self.attn_length, 1, 1]) * attn_vecs, [1, 2])\n",
    "        b = tf.contrib.layers.fully_connected(a, num_outputs=self.num_units)\n",
    "                                               \n",
    "\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "When A joins the game, each iteration is completed via three steps:\n",
    "<ul>\n",
    "<li>We feed the query as it is to B and run it alone (no optimization nor training is done in this step)</li>\n",
    "<li>We run A and B together (no optimization nor training is done in this step). At this step we get A's decition and B's loss on the edited query. We use B's loss to calculate the Bellman's value for each time step. </li>\n",
    "<li>We again run A and B together, since now we know the real values for each time step and the real action for each time step (these will be the same as in the previous step since we did not train optimize the parameters yet), we finaly train the model</li>\n",
    "</ul>\n",
    "<br>\n",
    "But first we check the model's performance with out A. Class Model has a set of conditioning variables that set a different ragularization methods in the model. We start by checking them with out A's interference. We can also make the model RNN become bidirectional (by setting useBidirectionalRnn to True) and use words level attention (by setting use_wordAttn to True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self,\n",
    "                 batch_size, \n",
    "                 num_hidden, \n",
    "                 \n",
    "                 #Image's vector size.\n",
    "                 img_dims, \n",
    "                 \n",
    "                 #Spaital features length.tra\n",
    "                 bbox_dims, \n",
    "                 vocab, \n",
    "                 \n",
    "                 # For learning rate exponential decay\n",
    "                 decay_steps, \n",
    "                 decay_rate, \n",
    "                 \n",
    "                 # whether to use bach normaliztion for the last attention layer\n",
    "                 bnorm,\n",
    "                 \n",
    "                 embed_size=embed_vecs.shape[1],\n",
    "                 \n",
    "                 # Whether B uses words levlel attention or not.\n",
    "                 use_wordAttn=False,\n",
    "                 \n",
    "                 # Whther to use bidirectional rnn\n",
    "                 useBidirectionalRnn=False,\n",
    "                 \n",
    "                 # Urnn_norm: Whether to use batch normalization for the queries.\n",
    "                 # Uatt_norm: Whether to use batch normalization for the VGG outputs.\n",
    "                 Urnn_norm=True, \n",
    "                 Uatt_norm=True,\n",
    "                 \n",
    "                 # Whether to scale and normelize queries \n",
    "                 # embedding to have zero mean and QSTD std\n",
    "                 toQscale = False,\n",
    "                 Qstd = 0.1\n",
    "                ):\n",
    "#         with tf.device('/cpu:0'):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.img_dims = img_dims\n",
    "        self.bbox_dims = bbox_dims \n",
    "        self.num_hidden = num_hidden\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab = vocab\n",
    "        self.toQscale=toQscale\n",
    "        self.Qstd=Qstd\n",
    "        \n",
    "        # length of the logest query (all queries will be padded to this length)\n",
    "        self.qlen = tf.placeholder(tf.int32, name='qlen_holder')\n",
    "        self.queries = tf.placeholder(tf.int32, [None, None], name='queries')\n",
    "        self.img  = tf.placeholder(tf.float32, [None, None, self.img_dims], name='img')# VGG output vectors\n",
    "        self.bboxes = tf.placeholder(tf.float32, [None, None, self.bbox_dims], name='bboxes')# spatial bbox's features.\n",
    "\n",
    "        # attn_idx: inicates whether attention box is a dummy (0) or not (1).\n",
    "        self.attn_idx = tf.placeholder(tf.float32, [None, None], name='attn_idx')\n",
    "\n",
    "        self.labels = tf.placeholder(tf.float32, [None, None], name='labels')\n",
    "        self.isEdit = tf.placeholder(tf.bool, name='isEdit') # whehter to edit the query or not.\n",
    "        \n",
    "        # Probabilty for choosing an action (edit or not) randomly, \n",
    "        self.editRandomlyProb = tf.placeholder(tf.float32, name='editRandomlyProb_holder')\n",
    "        # If the action is chosen randomly. edit with probability of rnn_editProb\n",
    "        self.rnn_editProb = tf.placeholder(tf.float32, name='rnn_editProb_holder')\n",
    "\n",
    "        # After we run B with no edited words \n",
    "        # this holds B cross enthropy loss per query, de\n",
    "        # and  A's rewards to be add to A's feature vectors.\n",
    "        self.BCE_holder = tf.placeholder(tf.float32, [None,1], name='BCE_holder')\n",
    "        self.reward_holder = tf.placeholder(tf.float32, [None,1], name='rewards_holder')\n",
    "        \n",
    "        # If (qeuries_length[i]-1)-current_timestamp>delta\n",
    "        # don't edit.\n",
    "        self.delta = tf.placeholder(tf.int32, name='delta_holder')\n",
    "\n",
    "        # Dropout ratio for rnn's inputs and outpouts\n",
    "        self.dropout_in = tf.placeholder(tf.float32, name='dropoutIn_holder')\n",
    "        self.dropout_out = tf.placeholder(tf.float32, name='dropoutOut_holder')\n",
    "\n",
    "        # Dropout ratio for attention vector (for the final attention layer before the loss function)\n",
    "        self.dropout_img = tf.placeholder(tf.float32, name='dropoutImg_holder')\n",
    "        # Dropout ratio for query vector (for the final attention layer before the loss function)\n",
    "        self.dropout_q = tf.placeholder(tf.float32, name='dropoutImg_holder')\n",
    "        \n",
    "        # A and B learning rates.\n",
    "        self.Alr = tf.placeholder(tf.float32, name='Alr_holder')\n",
    "        self.Blr = tf.placeholder(tf.float32, name='Blr_holder')\n",
    "\n",
    "        # B outputs vectors (with no words edits), These are A's attention vectors\n",
    "        # which it uses to decide whter to edit a word.\n",
    "        self.Aattn_vecs = tf.placeholder(tf.float32, [None, None, None], name='Aattn_vecs_holder')    \n",
    "        self.unk = tf.constant([[vocab['<unk>']]], tf.int32)\n",
    "\n",
    "        self.isTrain = tf.placeholder(tf.bool, name='isTrain_holder') \n",
    "        self.queries_lens = self.length(self.queries) # list of all the lengths  of the batch's queriey \n",
    "\n",
    "        # Concatinate images vectors and their spaital features. \n",
    "        # These vectors wlll be used for attenionn when \n",
    "        # we calculate the loss function.\n",
    "        attn_vecs = tf.concat([self.img, self.bboxes], 2) \n",
    "        voc_size = len(self.vocab)\n",
    "\n",
    "        # Load pre-trained word imaddings.\n",
    "        # w2v_embed is not trainable.\n",
    "        with tf.variable_scope('w2v'):\n",
    "            w2v_embed = tf.get_variable('w2v_embed', initializer=embed_vecs, trainable=False)\n",
    "            w2v_queries = tf.nn.embedding_lookup(w2v_embed, self.queries, name='w2v_queries')\n",
    "\n",
    "        with tf.variable_scope('embed'):\n",
    "            embed = tf.get_variable('embed', shape=[voc_size, self.embed_size], \n",
    "                                    initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "            embed_queries_tmp = tf.nn.embedding_lookup(embed, self.queries, name='embed_queries')\n",
    "\n",
    "        embed_queries = embed_queries_tmp+w2v_queries\n",
    "\n",
    "        with tf.variable_scope('rnn'):\n",
    "            Aattn_idx = tf.cast(tf.abs(tf.sign(self.queries)), tf.float32)\n",
    "\n",
    "            reward_loss = tf.concat([self.reward_holder, self.BCE_holder], 1)\n",
    "            cell = ALSTM(num_units=self.num_hidden, \n",
    "                            words_attn_dim=self.num_hidden, \n",
    "                            words_attn_states=self.Aattn_vecs, \n",
    "                            words_attn_idx=Aattn_idx,\n",
    "                            img_attn_dim=self.img_dims+self.bbox_dims,\n",
    "                            img_attn_states=attn_vecs,\n",
    "                            img_attn_idx=self.attn_idx,\n",
    "                            batch_size=self.batch_size, \n",
    "                            unk=tf.nn.embedding_lookup(embed, self.unk),\n",
    "                            isEdit=self.isEdit,\n",
    "                            reward_loss=reward_loss, use_wordAttn=use_wordAttn,\n",
    "                            editRandomlyProb=self.editRandomlyProb, rnn_editProb=self.rnn_editProb,\n",
    "                            dropout_in=self.dropout_in, dropout_out=self.dropout_out,\n",
    "                            delta=self.delta, qeuries_length=self.queries_lens)\n",
    "\n",
    "            if useBidirectionalRnn:\n",
    "                self.outputs, self.last_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw=cell,\n",
    "                    cell_bw=cell,\n",
    "                    dtype=tf.float32,\n",
    "                    sequence_length=self.queries_lens,\n",
    "                    inputs=embed_queries)\n",
    "\n",
    "                # self.last_states contain both forward state\n",
    "                # and backward state.\n",
    "                # We don't use A with bidirectional rnn\n",
    "                # so no need for self.values (action values as calculated by A)\n",
    "                Bstate = tf.concat(\n",
    "                    [tf.slice(self.last_states[0][1], [0,0], [-1, self.num_hidden]), \n",
    "                     tf.slice(self.last_states[1][1], [0,0], [-1, self.num_hidden])], -1)\n",
    "            else:\n",
    "                self.outputs, self.last_states = tf.nn.dynamic_rnn(\n",
    "                    cell=cell,\n",
    "                    dtype=tf.float32,\n",
    "                    sequence_length=self.queries_lens,\n",
    "                    inputs=embed_queries)\n",
    "\n",
    "                # self.values[0]=value for not editing, self.values[1]=value for editing\n",
    "                self.values = tf.slice(self.outputs, [0,0,0], [-1,-1,2])\n",
    "                Bstate = tf.slice(self.last_states[1], [0,0], [-1, self.num_hidden])  \n",
    "\n",
    "\n",
    "        Avars = cell.Avars\n",
    "        self.Avars = {var.name:var for var in Avars}\n",
    "\n",
    "        if bnorm:\n",
    "            self.scores = self.bnorm_attention(Bstate, Urnn_norm=Urnn_norm, Uatt_norm=Uatt_norm) \n",
    "        else:\n",
    "            self.scores = self.attention(Bstate) \n",
    "\n",
    "\n",
    "        # Cross entophy loss per query in the batch.\n",
    "        self.B_ce = -tf.reduce_sum(\n",
    "                        self.labels*tf.log(self.scores+0.00000001)+\n",
    "                            (1-self.labels)*tf.log((1-self.scores)+0.00000001), \n",
    "                            axis=-1)\n",
    "\n",
    "\n",
    "        # We don't use A with bidirectional rnn\n",
    "        if not useBidirectionalRnn:\n",
    "            # A's decision for each word.\n",
    "            # self.idx[i,j]=1 if word j in query i was edited, else zero.\n",
    "            self.idx = tf.squeeze(tf.slice(self.outputs, [0,0,2], [-1,-1,1]))\n",
    "            \n",
    "            # edit_ratio: Over all ratio of edited words per query\n",
    "            # \n",
    "            self.edit_ratio = tf.reduce_mean(tf.reduce_sum(\n",
    "                tf.cast(self.idx, tf.float32)*tf.expand_dims(\n",
    "                        1/tf.cast(self.queries_lens, tf.float32), axis=1), axis=1))\n",
    "\n",
    "            # After running A for the first time, we get A's decisions and their values.\n",
    "            # we then calulate the following tensors:\n",
    "            # actions_idx[j,i] = 1 if the word i in query j was edited or 0 otherwise.     \n",
    "            # bell_vall holds the values for each decision by the bellman function. \n",
    "            \n",
    "           \n",
    "            '''\n",
    "            Now that we know what actions A took,\n",
    "            we calculate the actual reward per word A gets:\n",
    "            act_rewards[i,j] = idx[i,j]*edit_reward*BCE[i]/queries_lens[i]\n",
    "\n",
    "            If word j in query i was edited then idx[i,j]=1 and:\n",
    "               act_rewards[i,j] = 1*edit_reward*BCE[i]/queries_lens[i]=edit_reward*BCE[i]/queries_lens[i]\n",
    "            else edited idx[i,j]=0 and:\n",
    "               act_rewards[i,j] = 0*edit_reward*BCE[i]/queries_lens[i]=0\n",
    "            \n",
    "            if the word is '<pad>' than act_rewards[i,j]=0 always\n",
    "            '''\n",
    "            q_mask = tf.cast(tf.abs(tf.sign(self.queries)), tf.float32)\n",
    "            act_rewards = q_mask*self.idx*self.reward_holder*self.BCE_holder/tf.expand_dims(tf.cast(self.queries_lens, tf.float32), 1)\n",
    "            \n",
    "            # Scale B's loss per query, when words were edited. \n",
    "            # We scale all the losses to have mean 1 and std 0.5,\n",
    "            # this gives A a fixed range of (state, action) values\n",
    "            Bce_min = tf.reduce_min(self.B_ce)\n",
    "            Bce_max = tf.reduce_max(self.B_ce)\n",
    "            Bce_scale = (self.B_ce-Bce_min)/(Bce_max-Bce_min)\n",
    "            s = tf.contrib.keras.backend.std(Bce_scale, keepdims=True)\n",
    "            # new_BCE is the scaled B's losses with words edits.\n",
    "            new_BCE = 0.5*Bce_scale/s # 0.5 std\n",
    "            new_BCE = 1 + new_BCE - tf.reduce_mean(new_BCE) # 1 mean\n",
    "            \n",
    "            # In order to get the values of A's real actions (see self.pyrite_val in __init)\n",
    "            # Tensorflow need us to change idx to a list where each item\n",
    "            # is [number of query x maximum number of words in a query, action (1 or 0 for edit or not respectively)]\n",
    "            self.actions_idx = tf.concat(\n",
    "                                [tf.expand_dims(tf.range(self.batch_size*self.qlen), 1), \n",
    "                                         tf.expand_dims(tf.reshape(tf.cast(self.idx,tf.int32), [-1]), 1)], -1)\n",
    "            \n",
    "            # Now that we have the real actions A took and\n",
    "            # B's loss per query, we can calculate the REAL (word, action) value \n",
    "            # using bellman equation.\n",
    "            bell_val = self.discount_rewards(act_rewards, new_BCE)\n",
    "            \n",
    "            # pyrite_val: holds A's predicted values for each of its (word, actions) pair.\n",
    "            # self.pyrite_val[i,j] = \n",
    "            #            self.values[i,j][self.actions_idx[i,j]] = \n",
    "            #                               the value A predicted (i.e. self.values[i,j])\n",
    "            #                               for the action it took (i.e. self.actions_idx[i,j])\n",
    "            #                               at query i word j.\n",
    "            # (see https://en.wikipedia.org/wiki/Pyrite.)\n",
    "            pyrite_val = tf.reshape(tf.gather_nd(tf.reshape(self.values, (-1,2)), \n",
    "                                                      self.actions_idx), (self.batch_size, -1))\n",
    "            \n",
    "            # RMSE loss between the real bellman value and \n",
    "            # A's predicted values. We mask the loss by zeroing the padded words.\n",
    "            # A smooth factor is added to prevent the gradient from being nan. \n",
    "            self.A_loss = tf.sqrt(tf.reduce_mean(\n",
    "                tf.reduce_sum(tf.square(q_mask*(bell_val-pyrite_val))/tf.expand_dims(tf.cast(self.queries_lens, tf.float32), axis=1), axis=-1)+0.00000001))\n",
    "\n",
    "        self.B_loss = tf.reduce_mean(self.B_ce)\n",
    "\n",
    "        ##############\n",
    "        # Optimizers #\n",
    "        ##############\n",
    "\n",
    "        if not useBidirectionalRnn:\n",
    "            # Train only A variables\n",
    "            A_starter_learning_rate = self.Alr\n",
    "            A_global_step = tf.Variable(0, name='A_global_step', trainable=False)\n",
    "            self.A_learning_rate = tf.train.exponential_decay(A_starter_learning_rate, A_global_step,\n",
    "                                                       decay_steps=decay_steps, decay_rate=decay_rate, staircase=True)\n",
    "        \n",
    "            self.A_optimizer =  tf.train.GradientDescentOptimizer(\n",
    "                    learning_rate=self.A_learning_rate).minimize(self.A_loss, global_step=A_global_step, var_list=Avars)  \n",
    "\n",
    "        # Train only B variables \n",
    "        B_starter_learning_rate = self.Blr\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.learning_rate = tf.train.exponential_decay(B_starter_learning_rate, self.global_step,\n",
    "                                                   decay_steps=decay_steps, decay_rate=decay_rate, staircase=True)\n",
    "\n",
    "        Bvars = [var for var in tf.trainable_variables() if var not in Avars]\n",
    "        self.B_optimizer =  tf.train.GradientDescentOptimizer(\n",
    "                    learning_rate=self.learning_rate).minimize(self.B_loss, global_step=self.global_step, \n",
    "                                                               var_list=Bvars)  \n",
    "\n",
    "        if not os.path.exists(params_dir):\n",
    "                os.makedirs(params_dir)\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "    def length(self, seq):\n",
    "        '''\n",
    "        Retruns real lengths (before addings) of all queries in seq  .\n",
    "        '''\n",
    "        return tf.cast(tf.reduce_sum(tf.sign(tf.abs(seq)), reduction_indices=1), tf.int32)\n",
    "       \n",
    "\n",
    "    def linear(self, inputs, output_dim, scope='linear', bias=True, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            W = tf.get_variable('W', initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                shape=(inputs.get_shape()[-1], output_dim))\n",
    "            if bias:\n",
    "                b = tf.get_variable('b', initializer=tf.constant_initializer(0.1),\n",
    "                               shape=[1, output_dim])\n",
    "                return tf.matmul(inputs, W) + b\n",
    "            \n",
    "            return tf.matmul(inputs, W)\n",
    "        \n",
    "    \n",
    "    def Qscale(self, qVecs):\n",
    "        '''\n",
    "        Scale queries embedding vectors to have zero mean and std STD.\n",
    "        \n",
    "        Params:\n",
    "            qVecs: Tensor (shape: batch_size x number_of_hidden_units) \n",
    "                   holding the queries vectors, \n",
    "        '''\n",
    "        qVecs_min = tf.reduce_min(qVecs, axis=-1, keep_dims=True)\n",
    "        qVecs_max = tf.reduce_max(qVecs, axis=-1, keep_dims=True)\n",
    "        qVecs_scale = (qVecs-qVecs_min)/(qVecs_max-qVecs_min) # Scale to 0-1\n",
    "        s = tf.contrib.keras.backend.std(qVecs_scale, axis=-1, keepdims=True)\n",
    "        tmp = self.Qstd * qVecs_scale/s #Scale to have lf.Qstd std\n",
    "        new_qVecs = tmp - tf.reduce_mean(tmp, axis=-1, keep_dims=True) # zero mean\n",
    "        \n",
    "        return new_qVecs\n",
    "        \n",
    "\n",
    "            \n",
    "    def attention(self, q_embed):\n",
    "        '''\n",
    "        Given B's output vector, calculate the attention over \n",
    "        all the query's bounding boxes vectors, That is, calculate:\n",
    "        \n",
    "        probs = softmax(relu(context(Sq+Satt+b)))\n",
    "        \n",
    "        Where:\n",
    "        Sq = <Wq, queries_states>\n",
    "        Sattn = <Wattn, attention_bboxes_vectors>\n",
    "        \n",
    "        The  bounding box with the highest attention score will be chosen as the correct bounding box.\n",
    "        \n",
    "        Params:\n",
    "            q_embed: Tensor of shape (batch size x num_hidden)B's outputs. \n",
    "            \n",
    "        Returns:\n",
    "            probs: Tensor of shape (batch_size x max bbox number for query).\n",
    "                   Attention score for each bbox.\n",
    "        '''\n",
    "        # concatenate img vectors with spaical features\n",
    "        attn_vecs = tf.concat([self.img, self.bboxes], 2)\n",
    "        \n",
    "        # B's outputs, shape: (batch size x num_hidden)\n",
    "        if self.toQscale:\n",
    "            Urnn = self.Qscale(q_embed)\n",
    "        else:\n",
    "            Urnn = q_embed\n",
    "        \n",
    "        # Attention vectors, \n",
    "        # shape: (batch size x max bbox number for query x attention vector size)\n",
    "        Uatt = attn_vecs\n",
    "           \n",
    "        with tf.variable_scope('l1'):\n",
    "            b = tf.get_variable(\n",
    "                    'b', \n",
    "                    initializer=tf.constant_initializer(0.1), \n",
    "                    shape=[1, self.num_hidden])\n",
    "\n",
    "            context = tf.get_variable(\n",
    "                    'context', \n",
    "                    initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1), \n",
    "                    shape=[self.num_hidden, 1])\n",
    "\n",
    "\n",
    "            Sq = tf.nn.dropout(\n",
    "                self.linear(Urnn, self.num_hidden, bias=False, scope='Sq'), \n",
    "                self.dropout_q)\n",
    "            \n",
    "            Sattn = tf.nn.dropout(\n",
    "                tf.reshape(\n",
    "                    self.linear(\n",
    "                        tf.reshape(Uatt, (-1, self.img_dims+self.bbox_dims)), \n",
    "                        self.num_hidden, \n",
    "                        bias=False, scope='Sattn'), \n",
    "                    [self.batch_size, -1, self.num_hidden]),\n",
    "                self.dropout_img)\n",
    "\n",
    "        out = tf.nn.relu(tf.expand_dims(Sq, 1) + Sattn + b)\n",
    "        logits = tf.reshape(tf.matmul(tf.reshape(out, (-1, tf.shape(out)[-1])),  context), (tf.shape(out)[0], -1))\n",
    "\n",
    "        # Calculate logits's masked softmax (we use self.attn_idx to mas\n",
    "        max_logits = tf.reduce_max(logits, axis=-1)\n",
    "        masked_logits = tf.exp(logits-tf.expand_dims(max_logits, axis=1))*self.attn_idx\n",
    "        probs = self.attn_idx*masked_logits/tf.reduce_sum(masked_logits, axis=-1, keep_dims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    \n",
    "    def bnorm_attention(self, q_embed, Urnn_norm=True, Uatt_norm=True):\n",
    "        '''\n",
    "        Given B's output vector, calculate the attention over \n",
    "        all the query's bounding boxes vectors, That is, calculate:\n",
    "        \n",
    "        probs = softmax(relu(context(Sq+Satt+b)))\n",
    "        \n",
    "        Where:\n",
    "        Sq = <Wq, queries_states>\n",
    "        Sattn = <Wattn, attention_bboxes_vectors>\n",
    "        \n",
    "        The  bounding box with the highest attention score will be chosen as the correct bounding box.\n",
    "        This function uses batch normalization. \n",
    "        \n",
    "        Params:\n",
    "            q_embed: Tensor of shape (batch size x num_hidden)B's outputs. \n",
    "            Urnn_norm: Whether to use batch normalization for the queries.\n",
    "            Uatt_norm: Whether to use batch normalization for the VGG outputs.\n",
    "            \n",
    "        Returns:\n",
    "            probs: Tensor of shape (batch_size x max bbox number for query).\n",
    "                   Attention score for each bbox.\n",
    "        '''\n",
    "        # concatenate img vectors with with spaical features\n",
    "        attn_vecs = tf.concat([self.img, self.bboxes], 2)\n",
    "        if self.toQscale:\n",
    "            q_embed = self.Qscale(q_embed)\n",
    "            \n",
    "        Urnn = q_embed\n",
    "        Uatt = attn_vecs\n",
    "        \n",
    "        if Urnn_norm:\n",
    "            # B's outputs with bath normalization. \n",
    "            # shape: (batch size x num_hidden)\n",
    "            Urnn = tf.contrib.layers.batch_norm(\n",
    "                q_embed, center=True, scale=True, is_training=self.isTrain) \n",
    "        \n",
    "        if Uatt_norm:\n",
    "            # Attention vectors with bath normalization. \n",
    "            # shape: (batch size x max bbox number for query x attention vector size)\n",
    "            Uatt = tf.contrib.layers.batch_norm(\n",
    "                attn_vecs, center=True, scale=True, is_training=self.isTrain)\n",
    "           \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            with tf.variable_scope('bnorm_l1'):\n",
    "                b = tf.get_variable(\n",
    "                        'b', \n",
    "                        initializer=tf.constant_initializer(0.1), \n",
    "                        shape=[1, self.num_hidden])\n",
    "\n",
    "                context = tf.get_variable(\n",
    "                        'context', \n",
    "                        initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1), \n",
    "                        shape=[self.num_hidden, 1])\n",
    "\n",
    "\n",
    "                Sq = tf.nn.dropout(\n",
    "                    self.linear(Urnn, self.num_hidden, bias=False, scope='Sq'), \n",
    "                    self.dropout_q)\n",
    "                \n",
    "                Sattn = tf.nn.dropout(\n",
    "                            tf.reshape(\n",
    "                                self.linear(\n",
    "                                    tf.reshape(Uatt, (-1, self.img_dims+self.bbox_dims)), \n",
    "                                    self.num_hidden, \n",
    "                                    bias=False, scope='Sattn'), \n",
    "                                 [self.batch_size, -1, self.num_hidden]),\n",
    "                            self.dropout_img)\n",
    "                    \n",
    "                   \n",
    "            out = tf.nn.relu(tf.expand_dims(Sq, 1) + Sattn + b)\n",
    "            logits = tf.reshape(tf.matmul(tf.reshape(out, (-1, tf.shape(out)[-1])),  context), (tf.shape(out)[0], -1))\n",
    "\n",
    "            # Calculate logits's masked softmax (we use self.attn_idx to mas\n",
    "            max_logits = tf.reduce_max(logits, axis=-1)\n",
    "            masked_logits = tf.exp(logits-tf.expand_dims(max_logits, axis=1))*self.attn_idx\n",
    "            probs = self.attn_idx*masked_logits/tf.reduce_sum(masked_logits, axis=-1, keep_dims=True)\n",
    "\n",
    "            return probs\n",
    "  \n",
    "        \n",
    "    def q_padding(self, seq, max_length):\n",
    "        '''\n",
    "        Pad  seq with vocab['<pad>'] (0) to max_length length.\n",
    "        '''                  \n",
    "        return seq + [self.vocab['<pad>']]*(max_length-len(seq))\n",
    "\n",
    "    \n",
    "    def build_data(self, data, start, end, imScale, addNoise=False):\n",
    "        '''\n",
    "        Build batch.\n",
    "        ------------\n",
    "        \n",
    "        Params:\n",
    "            data: each entry in this list has the following structure:\n",
    "                  [query indexes, [bounding box vector (VGG), bounding box spaital features], ..., \n",
    "                  [bounding box vector (VGG), bounding box spaital features], index of the true label]\n",
    "                  \n",
    "            start/end: batch data is built from data[start:end]\n",
    "            \n",
    "        Returns:\n",
    "            attn_idx: attn_idx[i, j]=1 if the j'th bbox in the i'th query is not padding, else equals to 0. \n",
    "            \n",
    "            padded_queries: list of queries, padded to the length of the longest query in the batch.\n",
    "                            Note: vocab['pad']=0\n",
    "                            \n",
    "            padded_im: list of bounding boxes vectors, padded to the maximum number of bbox per query.\n",
    "                       Note: padded vector is vector of zeros. \n",
    "                            \n",
    "            padded_bbox: list of bounding boxes spatial features, padded to the maximum number of bbox per query.\n",
    "                         Note: padded vector is vector of zeros.  \n",
    "        \n",
    "            onehot_labels: onehot_labels[i][j]=1 if j is the true bbox for query i, else  onehot_labels[i][j]=0\n",
    "            \n",
    "            addNoise: Boolean. Whether to add normal noise to the images.\n",
    "            \n",
    "            imScale: If not None, scale the image vectors (VGG16 outputs) to have 0 mean and imScale std\n",
    "                        \n",
    "        '''\n",
    "                          \n",
    "        qlen = max([len(data[i][0]) for i in range(start, end)]) # Length fo the longest query\n",
    "        imlen = max([len(data[i]) for i in range(start, end)])-2 # Maximum number of bbox per query.\n",
    "        padded_queries, padded_im, padded_bbox, attn_idx = [], [], [], []\n",
    "        \n",
    "        # Build one hot labels from the labels index, given in the data.                  \n",
    "        labels = [item[-1] for item in data[start:end]] #data[i][-1]=index of the true bbox of query i\n",
    "        dist_labels = np.zeros((end-start, imlen)) #label distribution\n",
    "        \n",
    "        # Real data points\n",
    "        dist_labels[[i for i in np.arange(end-start) if labels[i]>0], [l for l in labels if l>0]]=1\n",
    "        \n",
    "        # augmented data points\n",
    "        for i in np.arange(end-start):\n",
    "            if labels[i]<0:\n",
    "                dist_labels[i] = [-1/labels[i] for _ in range(-labels[i])]+[0. for _ in range(imlen+labels[i])]\n",
    "                          \n",
    "        im_dim, bbox_dim = data[0][1][0].shape[1], data[0][1][1].shape[1]\n",
    "        for i in range(start, end):\n",
    "            padded_queries.append(self.q_padding(data[i][0], qlen))\n",
    "            \n",
    "            attn_idx.append([1 for _ in range(len(data[i])-2)]+[0 for _ in range(imlen-(len(data[i])-2))])\n",
    "            \n",
    "            padded_im.append(np.concatenate([data[i][j][0] for j in range(1, len(data[i])-1)] + \n",
    "                                       [np.full((imlen-(len(data[i])-2), im_dim), vocab['<pad>'], dtype=np.float32)], axis=0))\n",
    "            \n",
    "            padded_bbox.append(np.concatenate([data[i][j][1] for j in range(1, len(data[i])-1)] + \n",
    "                                       [np.full((imlen-(len(data[i])-2),bbox_dim), vocab['<pad>'], dtype=np.float32)], axis=0))\n",
    "           \n",
    "        \n",
    "        \n",
    "        if addNoise:\n",
    "            padded_im+=(padded_im+np.random.normal(0, .1, np.array(padded_im).shape))*np.expand_dims(attn_idx, 2)\n",
    "        else:\n",
    "            padded_im=np.array(padded_im)\n",
    "            \n",
    "        if imScale is not None:\n",
    "            # Smoothing factor\n",
    "            halper = (1-np.expand_dims(np.array(attn_idx).astype(np.float32), 2))\n",
    "            img_min = np.min(padded_im, axis=-1, keepdims=True)\n",
    "            img_max = np.max(padded_im, axis=-1, keepdims=True)\n",
    "            img_scale = (padded_im-img_min)/((img_max-img_min)+halper) # Scale to 0-1\n",
    "            s = np.std(img_scale, axis=-1, keepdims=True)\n",
    "            padded_im = imScale*img_scale/(s+halper)  #Scale to have imScale std\n",
    "            padded_im = padded_im - np.mean(padded_im, axis=-1, keepdims=True) # scale to zero mean\n",
    "            \n",
    "        return qlen, np.array(attn_idx), np.array(padded_queries, dtype=np.int32), padded_im, np.array(padded_bbox), np.array(dist_labels)\n",
    "            \n",
    "   \n",
    "    def ground(self, data=None, start=None, end=None, sess=None, feed_dict = None, isEdit=True, imScale=False):\n",
    "        '''\n",
    "        Given a query and a list of bboxes, the function returns the index of the referred bbox.\n",
    "        '''\n",
    "        isSess = (sess==None)\n",
    "        if isSess:\n",
    "            sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            if isSess:\n",
    "                tf.global_variables_initializer().run()\n",
    "                ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "                else:\n",
    "                    print('Initializing variables')\n",
    "            if feed_dict is None:\n",
    "                qlen, attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(data, start, end, imScale=imScale)\n",
    "                feed_dict = {\n",
    "                        self.qlen:qlen,\n",
    "                        self.queries:padded_queries,\n",
    "                        self.img:padded_im,\n",
    "                        self.bboxes:padded_bbox,\n",
    "                        self.labels:labels,\n",
    "                        self.attn_idx:attn_idx\n",
    "                    }\n",
    "            feed_dict[self.isTrain]=False\n",
    "            feed_dict[self.isEdit] = isEdit\n",
    "            feed_dict[self.dropout_in]=1.\n",
    "            feed_dict[self.dropout_out]=1.\n",
    "            feed_dict[self.dropout_img]=1.\n",
    "            feed_dict[self.dropout_q]=1.\n",
    "            scores = sess.run(self.scores, feed_dict=feed_dict) # get score for each bbox\n",
    "\n",
    "        return np.argmax(scores, axis=1), np.argmax(feed_dict[self.labels], axis=1)\n",
    "        \n",
    "        \n",
    "    def iou_accuracy(self, data, start, end, imScale, sess=None, feed_dict = None, \n",
    "                     threshold=0.5, test=False, isEdit=True, scores=[], labels=[]):\n",
    "        '''\n",
    "        Calculate the IOU score between the Model bbox and the true bbox.\n",
    "        ''' \n",
    "        # Get score for each bbox (labels) and the true bbox index (gt_idx)  \n",
    "        if len(scores)!=0  and len(labels)!=0:\n",
    "            labels, gt_idx = np.argmax(scores, axis=1), np.argmax(labels, axis=1)\n",
    "        else:             \n",
    "            if feed_dict is None:\n",
    "                labels, gt_idx = self.ground(data, start, end, sess=sess, feed_dict=feed_dict, \n",
    "                                             isEdit=isEdit, imScale=imScale)\n",
    "            else: labels, gt_idx = self.ground(sess=sess, feed_dict=feed_dict, \n",
    "                                               isEdit=isEdit, imScale=imScale)\n",
    "            \n",
    "        acc = 0\n",
    "        for i in range(start, end):\n",
    "            gt = data[i][gt_idx[i-start]+1][1][0] # ground truth bbox\n",
    "            crops = np.expand_dims(data[i][labels[i-start]+1][1][0], axis=0) #Model chosen bbox\n",
    "            acc += (retriever.compute_iou(crops, gt)[0]>threshold) #IOU for the i sample.\n",
    "            \n",
    "        return acc/(end-start)\n",
    "        \n",
    "    def accuracy(self, imScale, data=None, start=None, end=None, sess=None, feed_dict = None, isEdit=True):\n",
    "        isSess = (sess==None)\n",
    "        if isSess:\n",
    "            print('Building sess')\n",
    "            sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            if isSess:\n",
    "                print('Building sess used')\n",
    "                tf.global_variables_initializer().run()\n",
    "                ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    print('3')\n",
    "                    self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "                else:\n",
    "                    print('Initializing variables')\n",
    "            if feed_dict is None:\n",
    "                print('Building feed_dict')\n",
    "                qlen, attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(data, start, end, imScale=imScale)\n",
    "                feed_dict = {\n",
    "                        self.qlen:qlen,\n",
    "                        self.queries:padded_queries,\n",
    "                        self.img:padded_im,\n",
    "                        self.bboxes:padded_bbox,\n",
    "                        self.attn_idx:attn_idx,\n",
    "                        self.labels:labels,\n",
    "                    }\n",
    "                \n",
    "            feed_dict[self.isTrain]=False\n",
    "            feed_dict[self.isEdit] = isEdit\n",
    "            feed_dict[self.dropout_in]=1.\n",
    "            feed_dict[self.dropout_out]=1.\n",
    "            feed_dict[self.dropout_img]=1.\n",
    "            feed_dict[self.dropout_q]=1.\n",
    "            scores = sess.run(self.scores, feed_dict=feed_dict)\n",
    "            acc = sum(np.equal(np.argmax(scores, axis=1), np.argmax(feed_dict[self.labels], axis=1))/len(feed_dict[self.labels]))\n",
    "\n",
    "                    \n",
    "        return acc\n",
    "    \n",
    "    def discount_rewards(self, rewards, last_rewards, gamma=1.0):\n",
    "        \"\"\" \n",
    "        take 1D float array of rewards and compute discounted reward \n",
    "        using bellman function, i.e. for some query:\n",
    "                \n",
    "                value_(time step t) = rewards[t] + gamma*values_(time step t+1)\n",
    "    \n",
    "        We need to iterate over the rewards tensor, which is tricky \n",
    "        with Tensorflow, specially when the query maximum length (rewards.shape[1])\n",
    "        can change at each iteration.\n",
    "        \n",
    "        We start with initializer tensor with shape (batch_size+1) x self.qlen.\n",
    "        The first vector in initializer is BCE (B's loss per query) all the other \n",
    "        vectors are zero vectors, that is:\n",
    "        \n",
    "        initializer[0][i] is B's loss for query i.\n",
    "        initializer[1:batch_size] are all zero vectors.\n",
    "        \n",
    "        This is doen since tf.foldl does not allow the shape of the function accumulator (bell_vals) to change.\n",
    "        We then use tf.foldl function to go over all the words in reverse (see tf.range(self.qlen)[::-1] in \n",
    "        tf.foldl below). tf.foldl initialize fn's argument, bell_vals, with initializer and change it at each iteration.\n",
    "        \n",
    "        params:\n",
    "            rewards: rewards[i,j] is the reward for the action A took on word j at query i.\n",
    "            last_rewards: last_rewards[i] is B's loss for query i with no edits. This is the final reward.\n",
    "            gamma: discount factor.\n",
    "        \"\"\"\n",
    "                 \n",
    "        initializer = tf.concat(\n",
    "            [tf.expand_dims(last_rewards,1), tf.zeros(shape=(self.batch_size, self.qlen), dtype=tf.float32)], 1)\n",
    "        \n",
    "        def fn(bell_vals, time_steps):\n",
    "            '''\n",
    "            calculate bellman function.\n",
    "            \n",
    "            returns: \n",
    "                bell_vals: Tensor where bell_vals[i][t] is the value for word t in query i. \n",
    "            \n",
    "            algorithm description:\n",
    "                initialization: bell_vals=intitializer.\n",
    "                for time_step in range(maximum_time_step-1, 0, -1):\n",
    "                    1. val_ts = rewards[:,time_step]+bell_vals[0] (time step t values at each query)\n",
    "                    2. bell_vals = bell_vals[:,:-1] (pop last column (zero vector) from bell_vals)\n",
    "                    3. push (concatenate) rewards_ts to the begining of bell_vals so that\n",
    "                       bell_vals[0] = val_ts (time step t values)\n",
    "                    \n",
    "                \n",
    "            '''\n",
    "            \n",
    "            val_ts = tf.slice(bell_vals, [0, 0], [-1, 1]) + tf.slice(rewards, [0, time_steps], [-1 ,1])\n",
    "            bell_vals = tf.concat([val_ts, bell_vals[:,:-1]], 1)\n",
    "            return bell_vals\n",
    "        \n",
    "        bell_vals = tf.foldl(fn, tf.range(self.qlen)[::-1], initializer=initializer)[:,:-1]\n",
    "        \n",
    "        return bell_vals\n",
    "        \n",
    "    def train(self, trn_data, tst_data, val_data, ephocs_num, edit_reward, startA=10, \n",
    "              activation_ephoc=19, muteBnum=3, start_ephoc=0, dropout_in=1., rnn_editProb=0.2,\n",
    "              dropout_out=1., dropout_img=1., dropout_q=1., queries_editProb=0.95,  \n",
    "              addNoise=False, imScale=None, Alr=0.1, Blr=0.05):\n",
    "                          \n",
    "        '''\n",
    "        Params:\n",
    "            trn_data: list, train set. \n",
    " \n",
    "            tst_data: list, test set.\n",
    "            \n",
    "            val_data: list, validation data.\n",
    "                      At each ephoc>activation_ephoc, if we don't train\n",
    "                      B at this ephoc than we run the model twice on val_data:\n",
    "                      \n",
    "                      * The first time no action will be chosen randomly. \n",
    "                        We calculate Bloss (ABloss) and the average ratio of \n",
    "                        edited words per query (p). \n",
    "                      * Then, at the second run, all the actions are chosen\n",
    "                        randomly with probability of p to edit a word. Again, we\n",
    "                        calculate B's loss (RandBloss)\n",
    "                        \n",
    "                      We then define A's gain at ephoc ep as:\n",
    "                          AGain_ep = ABloss_ep - RandBloss_ep\n",
    "                          if len(self.AGain)>3 and s\n",
    "                      If AGain_ep<AGain_(ep-1) we reduce A's learning rate\n",
    "                      and aventialy stop A training but keep training B with A's\n",
    "                      edits.\n",
    "             \n",
    "            ephocs_num: number of ephocs\n",
    "             \n",
    "            start_ephoc: number of first ephoc.\n",
    "             \n",
    "            edit_reward: int, coefficient to multiply the reward by when editing a word.\n",
    "             \n",
    "            startA: int, Start competition only at ephoc # startA.\n",
    "             \n",
    "            muteBnum: After A starts, for each ephoc which A & B trains, \n",
    "                      only A will be trained for this amount of ephocs.\n",
    "                   \n",
    "            activation_ephoc: at ephoc numer \"activation_ephoc\", A will be activate.\n",
    "                               That is, for (activation_ephoc-startA) number of ephocs, \n",
    "                               A will choose an action randomly and train only when B \n",
    "                               does not (see muteBnum argument).\n",
    "                   \n",
    "            queries_editProb: probabilty for editing a query.\n",
    "            \n",
    "            rnn_edit_prob: If the action is chosen randomly. edit with probability of rnn_editProb\n",
    "            \n",
    "            dropout_in: dropout ratio of B's rnn inputs.\n",
    "            \n",
    "            dropout_output: dropout ratio of B's rnn output.\n",
    "            \n",
    "            dropout_img: dropout ratio of images vectors before the last attention layer .\n",
    "            \n",
    "            addNoise: Boolean. Whether to add normal noise to the images (see build_data).\n",
    "            \n",
    "            imScale: If not None, scale the image vectors (VGG16 outputs) to have 0 mean and 1/imScale std\n",
    "            \n",
    "            Alr: A's learning rate\n",
    "            \n",
    "            Blr: B's learning rate\n",
    "                               \n",
    "        '''                  \n",
    "        \n",
    "        trn_nbatch = len(trn_data)\n",
    "        tst_nbatch = len(tst_data)\n",
    "        val_nbatch = len(val_data)\n",
    "        print('# Train set size:', sum([len(batch) for batch in trn_data]))\n",
    "        print('# Training batches:', trn_nbatch)\n",
    "        print('# Validation set size:', sum([len(batch) for batch in val_data]))\n",
    "        print('# Validation batches:', val_nbatch)\n",
    "        print('# Test set size:', sum([len(batch) for batch in tst_data]))\n",
    "        print('# Testing batches:', tst_nbatch)\n",
    "        self.test_res, self.train_res, self.val_res = [], [], [] #list to hold reults for train, test and validation sets\n",
    "        \n",
    "        # Holds different between validation B loss with and without random actions selections (see val_data in params)\n",
    "        self.AGain = []\n",
    "        \n",
    "        Nrounds = 1 # number of time editRandomlyProb was decreased (see below for details)\n",
    "        AlrDecreaseNum = 0 # number of time we've decrease A's learing rate\n",
    "        \n",
    "        # When AlrDecreaseNum=maxDecreaseNum stop training A (toTrainA=False)\n",
    "        # but keeping using it for B training.\n",
    "        maxDecreaseNum = 3 \n",
    "        toTrainA = True\n",
    "        \n",
    "        # A will not edit any word if for the first W-delta words in a query\n",
    "        # where W is the number of words (not including paddings) in the query.\n",
    "        delta = 1 \n",
    "        \n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            tf.global_variables_initializer().run()\n",
    "            ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                print('Loading parameters from', ckpt.model_checkpoint_path)\n",
    "                self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "            else:\n",
    "                print('Initializing variables')\n",
    "                \n",
    "            for ephoc in range(start_ephoc, ephocs_num):\n",
    "                startTime = datetime.now().replace(microsecond=0)\n",
    "                          \n",
    "                # Start training A only at ephooc number startA. \n",
    "                # when ephoc>=startA, each time we train B we train \n",
    "                # only A for muteBnum ephocs, so at ephoc startA, only \n",
    "                # A will train for muteBnum ephocs and B will only then be trained again.\n",
    "                toTrainB = toTrainA==False or ephoc<startA or (ephoc-startA)%(muteBnum+1)==0 # Whether to train only B \n",
    "                  \n",
    "                '''\n",
    "                If activation_ephoc> ephoc >=startA, A will always choose an action\n",
    "                randomly (i.e. editRandomlyProb=1) and will not edit words while B\n",
    "                is training. At ephoc 'activation_ephoc', editRandomlyProb will decrease \n",
    "                and an action will be chosen randomly only if only A is training, in \n",
    "                other words, when B is training editRandomlyProb will always be 0.\n",
    "                \n",
    "                If ephoc >=activation_ephoc, after each 2 rounds (in each round we train only A \n",
    "                for muteBnum ephocs and then B for one ephoc, so each round takes muteBnum+1 epohocs\n",
    "                and the first round start at ephoc startA), the probability for choosing an action \n",
    "                randomly will decrease and delta will increase by one:\n",
    "                let Nrounds be the number of time editRandomlyProb was decreased so far \n",
    "                (Nrounds initialize to be 1), than for every round:\n",
    "                    Nrounds+=1\n",
    "                    editRandomlyProb = 1/Nrounds\n",
    "                    \n",
    "                While training B we never choose an action randomly.\n",
    "                    \n",
    "                Note that we set activation_ephoc so that (activation_ephoc-startA)%(muteBnum+1)=0\n",
    "                thereby insuring that editRandomlyProb will start decreasing at the begining of\n",
    "                a round and won't decrease during the following round.\n",
    "                '''\n",
    "                assert (activation_ephoc-startA)%(muteBnum+1)==0, 'activation_ephoc-startA mod %d != 0'%(muteBnum+1)\n",
    "                \n",
    "                # editRandomlyProb: Probabilty for choosing an action (edit or not) randomly, \n",
    "                if ephoc>=activation_ephoc and (toTrainB or not toTrainA):\n",
    "                    # Never choose an action randomly if training B or \n",
    "                    # A stoped its training (toTrainA=False).\n",
    "                    # If ephoc<activation_ephoc we set isEdit to False\n",
    "                    # when training B hence no words will be edited \n",
    "                    # during B's training any way, only A training \n",
    "                    # will be effected so we can ignore this case.\n",
    "                    editRandomlyProb=0\n",
    "                else:\n",
    "                    editRandomlyProb=1/Nrounds\n",
    "                    if ephoc>startA and (ephoc-startA)%(muteBnum+1)==0: # If this is true toTrainB=False\n",
    "                        if delta<100:\n",
    "                            delta+=1\n",
    "                        Nrounds+=1 \n",
    "                    \n",
    "                        \n",
    "                # When we train B, we show B's results on the trainset \n",
    "                # with edits (itrInEphoc=1) and without (itrInEphoc=2)\n",
    "                itrInEphoc=1 \n",
    "                if ephoc>=startA and toTrainB:\n",
    "                    # we show B's results on the trainset without edits\n",
    "                    itrInEphoc=2\n",
    "                    \n",
    "                print('='*50,'\\nTRAIN, ephoc:%d; Training B:%s; A is a know-it-all:%s; ActivateA:%s'%(ephoc, toTrainB, toTrainA==False, ephoc>=activation_ephoc))\n",
    "                np.random.shuffle(trn_data)\n",
    "                for ep in range(itrInEphoc):\n",
    "                    A_trn_loss, B_trn_loss, trn_acc, trn_iou = 0, 0, 0, 0\n",
    "                    ephoc_edits_ratio = 0 # edited words per query average\n",
    "                    tst_loss, tst_acc, tst_iou = 0, 0, 0\n",
    "                    if ep==1:\n",
    "                        # If ep=1, we're in round 2 in which we only show B's \n",
    "                        # results with out editing (on the train set).\n",
    "                        print('No Edit')\n",
    "                        print('ooooooo')\n",
    "                        \n",
    "                    \n",
    "                    # There's a probability of 1-queries_editProb that B will be trained \n",
    "                    # with no edits in a batch. \n",
    "                    # editRount_count counts the number of batches with competition.\n",
    "                    editRount_count = 0 \n",
    "                    for b in range(trn_nbatch):\n",
    "                        if ep==1:\n",
    "                            # If ep=1, we're in round 2 in which we only show B's \n",
    "                            # results with out editing (on the train set).\n",
    "                            isEdit=False\n",
    "                        else:\n",
    "                            # if A started (ephoc>=startA), edit queries in this \n",
    "                            # batch with probability of queries_editProb else don't edit.\n",
    "                            isEdit = ephoc>=startA and (toTrainB==False or np.random.rand(1)[0]<queries_editProb)\n",
    "                        \n",
    "                        np.random.shuffle(trn_data[b])\n",
    "                        qlen, attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(trn_data[b], \n",
    "                                                                                            0, \n",
    "                                                                                            self.batch_size, \n",
    "                                                                                            addNoise=addNoise,\n",
    "                                                                                            imScale=imScale)\n",
    "                \n",
    "                        feed_dict = {\n",
    "                            self.qlen:qlen,\n",
    "                            self.queries:padded_queries,\n",
    "                            self.img:padded_im,\n",
    "                            self.bboxes:padded_bbox,\n",
    "                            self.attn_idx:attn_idx,\n",
    "                            self.labels: labels,\n",
    "                            self.unk:np.array([[vocab['<unk>']]]),\n",
    "                            self.isEdit:isEdit,\n",
    "                            self.reward_holder:np.array([[0.] for _ in range(self.batch_size)]), # dummy holder \n",
    "                            self.BCE_holder:np.array([[0.] for _ in range(self.batch_size)]), # dummy holder \n",
    "                            self.Aattn_vecs:[[[]]],# dummy holder \n",
    "                            self.dropout_in:dropout_in,\n",
    "                            self.dropout_out:dropout_out,\n",
    "                            self.dropout_img:dropout_img,\n",
    "                            self.dropout_q:dropout_q,\n",
    "                            self.editRandomlyProb:editRandomlyProb,\n",
    "                            self.rnn_editProb:rnn_editProb,\n",
    "                            self.Alr:Alr,\n",
    "                            self.Blr:Blr,\n",
    "                            self.delta:delta,\n",
    "                            self.isTrain:True\n",
    "                        }\n",
    "                        \n",
    "                        \n",
    "                        if isEdit:\n",
    "                            editRount_count+=1\n",
    "\n",
    "                            # We first run B with no edit in order to get it's loss, which will be given to A,\n",
    "                            # and outputs, which A will use attention mechanism over them.   \n",
    "                            feed_dict[self.isEdit] = False\n",
    "                            queries_lens, B_ce, outputs = sess.run([self.queries_lens, self.B_ce, self.outputs], feed_dict=feed_dict)\n",
    "                            feed_dict[self.isEdit] = True\n",
    "                            \n",
    "                            # B_ce contain the cross entrophy loss for each query.\n",
    "                            # We scale all the losses to have mean 1 and std 0.5,\n",
    "                            # this gives A a fixed range of (state, action) values\n",
    "                            Bce_min = min(B_ce)\n",
    "                            Bce_max = max(B_ce)\n",
    "                            Bce_scale = (B_ce-Bce_min)/(Bce_max-Bce_min)\n",
    "                            s = np.std(Bce_scale)\n",
    "                            # BCE is the scaled B's losses\n",
    "                            BCE = 0.5*Bce_scale/s\n",
    "                            BCE = 1 + BCE - np.mean(BCE)\n",
    "                          \n",
    "                            # Reward for edit a word = edit_reward*BCE/queries_lens.\n",
    "                            # A's get the reward for edit a word and loss per query as 2 of its features.\n",
    "                            feed_dict[self.reward_holder]=np.expand_dims(edit_reward*BCE/queries_lens, 1)\n",
    "                            feed_dict[self.BCE_holder]=np.expand_dims(BCE, 1)\n",
    "                           \n",
    "                            \n",
    "                            # A's attention vectors are B's outputs \n",
    "                            # when not words were edited (for all time steps).\n",
    "                            Aattn_vecs = outputs[:,:,:self.num_hidden]\n",
    "                            feed_dict[self.Aattn_vecs]=Aattn_vecs\n",
    "                           \n",
    "                            # Next we run B with edits. Now we have everything we need to train A.\n",
    "                            # Note that we din't optemize A or B yet, hence \n",
    "                            # B will losses and un-edited outputs is still relevent\n",
    "                            if toTrainA:\n",
    "                                gs, scores, batch_Alr, batch_Blr, B_loss, batch_edit_ratio, A_loss, _ = sess.run([self.global_step, self.scores, self.A_learning_rate, self.learning_rate,\n",
    "                                                                                                                   self.B_loss, self.edit_ratio, self.A_loss, self.A_optimizer], \n",
    "                                                                                                                  feed_dict=feed_dict)\n",
    "                            else:\n",
    "                                gs, scores, batch_Blr, B_loss, batch_edit_ratio, A_loss, _ = sess.run([self.global_step, self.scores, self.learning_rate, self.B_loss, \n",
    "                                                                                                    self.edit_ratio, self.A_loss, self.B_optimizer], \n",
    "                                                                                               feed_dict=feed_dict)\n",
    "                                \n",
    "                            # After calculating A results we  want to see\n",
    "                            # B results in this context, therefore we calulate B\n",
    "                            # metrics before optimizing it.\n",
    "                            acc = sum(np.equal(np.argmax(scores, axis=1), np.argmax(labels, axis=1))/len(labels))\n",
    "                            iou_acc = self.iou_accuracy(\n",
    "                                trn_data[b], 0, self.batch_size, imScale=imScale,\n",
    "                                sess=sess, feed_dict=feed_dict, isEdit=isEdit, scores=scores, labels=labels)\n",
    "\n",
    "                            trn_acc += acc/trn_nbatch\n",
    "                            A_trn_loss += A_loss\n",
    "                            B_trn_loss += B_loss/trn_nbatch\n",
    "                            trn_iou += iou_acc/trn_nbatch\n",
    "                            ephoc_edits_ratio += batch_edit_ratio\n",
    "                            \n",
    "                            # Optimize B separately from A to insure the correct flow\n",
    "                            if toTrainB and toTrainA==True:\n",
    "                                if ephoc<activation_ephoc:\n",
    "                                    # if ephoc<activation_ephoc all actions\n",
    "                                    # are chosen randomly and we don't edit words \n",
    "                                    # while B is training.\n",
    "                                    isEdit = False\n",
    "                                    feed_dict[self.isEdit]=isEdit\n",
    "                                    \n",
    "                                batch_Blr, gs, _ = sess.run(\n",
    "                                    [self.learning_rate, self.global_step, self.B_optimizer], \n",
    "                                    feed_dict=feed_dict)\n",
    "                            if b%50==0:\n",
    "                                if toTrainA==0:\n",
    "                                    batch_Alr=-1\n",
    "                                print( 'Edit:', isEdit, ';delta:', delta,\n",
    "                                      ';batch:', b, ';gs:', gs, ';Blr: %.3f'%(batch_Blr), \n",
    "                                      ';Alr: %.3f'%(batch_Alr), ';editRandomlyProb:',editRandomlyProb, \n",
    "                                      ';B loss: %.3f'%(B_loss), ';A loss: %.3f'%(A_loss), \n",
    "                                      ';edits ratio: %.3f'%(batch_edit_ratio), ';acc: %.3f'%(acc), \n",
    "                                      ';iou: %.3f'%(iou_acc), ';time:', datetime.now().replace(microsecond=0)-startTime)    \n",
    "                                        \n",
    "                        else: # isEdit=False\n",
    "                            if ep==0:\n",
    "                                B_loss, lr, gs, _ = sess.run([self.B_loss, self.learning_rate, \n",
    "                                                                self.global_step, self.B_optimizer], feed_dict=feed_dict)\n",
    "                            else: \n",
    "                                # just show results of B with no edits\n",
    "                                B_loss, lr, gs = sess.run([self.B_loss, self.learning_rate, self.global_step], feed_dict=feed_dict)\n",
    "\n",
    "                            acc = self.accuracy(sess=sess, feed_dict=feed_dict, isEdit=isEdit, imScale=imScale)  \n",
    "                            iou_acc = self.iou_accuracy(trn_data[b], 0, self.batch_size, imScale=imScale,\n",
    "                                                        sess=sess, feed_dict=feed_dict, isEdit=isEdit)\n",
    "\n",
    "                            trn_acc += acc/trn_nbatch\n",
    "                            B_trn_loss += B_loss/trn_nbatch\n",
    "                            trn_iou += iou_acc/trn_nbatch\n",
    "\n",
    "                            if b%50==0:\n",
    "                                print('Edit:', isEdit, \n",
    "                                      ';batch:', b, ';gs:', gs, ';Blr: %.3f'%(lr), \n",
    "                                      ';B loss: %.3f'%(B_loss),  ';acc: %.3f'%(acc), \n",
    "                                      ';iou: %.3f'%(iou_acc),';time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "\n",
    "                    if editRount_count>0: \n",
    "                        # If we edited any words during this ephoc,\n",
    "                        # if ephoc <startA we don't.\n",
    "                        print('\\n*B Train loss: %.3f'%(B_trn_loss), \n",
    "                              ';A Train loss: %.3f'%(A_trn_loss/editRount_count),       \n",
    "                              ';Edit ratio: %.3f'%(ephoc_edits_ratio/editRount_count), \n",
    "                              ';Train accuracy: %.3f'%(trn_acc),  ';IOU accuracy: %.3f'%(trn_iou), \n",
    "                              ';Time:', datetime.now().replace(microsecond=0)-startTime, '\\n')\n",
    "                        self.train_res.append([trn_acc, trn_iou, B_trn_loss, A_trn_loss/editRount_count])\n",
    "                    else:\n",
    "                        print('\\n*B Train loss: %.3f'%(B_trn_loss),\n",
    "                              ';Train accuracy: %.3f'%(trn_acc), \n",
    "                              ';IOU accuracy: %.3f'%(trn_iou),  \n",
    "                              ';Time:', datetime.now().replace(microsecond=0)-startTime, '\\n')\n",
    "                        self.train_res.append([trn_acc, trn_iou, B_trn_loss, 0])\n",
    "                self.saver.save(sess, params_dir + \"/model.ckpt\", global_step=ephoc)    \n",
    "                \n",
    "            \n",
    "                if toTrainB==False:\n",
    "                    print('VALIDATION, ephoc:',ephoc)\n",
    "                    print('-'*len('VALIDATION, ephoc: %d'%(ephoc)))\n",
    "                    '''\n",
    "                    VALIDATION\n",
    "\n",
    "                    If ephoc>activation_ephoc, and B's not training (toTrainB=False)\n",
    "                    we run the model twice on val_data:\n",
    "\n",
    "                    * The first time no action will be chosen randomly. \n",
    "                        We calculate Bloss (ABloss) and the average ratio of \n",
    "                        edited words per query (p). \n",
    "                    * Then, at the second run, all the actions are chosen\n",
    "                        randomly with probability of p to edit a word. Again, we\n",
    "                        calculate B's loss (RandBloss)\n",
    "\n",
    "                    We then define A's gain at ephoc # ep as: AGain_ep = ABloss_ep - RandBloss_ep\n",
    "\n",
    "                    If AGain_ep<AGain_(ep-1) we reduce A's learning rate, after maxDecreaseNum \n",
    "                    reductions we stop A training but keep training B with A's edits.\n",
    "                    '''\n",
    "                    ABloss = 0 # B loss when none of the actions are chosen randomly\n",
    "                    RandBloss = 0 # B loss when all actions are chosen randomly\n",
    "                    for ep in range(2):\n",
    "                        startTime = datetime.now().replace(microsecond=0)\n",
    "                        \n",
    "                        if ep==0:\n",
    "                            # If ep==0, none of the actions will be chosen randomly\n",
    "                            editRandomlyProb=0\n",
    "                            val_rnn_editProb = rnn_editProb\n",
    "                        else:\n",
    "                            # If ep==1, all actions will be chosen randomly\n",
    "                            editRandomlyProb=1\n",
    "                            val_rnn_editProb = ephoc_edits_ratio\n",
    "                            \n",
    "                        Aval_loss, Bval_loss, val_acc, val_iou, ephoc_edits_ratio = 0, 0, 0, 0, 0\n",
    "                        print('\\nRandom choice:', ep==1)\n",
    "                        for b in range(val_nbatch):\n",
    "                            qlen, attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(val_data[b],\n",
    "                                                                                        0, self.batch_size, imScale=imScale)\n",
    "                            feed_dict = {\n",
    "                                        self.qlen:qlen,\n",
    "                                        self.queries:padded_queries,\n",
    "                                        self.img:padded_im,\n",
    "                                        self.bboxes:padded_bbox,\n",
    "                                        self.attn_idx:attn_idx,\n",
    "                                        self.labels: labels,\n",
    "                                        self.unk:np.array([[vocab['<unk>']]]),\n",
    "                                        self.isEdit:True,\n",
    "                                        self.reward_holder:np.array([[0.] for _ in range(self.batch_size)]), # dummy holder \n",
    "                                        self.BCE_holder:np.array([[0.] for _ in range(self.batch_size)]), # dummy holder \n",
    "                                        self.Aattn_vecs:[[[]]],# dummy holder \n",
    "                                        self.dropout_in:1.,\n",
    "                                        self.dropout_out:1.,\n",
    "                                        self.dropout_img:1.,\n",
    "                                        self.dropout_q:1.,\n",
    "                                        self.editRandomlyProb:editRandomlyProb,\n",
    "                                        self.rnn_editProb:val_rnn_editProb,\n",
    "                                        self.Alr:Alr,\n",
    "                                        self.Blr:Blr,\n",
    "                                        self.delta:delta,\n",
    "                                        self.isTrain:False\n",
    "                                    }\n",
    "\n",
    "                            feed_dict[self.isEdit] = False\n",
    "                            queries_lens, B_ce, outputs = sess.run([self.queries_lens, self.B_ce, self.outputs], feed_dict=feed_dict)\n",
    "                            feed_dict[self.isEdit] = True\n",
    "\n",
    "                            Bce_min = min(B_ce)\n",
    "                            Bce_max = max(B_ce)\n",
    "                            Bce_scale = (B_ce-Bce_min)/(Bce_max-Bce_min)\n",
    "                            s = np.std(Bce_scale)\n",
    "                            BCE = 0.5*Bce_scale/s\n",
    "                            BCE = 1 + BCE - np.mean(BCE)\n",
    "\n",
    "                            feed_dict[self.reward_holder]=np.expand_dims(edit_reward*BCE/queries_lens, 1)\n",
    "                            feed_dict[self.BCE_holder]=np.expand_dims(BCE, 1)\n",
    "\n",
    "                            Aattn_vecs = outputs[:,:,:self.num_hidden]\n",
    "                            feed_dict[self.Aattn_vecs]=Aattn_vecs\n",
    "\n",
    "                            scores, B_loss, A_loss, batch_edit_ratio = sess.run([self.scores, self.B_loss, self.A_loss, self.edit_ratio], feed_dict=feed_dict)\n",
    "                            acc = sum(np.equal(np.argmax(scores, axis=1), np.argmax(labels, axis=1))/len(labels))\n",
    "                            iou_acc = self.iou_accuracy(\n",
    "                                val_data[b], 0, self.batch_size, sess=sess, \n",
    "                                feed_dict=feed_dict, isEdit=True, imScale=imScale)\n",
    "\n",
    "                            val_acc += acc/val_nbatch\n",
    "                            Bval_loss += B_loss/val_nbatch\n",
    "                            Aval_loss += A_loss/val_nbatch\n",
    "                            val_iou += iou_acc/val_nbatch\n",
    "                            ephoc_edits_ratio += batch_edit_ratio/val_nbatch\n",
    "\n",
    "                            if b%50==0:\n",
    "                                print('batch:', b, ';edits ratio: %.3f'%(batch_edit_ratio), \n",
    "                                      ';B loss: %.3f'%(B_loss), ';A loss: %.3f'%(A_loss), ';acc: %.3f'%(acc), \n",
    "                                      ';iou_acc: %.3f'%(iou_acc), ';time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "\n",
    "                        print('\\n*B loss: %.3f'%(Bval_loss), ';A loss: %.3f'%(Aval_loss), ';Edit ratio: %.3f'%(ephoc_edits_ratio),  \n",
    "                              ';Accuracy %.3f'%(val_acc), ';IOU accuracy: %.3f'%(val_iou), ';Time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "                        \n",
    "                        if ep==0:\n",
    "                            self.val_res.append([[val_acc, val_iou, Bval_loss, Aval_loss]])\n",
    "                            self.AGain.append(Bval_loss)\n",
    "                        else:\n",
    "                            self.val_res[-1].append([val_acc, val_iou, Bval_loss, Aval_loss])\n",
    "                            self.AGain[-1] = self.AGain[-1]-Bval_loss\n",
    "\n",
    "                    if toTrainA and len(self.AGain)>3 and self.AGain[-1]<min(self.AGain[-4:-1]):\n",
    "                        Alr=Alr*0.8\n",
    "                        AlrDecreaseNum+=1\n",
    "                        toTrainA = AlrDecreaseNum<=maxDecreaseNum\n",
    "                        \n",
    "                    print('\\n*AlrDecreaseNum: %d'%(AlrDecreaseNum), '; maxDecreaseNum: %d'%(maxDecreaseNum), \n",
    "                          ';AGain: %.3f'%(self.AGain[-1]), ';toTrainA: %s'%(toTrainA))\n",
    "                    print('-'*len('*AlrDecreaseNum: %d ; maxDecreaseNum: %d ;AGain: %.3f ;toTrainA: %s'%(AlrDecreaseNum, maxDecreaseNum, self.AGain[-1], toTrainA)), '\\n')\n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "                '''\n",
    "                TESTING\n",
    "                '''\n",
    "                if toTrainB:\n",
    "                    print('TESTING, ephoc:',ephoc)\n",
    "                    tstTime = datetime.now().replace(microsecond=0)\n",
    "                    tst_loss, tst_acc, tst_iou = 0, 0, 0\n",
    "                    t_data = tst_data+val_data\n",
    "                    t_nbatch = len(t_data)\n",
    "                    print('# t_data size:', sum([len(batch) for batch in t_data]))\n",
    "                    print('# t_data batches:', t_nbatch)\n",
    "                    \n",
    "                    for b in range(t_nbatch):\n",
    "                        qlen, attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(t_data[b],\n",
    "                                                                                    0, self.batch_size, imScale=imScale)\n",
    "                        \n",
    "                        feed_dict={\n",
    "                                    self.qlen:qlen,\n",
    "                                    self.queries:padded_queries,\n",
    "                                    self.img:padded_im,\n",
    "                                    self.bboxes:padded_bbox,\n",
    "                                    self.attn_idx:attn_idx,\n",
    "                                    self.labels: labels,\n",
    "                                    self.unk:np.array([[vocab['<unk>']]]),\n",
    "                                    self.isEdit:False,\n",
    "                                    self.reward_holder:np.array([[0.] for _ in range(self.batch_size)]),# dummy holder \n",
    "                                    self.BCE_holder:np.array([[0.] for _ in range(self.batch_size)]),# dummy holder \n",
    "                                    self.Aattn_vecs:[[[]]],# dummy holder \n",
    "                                    self.dropout_in:1.,\n",
    "                                    self.dropout_out:1.,\n",
    "                                    self.dropout_img:1.,\n",
    "                                    self.dropout_q:1.,\n",
    "                                    self.editRandomlyProb:0.,\n",
    "                                    self.rnn_editProb:0.,\n",
    "                                    self.Alr:Alr,\n",
    "                                    self.Blr:Blr,\n",
    "                                    self.delta:delta,\n",
    "                                    self.isTrain:False\n",
    "                                }\n",
    "                       \n",
    "                        scores, B_loss = sess.run([self.scores, self.B_loss], feed_dict=feed_dict)\n",
    "                        acc = sum(np.equal(np.argmax(scores, axis=1), np.argmax(labels, axis=1))/len(labels))\n",
    "\n",
    "                        iou_acc = self.iou_accuracy(\n",
    "                            t_data[b], 0, self.batch_size, sess=sess, \n",
    "                            feed_dict=feed_dict, isEdit=False, imScale=imScale)\n",
    "                        \n",
    "                        tst_acc += acc/t_nbatch\n",
    "                        tst_loss += B_loss/t_nbatch\n",
    "                        tst_iou += iou_acc/t_nbatch\n",
    "                        if b%50==0:\n",
    "                            print('batch:', b, ';B loss: %.3f'%(B_loss), ';acc: %.3f'%(acc), \n",
    "                                   ';iou_acc: %.3f'%(iou_acc), ';time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "                            \n",
    "                    print('\\n*Test loss: %.3f'%(tst_loss), ';Test accuracy %.3f'%(tst_acc), \n",
    "                          ';IOU accuracy: %.3f'%(tst_iou), ';Time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "                    self.test_res.append([tst_acc, tst_iou, tst_loss])\n",
    "                    if len(self.test_res)>10 and self.test_res[-1]>max([item[-1] for item in self.test_res[-6:-1]]):\n",
    "                        Blr=Blr*0.9\n",
    "                    \n",
    "                print('='*50,'\\n')\n",
    "            return self.test_res, self.train_res, self.val_res, self.AGain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We start by runnging the Grounder in its simplest form\n",
    "<p> We test the model with different state sizes: 50, 100, 150 and 200. We run each test for 100 ephocs</p>\n",
    "<p> We get about 60% IOU in all the test, about the same as the baseline.<br> On the other hand, the train set IOU gets bigger as the hidden state gets bigger, starting from about 89%, for 50 hidden units, to about 97% with 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_hidden=30\n",
    "params_dir = params_dir_tmp+'RL/testNIXER_hidden:'+str(num_hidden)\n",
    "tf.reset_default_graph()\n",
    "m = Model(\n",
    "    batch_size=200, \n",
    "    num_hidden=num_hidden,\n",
    "    embed_size=embed_vecs.shape[1],\n",
    "    img_dims=trainset[0][0][1][0].shape[1], \n",
    "    bbox_dims=trainset[0][0][1][1].shape[1], \n",
    "    vocab=vocab, \n",
    "    decay_steps=10000, \n",
    "    decay_rate=0.9, \n",
    "    bnorm=False,\n",
    "    toQscale=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params_dir: ../data/training/models/All/RL/testNIXER_hidden:30\n",
      "num_hidden: 30\n",
      "# Train set size: 59400\n",
      "# Training batches: 297\n",
      "# Validation set size: 29800\n",
      "# Validation batches: 149\n",
      "# Test set size: 29600\n",
      "# Testing batches: 148\n",
      "Loading parameters from ../data/training/models/All/RL/testNIXER_hidden:30/model.ckpt-7\n",
      "INFO:tensorflow:Restoring parameters from ../data/training/models/All/RL/testNIXER_hidden:30/model.ckpt-7\n",
      "================================================== \n",
      "TRAIN, ephoc:6; Training B:True; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 1 ;batch: 0 ;gs: 1189 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.551 ;A loss: 0.067 ;edits ratio: 0.102 ;acc: 0.580 ;iou: 0.680 ;time: 0:00:01\n",
      "Edit: True ;delta: 1 ;batch: 50 ;gs: 1239 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.824 ;A loss: 0.052 ;edits ratio: 0.053 ;acc: 0.605 ;iou: 0.710 ;time: 0:00:15\n",
      "Edit: True ;delta: 1 ;batch: 100 ;gs: 1289 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.171 ;A loss: 0.347 ;edits ratio: 0.356 ;acc: 0.375 ;iou: 0.460 ;time: 0:00:29\n",
      "Edit: True ;delta: 1 ;batch: 150 ;gs: 1339 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 3.528 ;A loss: 0.285 ;edits ratio: 0.221 ;acc: 0.140 ;iou: 0.335 ;time: 0:00:42\n",
      "Edit: True ;delta: 1 ;batch: 200 ;gs: 1389 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.662 ;A loss: 0.323 ;edits ratio: 0.424 ;acc: 0.545 ;iou: 0.615 ;time: 0:00:57\n",
      "Edit: True ;delta: 1 ;batch: 250 ;gs: 1439 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.572 ;A loss: 0.199 ;edits ratio: 0.200 ;acc: 0.290 ;iou: 0.390 ;time: 0:01:11\n",
      "\n",
      "*B Train loss: 2.214 ;A Train loss: 0.257 ;Edit ratio: 0.230 ;Train accuracy: 0.423 ;IOU accuracy: 0.540 ;Time: 0:01:24 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;batch: 0 ;gs: 1485 ;Blr: 0.050 ;B loss: 1.485 ;acc: 0.740 ;iou: 0.800 ;time: 0:01:24\n",
      "Edit: False ;batch: 50 ;gs: 1485 ;Blr: 0.050 ;B loss: 1.741 ;acc: 0.660 ;iou: 0.735 ;time: 0:01:31\n",
      "Edit: False ;batch: 100 ;gs: 1485 ;Blr: 0.050 ;B loss: 1.911 ;acc: 0.615 ;iou: 0.675 ;time: 0:01:39\n",
      "Edit: False ;batch: 150 ;gs: 1485 ;Blr: 0.050 ;B loss: 3.330 ;acc: 0.260 ;iou: 0.465 ;time: 0:01:46\n",
      "Edit: False ;batch: 200 ;gs: 1485 ;Blr: 0.050 ;B loss: 1.514 ;acc: 0.745 ;iou: 0.795 ;time: 0:01:54\n",
      "Edit: False ;batch: 250 ;gs: 1485 ;Blr: 0.050 ;B loss: 2.460 ;acc: 0.410 ;iou: 0.525 ;time: 0:02:02\n",
      "\n",
      "*B Train loss: 2.073 ;Train accuracy: 0.556 ;IOU accuracy: 0.671 ;Time: 0:02:09 \n",
      "\n",
      "TESTING, ephoc: 6\n",
      "# t_data size: 59400\n",
      "# t_data batches: 297\n",
      "batch: 0 ;B loss: 3.272 ;acc: 0.250 ;iou_acc: 0.415 ;time: 0:02:10\n",
      "batch: 50 ;B loss: 1.971 ;acc: 0.595 ;iou_acc: 0.685 ;time: 0:02:16\n",
      "batch: 100 ;B loss: 3.073 ;acc: 0.215 ;iou_acc: 0.410 ;time: 0:02:22\n",
      "batch: 150 ;B loss: 1.342 ;acc: 0.775 ;iou_acc: 0.840 ;time: 0:02:29\n",
      "batch: 200 ;B loss: 2.813 ;acc: 0.365 ;iou_acc: 0.500 ;time: 0:02:35\n",
      "batch: 250 ;B loss: 2.433 ;acc: 0.415 ;iou_acc: 0.565 ;time: 0:02:41\n",
      "\n",
      "*Test loss: 2.081 ;Test accuracy 0.551 ;IOU accuracy: 0.667 ;Time: 0:02:47\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:7; Training B:False; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 1 ;batch: 0 ;gs: 1485 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 2.057 ;A loss: 0.235 ;edits ratio: 0.133 ;acc: 0.575 ;iou: 0.665 ;time: 0:00:00\n",
      "Edit: True ;delta: 1 ;batch: 50 ;gs: 1485 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 3.269 ;A loss: 0.194 ;edits ratio: 0.104 ;acc: 0.295 ;iou: 0.460 ;time: 0:00:09\n",
      "Edit: True ;delta: 1 ;batch: 100 ;gs: 1485 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 1.355 ;A loss: 0.276 ;edits ratio: 0.142 ;acc: 0.730 ;iou: 0.780 ;time: 0:00:19\n",
      "Edit: True ;delta: 1 ;batch: 150 ;gs: 1485 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 2.519 ;A loss: 0.190 ;edits ratio: 0.125 ;acc: 0.375 ;iou: 0.470 ;time: 0:00:31\n",
      "Edit: True ;delta: 1 ;batch: 200 ;gs: 1485 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 2.193 ;A loss: 0.175 ;edits ratio: 0.109 ;acc: 0.485 ;iou: 0.605 ;time: 0:00:40\n",
      "Edit: True ;delta: 1 ;batch: 250 ;gs: 1485 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 2.171 ;A loss: 0.159 ;edits ratio: 0.137 ;acc: 0.450 ;iou: 0.590 ;time: 0:00:50\n",
      "\n",
      "*B Train loss: 2.121 ;A Train loss: 0.239 ;Edit ratio: 0.141 ;Train accuracy: 0.506 ;IOU accuracy: 0.622 ;Time: 0:00:59 \n",
      "\n",
      "VALIDATION, ephoc: 7\n",
      "--------------------\n",
      "\n",
      "Random choice: False\n",
      "batch: 0 ;edits ratio: 0.320 ;B loss: 1.727 ;A loss: 0.372 ;acc: 0.480 ;iou_acc: 0.610 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.357 ;B loss: 1.873 ;A loss: 0.356 ;acc: 0.420 ;iou_acc: 0.505 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.304 ;B loss: 2.521 ;A loss: 0.293 ;acc: 0.315 ;iou_acc: 0.440 ;time: 0:00:18\n",
      "\n",
      "*B loss: 2.206 ;A loss: 0.307 ;Edit ratio: 0.313 ;Accuracy 0.403 ;IOU accuracy: 0.516 ;Time: 0:00:28\n",
      "\n",
      "Random choice: True\n",
      "batch: 0 ;edits ratio: 0.231 ;B loss: 1.635 ;A loss: 0.295 ;acc: 0.550 ;iou_acc: 0.680 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.268 ;B loss: 1.833 ;A loss: 0.301 ;acc: 0.520 ;iou_acc: 0.630 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.238 ;B loss: 2.429 ;A loss: 0.245 ;acc: 0.445 ;iou_acc: 0.555 ;time: 0:00:19\n",
      "\n",
      "*B loss: 2.129 ;A loss: 0.290 ;Edit ratio: 0.221 ;Accuracy 0.485 ;IOU accuracy: 0.598 ;Time: 0:00:29\n",
      "\n",
      "*AlrDecreaseNum: 0 ; maxDecreaseNum: 3 ;AGain: 0.077 ;toTrainA: True\n",
      "-------------------------------------------------------------------- \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:8; Training B:True; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 1 ;batch: 0 ;gs: 1486 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.565 ;A loss: 0.185 ;edits ratio: 0.253 ;acc: 0.320 ;iou: 0.420 ;time: 0:00:00\n",
      "Edit: False ;batch: 50 ;gs: 1536 ;Blr: 0.050 ;B loss: 1.584 ;acc: 0.695 ;iou: 0.770 ;time: 0:00:15\n",
      "Edit: True ;delta: 1 ;batch: 100 ;gs: 1586 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.814 ;A loss: 0.296 ;edits ratio: 0.327 ;acc: 0.295 ;iou: 0.395 ;time: 0:00:29\n",
      "Edit: True ;delta: 1 ;batch: 150 ;gs: 1636 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.012 ;A loss: 0.372 ;edits ratio: 0.434 ;acc: 0.395 ;iou: 0.490 ;time: 0:00:44\n",
      "Edit: True ;delta: 1 ;batch: 200 ;gs: 1686 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.622 ;A loss: 0.227 ;edits ratio: 0.095 ;acc: 0.375 ;iou: 0.555 ;time: 0:00:59\n",
      "Edit: False ;batch: 250 ;gs: 1736 ;Blr: 0.050 ;B loss: 1.613 ;acc: 0.660 ;iou: 0.745 ;time: 0:01:14\n",
      "\n",
      "*B Train loss: 2.203 ;A Train loss: 0.316 ;Edit ratio: 0.311 ;Train accuracy: 0.418 ;IOU accuracy: 0.535 ;Time: 0:01:29 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;batch: 0 ;gs: 1782 ;Blr: 0.050 ;B loss: 2.358 ;acc: 0.480 ;iou: 0.580 ;time: 0:01:29\n",
      "Edit: False ;batch: 50 ;gs: 1782 ;Blr: 0.050 ;B loss: 1.537 ;acc: 0.700 ;iou: 0.780 ;time: 0:01:36\n",
      "Edit: False ;batch: 100 ;gs: 1782 ;Blr: 0.050 ;B loss: 2.518 ;acc: 0.440 ;iou: 0.570 ;time: 0:01:43\n",
      "Edit: False ;batch: 150 ;gs: 1782 ;Blr: 0.050 ;B loss: 1.693 ;acc: 0.725 ;iou: 0.780 ;time: 0:01:52\n",
      "Edit: False ;batch: 200 ;gs: 1782 ;Blr: 0.050 ;B loss: 2.505 ;acc: 0.430 ;iou: 0.615 ;time: 0:01:59\n",
      "Edit: False ;batch: 250 ;gs: 1782 ;Blr: 0.050 ;B loss: 1.614 ;acc: 0.675 ;iou: 0.755 ;time: 0:02:07\n",
      "\n",
      "*B Train loss: 2.002 ;Train accuracy: 0.582 ;IOU accuracy: 0.694 ;Time: 0:02:15 \n",
      "\n",
      "TESTING, ephoc: 8\n",
      "# t_data size: 59400\n",
      "# t_data batches: 297\n",
      "batch: 0 ;B loss: 3.182 ;acc: 0.240 ;iou_acc: 0.435 ;time: 0:02:15\n",
      "batch: 50 ;B loss: 1.887 ;acc: 0.600 ;iou_acc: 0.675 ;time: 0:02:22\n",
      "batch: 100 ;B loss: 2.977 ;acc: 0.300 ;iou_acc: 0.475 ;time: 0:02:28\n",
      "batch: 150 ;B loss: 1.333 ;acc: 0.800 ;iou_acc: 0.875 ;time: 0:02:35\n",
      "batch: 200 ;B loss: 2.711 ;acc: 0.400 ;iou_acc: 0.540 ;time: 0:02:41\n",
      "batch: 250 ;B loss: 2.319 ;acc: 0.465 ;iou_acc: 0.595 ;time: 0:02:48\n",
      "\n",
      "*Test loss: 2.012 ;Test accuracy 0.576 ;IOU accuracy: 0.689 ;Time: 0:02:54\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:9; Training B:False; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 2 ;batch: 0 ;gs: 1782 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 2.793 ;A loss: 0.181 ;edits ratio: 0.116 ;acc: 0.325 ;iou: 0.510 ;time: 0:00:00\n",
      "Edit: True ;delta: 2 ;batch: 50 ;gs: 1782 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 1.594 ;A loss: 0.496 ;edits ratio: 0.203 ;acc: 0.785 ;iou: 0.870 ;time: 0:00:11\n",
      "Edit: True ;delta: 2 ;batch: 100 ;gs: 1782 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 1.590 ;A loss: 0.430 ;edits ratio: 0.180 ;acc: 0.755 ;iou: 0.845 ;time: 0:00:21\n",
      "Edit: True ;delta: 2 ;batch: 150 ;gs: 1782 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 2.429 ;A loss: 0.181 ;edits ratio: 0.169 ;acc: 0.415 ;iou: 0.525 ;time: 0:00:32\n",
      "Edit: True ;delta: 2 ;batch: 200 ;gs: 1782 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 1.701 ;A loss: 0.315 ;edits ratio: 0.177 ;acc: 0.570 ;iou: 0.680 ;time: 0:00:42\n",
      "Edit: True ;delta: 2 ;batch: 250 ;gs: 1782 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 1.0 ;B loss: 3.045 ;A loss: 0.172 ;edits ratio: 0.165 ;acc: 0.235 ;iou: 0.425 ;time: 0:00:53\n",
      "\n",
      "*B Train loss: 2.063 ;A Train loss: 0.260 ;Edit ratio: 0.165 ;Train accuracy: 0.526 ;IOU accuracy: 0.636 ;Time: 0:01:02 \n",
      "\n",
      "VALIDATION, ephoc: 9\n",
      "--------------------\n",
      "\n",
      "Random choice: False\n",
      "batch: 0 ;edits ratio: 0.400 ;B loss: 1.753 ;A loss: 0.460 ;acc: 0.445 ;iou_acc: 0.550 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.421 ;B loss: 1.906 ;A loss: 0.422 ;acc: 0.395 ;iou_acc: 0.475 ;time: 0:00:10\n",
      "batch: 100 ;edits ratio: 0.416 ;B loss: 2.606 ;A loss: 0.365 ;acc: 0.260 ;iou_acc: 0.365 ;time: 0:00:20\n",
      "\n",
      "*B loss: 2.242 ;A loss: 0.367 ;Edit ratio: 0.400 ;Accuracy 0.375 ;IOU accuracy: 0.483 ;Time: 0:00:30\n",
      "\n",
      "Random choice: True\n",
      "batch: 0 ;edits ratio: 0.344 ;B loss: 1.624 ;A loss: 0.388 ;acc: 0.580 ;iou_acc: 0.680 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.358 ;B loss: 1.793 ;A loss: 0.352 ;acc: 0.545 ;iou_acc: 0.555 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.321 ;B loss: 2.422 ;A loss: 0.280 ;acc: 0.370 ;iou_acc: 0.460 ;time: 0:00:20\n",
      "\n",
      "*B loss: 2.116 ;A loss: 0.358 ;Edit ratio: 0.335 ;Accuracy 0.468 ;IOU accuracy: 0.579 ;Time: 0:00:29\n",
      "\n",
      "*AlrDecreaseNum: 0 ; maxDecreaseNum: 3 ;AGain: 0.126 ;toTrainA: True\n",
      "-------------------------------------------------------------------- \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:10; Training B:True; A is a know-it-all:False; ActivateA:True\n",
      "Edit: False ;batch: 0 ;gs: 1783 ;Blr: 0.050 ;B loss: 3.148 ;acc: 0.315 ;iou: 0.465 ;time: 0:00:00\n",
      "Edit: True ;delta: 2 ;batch: 50 ;gs: 1833 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.999 ;A loss: 0.269 ;edits ratio: 0.442 ;acc: 0.185 ;iou: 0.325 ;time: 0:00:16\n",
      "Edit: True ;delta: 2 ;batch: 100 ;gs: 1883 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.692 ;A loss: 0.451 ;edits ratio: 0.441 ;acc: 0.430 ;iou: 0.535 ;time: 0:00:30\n",
      "Edit: True ;delta: 2 ;batch: 150 ;gs: 1933 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.544 ;A loss: 0.286 ;edits ratio: 0.441 ;acc: 0.245 ;iou: 0.355 ;time: 0:00:46\n",
      "Edit: True ;delta: 2 ;batch: 200 ;gs: 1983 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 3.140 ;A loss: 0.369 ;edits ratio: 0.443 ;acc: 0.155 ;iou: 0.215 ;time: 0:01:01\n",
      "Edit: False ;batch: 250 ;gs: 2033 ;Blr: 0.050 ;B loss: 1.322 ;acc: 0.795 ;iou: 0.865 ;time: 0:01:16\n",
      "\n",
      "*B Train loss: 2.256 ;A Train loss: 0.345 ;Edit ratio: 0.438 ;Train accuracy: 0.371 ;IOU accuracy: 0.481 ;Time: 0:01:29 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;batch: 0 ;gs: 2079 ;Blr: 0.050 ;B loss: 3.153 ;acc: 0.305 ;iou: 0.500 ;time: 0:01:30\n",
      "Edit: False ;batch: 50 ;gs: 2079 ;Blr: 0.050 ;B loss: 2.609 ;acc: 0.440 ;iou: 0.610 ;time: 0:01:38\n",
      "Edit: False ;batch: 100 ;gs: 2079 ;Blr: 0.050 ;B loss: 1.438 ;acc: 0.710 ;iou: 0.820 ;time: 0:01:46\n",
      "Edit: False ;batch: 150 ;gs: 2079 ;Blr: 0.050 ;B loss: 2.206 ;acc: 0.540 ;iou: 0.625 ;time: 0:01:54\n",
      "Edit: False ;batch: 200 ;gs: 2079 ;Blr: 0.050 ;B loss: 2.715 ;acc: 0.360 ;iou: 0.505 ;time: 0:02:02\n",
      "Edit: False ;batch: 250 ;gs: 2079 ;Blr: 0.050 ;B loss: 1.319 ;acc: 0.825 ;iou: 0.890 ;time: 0:02:10\n",
      "\n",
      "*B Train loss: 1.994 ;Train accuracy: 0.593 ;IOU accuracy: 0.704 ;Time: 0:02:16 \n",
      "\n",
      "TESTING, ephoc: 10\n",
      "# t_data size: 59400\n",
      "# t_data batches: 297\n",
      "batch: 0 ;B loss: 3.178 ;acc: 0.270 ;iou_acc: 0.475 ;time: 0:02:17\n",
      "batch: 50 ;B loss: 1.889 ;acc: 0.630 ;iou_acc: 0.695 ;time: 0:02:24\n",
      "batch: 100 ;B loss: 2.980 ;acc: 0.280 ;iou_acc: 0.485 ;time: 0:02:30\n",
      "batch: 150 ;B loss: 1.326 ;acc: 0.810 ;iou_acc: 0.890 ;time: 0:02:37\n",
      "batch: 200 ;B loss: 2.700 ;acc: 0.380 ;iou_acc: 0.560 ;time: 0:02:44\n",
      "batch: 250 ;B loss: 2.304 ;acc: 0.475 ;iou_acc: 0.615 ;time: 0:02:50\n",
      "\n",
      "*Test loss: 2.006 ;Test accuracy 0.585 ;IOU accuracy: 0.698 ;Time: 0:02:55\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:11; Training B:False; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 2 ;batch: 0 ;gs: 2079 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.788 ;A loss: 0.338 ;edits ratio: 0.357 ;acc: 0.535 ;iou: 0.635 ;time: 0:00:00\n",
      "Edit: True ;delta: 2 ;batch: 50 ;gs: 2079 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.430 ;A loss: 0.432 ;edits ratio: 0.262 ;acc: 0.735 ;iou: 0.785 ;time: 0:00:10\n",
      "Edit: True ;delta: 2 ;batch: 100 ;gs: 2079 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.984 ;A loss: 0.270 ;edits ratio: 0.268 ;acc: 0.235 ;iou: 0.395 ;time: 0:00:20\n",
      "Edit: True ;delta: 2 ;batch: 150 ;gs: 2079 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.981 ;A loss: 0.236 ;edits ratio: 0.174 ;acc: 0.250 ;iou: 0.450 ;time: 0:00:30\n",
      "Edit: True ;delta: 2 ;batch: 200 ;gs: 2079 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.851 ;A loss: 0.235 ;edits ratio: 0.267 ;acc: 0.270 ;iou: 0.405 ;time: 0:00:41\n",
      "Edit: True ;delta: 2 ;batch: 250 ;gs: 2079 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.293 ;A loss: 0.300 ;edits ratio: 0.357 ;acc: 0.360 ;iou: 0.475 ;time: 0:00:51\n",
      "\n",
      "*B Train loss: 2.136 ;A Train loss: 0.308 ;Edit ratio: 0.283 ;Train accuracy: 0.457 ;IOU accuracy: 0.569 ;Time: 0:01:01 \n",
      "\n",
      "VALIDATION, ephoc: 11\n",
      "---------------------\n",
      "\n",
      "Random choice: False\n",
      "batch: 0 ;edits ratio: 0.470 ;B loss: 1.734 ;A loss: 0.430 ;acc: 0.445 ;iou_acc: 0.560 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.496 ;B loss: 1.877 ;A loss: 0.382 ;acc: 0.400 ;iou_acc: 0.490 ;time: 0:00:10\n",
      "batch: 100 ;edits ratio: 0.440 ;B loss: 2.580 ;A loss: 0.293 ;acc: 0.250 ;iou_acc: 0.370 ;time: 0:00:20\n",
      "\n",
      "*B loss: 2.234 ;A loss: 0.336 ;Edit ratio: 0.432 ;Accuracy 0.374 ;IOU accuracy: 0.482 ;Time: 0:00:29\n",
      "\n",
      "Random choice: True\n",
      "batch: 0 ;edits ratio: 0.374 ;B loss: 1.622 ;A loss: 0.394 ;acc: 0.550 ;iou_acc: 0.645 ;time: 0:00:01\n",
      "batch: 50 ;edits ratio: 0.387 ;B loss: 1.800 ;A loss: 0.314 ;acc: 0.515 ;iou_acc: 0.635 ;time: 0:00:10\n",
      "batch: 100 ;edits ratio: 0.334 ;B loss: 2.402 ;A loss: 0.226 ;acc: 0.375 ;iou_acc: 0.475 ;time: 0:00:20\n",
      "\n",
      "*B loss: 2.112 ;A loss: 0.359 ;Edit ratio: 0.362 ;Accuracy 0.457 ;IOU accuracy: 0.568 ;Time: 0:00:30\n",
      "\n",
      "*AlrDecreaseNum: 0 ; maxDecreaseNum: 3 ;AGain: 0.122 ;toTrainA: True\n",
      "-------------------------------------------------------------------- \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:12; Training B:True; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 2 ;batch: 0 ;gs: 2080 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.354 ;A loss: 0.273 ;edits ratio: 0.458 ;acc: 0.305 ;iou: 0.405 ;time: 0:00:00\n",
      "Edit: True ;delta: 2 ;batch: 50 ;gs: 2130 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.491 ;A loss: 0.485 ;edits ratio: 0.452 ;acc: 0.600 ;iou: 0.685 ;time: 0:00:14\n",
      "Edit: True ;delta: 2 ;batch: 100 ;gs: 2180 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.177 ;A loss: 0.352 ;edits ratio: 0.463 ;acc: 0.370 ;iou: 0.470 ;time: 0:00:28\n",
      "Edit: True ;delta: 2 ;batch: 150 ;gs: 2230 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.489 ;A loss: 0.505 ;edits ratio: 0.436 ;acc: 0.615 ;iou: 0.710 ;time: 0:00:44\n",
      "Edit: True ;delta: 2 ;batch: 200 ;gs: 2280 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.735 ;A loss: 0.475 ;edits ratio: 0.533 ;acc: 0.465 ;iou: 0.550 ;time: 0:00:59\n",
      "Edit: True ;delta: 2 ;batch: 250 ;gs: 2330 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.933 ;A loss: 0.360 ;edits ratio: 0.558 ;acc: 0.375 ;iou: 0.500 ;time: 0:01:12\n",
      "\n",
      "*B Train loss: 2.215 ;A Train loss: 0.342 ;Edit ratio: 0.427 ;Train accuracy: 0.395 ;IOU accuracy: 0.505 ;Time: 0:01:25 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;batch: 0 ;gs: 2376 ;Blr: 0.050 ;B loss: 1.995 ;acc: 0.590 ;iou: 0.680 ;time: 0:01:25\n",
      "Edit: False ;batch: 50 ;gs: 2376 ;Blr: 0.050 ;B loss: 1.284 ;acc: 0.810 ;iou: 0.885 ;time: 0:01:32\n",
      "Edit: False ;batch: 100 ;gs: 2376 ;Blr: 0.050 ;B loss: 1.834 ;acc: 0.615 ;iou: 0.705 ;time: 0:01:40\n",
      "Edit: False ;batch: 150 ;gs: 2376 ;Blr: 0.050 ;B loss: 1.266 ;acc: 0.820 ;iou: 0.880 ;time: 0:01:48\n",
      "Edit: False ;batch: 200 ;gs: 2376 ;Blr: 0.050 ;B loss: 1.402 ;acc: 0.760 ;iou: 0.830 ;time: 0:01:56\n",
      "Edit: False ;batch: 250 ;gs: 2376 ;Blr: 0.050 ;B loss: 1.620 ;acc: 0.665 ;iou: 0.785 ;time: 0:02:04\n",
      "\n",
      "*B Train loss: 1.945 ;Train accuracy: 0.614 ;IOU accuracy: 0.724 ;Time: 0:02:11 \n",
      "\n",
      "TESTING, ephoc: 12\n",
      "# t_data size: 59400\n",
      "# t_data batches: 297\n",
      "batch: 0 ;B loss: 3.099 ;acc: 0.280 ;iou_acc: 0.480 ;time: 0:02:12\n",
      "batch: 50 ;B loss: 1.821 ;acc: 0.620 ;iou_acc: 0.710 ;time: 0:02:19\n",
      "batch: 100 ;B loss: 2.909 ;acc: 0.295 ;iou_acc: 0.490 ;time: 0:02:25\n",
      "batch: 150 ;B loss: 1.332 ;acc: 0.830 ;iou_acc: 0.895 ;time: 0:02:32\n",
      "batch: 200 ;B loss: 2.637 ;acc: 0.375 ;iou_acc: 0.515 ;time: 0:02:39\n",
      "batch: 250 ;B loss: 2.243 ;acc: 0.535 ;iou_acc: 0.650 ;time: 0:02:46\n",
      "\n",
      "*Test loss: 1.958 ;Test accuracy 0.604 ;IOU accuracy: 0.716 ;Time: 0:02:52\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:13; Training B:False; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 2 ;batch: 0 ;gs: 2376 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.283 ;A loss: 0.299 ;edits ratio: 0.268 ;acc: 0.390 ;iou: 0.515 ;time: 0:00:00\n",
      "Edit: True ;delta: 2 ;batch: 50 ;gs: 2376 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.146 ;A loss: 0.268 ;edits ratio: 0.283 ;acc: 0.440 ;iou: 0.550 ;time: 0:00:11\n",
      "Edit: True ;delta: 2 ;batch: 100 ;gs: 2376 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.846 ;A loss: 0.268 ;edits ratio: 0.292 ;acc: 0.265 ;iou: 0.425 ;time: 0:00:21\n",
      "Edit: True ;delta: 2 ;batch: 150 ;gs: 2376 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 3.209 ;A loss: 0.267 ;edits ratio: 0.254 ;acc: 0.230 ;iou: 0.410 ;time: 0:00:31\n",
      "Edit: True ;delta: 2 ;batch: 200 ;gs: 2376 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.419 ;A loss: 0.452 ;edits ratio: 0.282 ;acc: 0.675 ;iou: 0.770 ;time: 0:00:42\n",
      "Edit: True ;delta: 2 ;batch: 250 ;gs: 2376 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.648 ;A loss: 0.350 ;edits ratio: 0.355 ;acc: 0.760 ;iou: 0.850 ;time: 0:00:53\n",
      "\n",
      "*B Train loss: 2.097 ;A Train loss: 0.313 ;Edit ratio: 0.288 ;Train accuracy: 0.480 ;IOU accuracy: 0.592 ;Time: 0:01:03 \n",
      "\n",
      "VALIDATION, ephoc: 13\n",
      "---------------------\n",
      "\n",
      "Random choice: False\n",
      "batch: 0 ;edits ratio: 0.455 ;B loss: 1.728 ;A loss: 0.433 ;acc: 0.490 ;iou_acc: 0.600 ;time: 0:00:01\n",
      "batch: 50 ;edits ratio: 0.497 ;B loss: 1.815 ;A loss: 0.386 ;acc: 0.480 ;iou_acc: 0.560 ;time: 0:00:10\n",
      "batch: 100 ;edits ratio: 0.463 ;B loss: 2.606 ;A loss: 0.325 ;acc: 0.240 ;iou_acc: 0.380 ;time: 0:00:20\n",
      "\n",
      "*B loss: 2.215 ;A loss: 0.356 ;Edit ratio: 0.460 ;Accuracy 0.388 ;IOU accuracy: 0.495 ;Time: 0:00:30\n",
      "\n",
      "Random choice: True\n",
      "batch: 0 ;edits ratio: 0.421 ;B loss: 1.604 ;A loss: 0.488 ;acc: 0.595 ;iou_acc: 0.670 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.418 ;B loss: 1.763 ;A loss: 0.333 ;acc: 0.530 ;iou_acc: 0.595 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.354 ;B loss: 2.387 ;A loss: 0.300 ;acc: 0.395 ;iou_acc: 0.520 ;time: 0:00:19\n",
      "\n",
      "*B loss: 2.095 ;A loss: 0.375 ;Edit ratio: 0.385 ;Accuracy 0.470 ;IOU accuracy: 0.578 ;Time: 0:00:29\n",
      "\n",
      "*AlrDecreaseNum: 0 ; maxDecreaseNum: 3 ;AGain: 0.120 ;toTrainA: True\n",
      "-------------------------------------------------------------------- \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:14; Training B:True; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 2 ;batch: 0 ;gs: 2377 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.664 ;A loss: 0.305 ;edits ratio: 0.516 ;acc: 0.235 ;iou: 0.375 ;time: 0:00:00\n",
      "Edit: True ;delta: 2 ;batch: 50 ;gs: 2427 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.624 ;A loss: 0.290 ;edits ratio: 0.462 ;acc: 0.710 ;iou: 0.845 ;time: 0:00:16\n",
      "Edit: True ;delta: 2 ;batch: 100 ;gs: 2477 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.572 ;A loss: 0.217 ;edits ratio: 0.123 ;acc: 0.365 ;iou: 0.585 ;time: 0:00:30\n",
      "Edit: False ;batch: 150 ;gs: 2527 ;Blr: 0.050 ;B loss: 2.710 ;acc: 0.385 ;iou: 0.590 ;time: 0:00:45\n",
      "Edit: True ;delta: 2 ;batch: 200 ;gs: 2577 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.318 ;A loss: 0.324 ;edits ratio: 0.384 ;acc: 0.375 ;iou: 0.490 ;time: 0:01:00\n",
      "Edit: True ;delta: 2 ;batch: 250 ;gs: 2627 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 3.184 ;A loss: 0.257 ;edits ratio: 0.349 ;acc: 0.155 ;iou: 0.305 ;time: 0:01:14\n",
      "\n",
      "*B Train loss: 2.197 ;A Train loss: 0.343 ;Edit ratio: 0.430 ;Train accuracy: 0.403 ;IOU accuracy: 0.512 ;Time: 0:01:29 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;batch: 0 ;gs: 2673 ;Blr: 0.050 ;B loss: 2.221 ;acc: 0.510 ;iou: 0.690 ;time: 0:01:29\n",
      "Edit: False ;batch: 50 ;gs: 2673 ;Blr: 0.050 ;B loss: 1.655 ;acc: 0.870 ;iou: 0.925 ;time: 0:01:38\n",
      "Edit: False ;batch: 100 ;gs: 2673 ;Blr: 0.050 ;B loss: 2.417 ;acc: 0.445 ;iou: 0.660 ;time: 0:01:45\n",
      "Edit: False ;batch: 150 ;gs: 2673 ;Blr: 0.050 ;B loss: 2.649 ;acc: 0.365 ;iou: 0.580 ;time: 0:01:52\n",
      "Edit: False ;batch: 200 ;gs: 2673 ;Blr: 0.050 ;B loss: 2.004 ;acc: 0.640 ;iou: 0.720 ;time: 0:02:00\n",
      "Edit: False ;batch: 250 ;gs: 2673 ;Blr: 0.050 ;B loss: 2.834 ;acc: 0.410 ;iou: 0.575 ;time: 0:02:07\n",
      "\n",
      "*B Train loss: 1.911 ;Train accuracy: 0.620 ;IOU accuracy: 0.730 ;Time: 0:02:15 \n",
      "\n",
      "TESTING, ephoc: 14\n",
      "# t_data size: 59400\n",
      "# t_data batches: 297\n",
      "batch: 0 ;B loss: 3.053 ;acc: 0.285 ;iou_acc: 0.500 ;time: 0:02:16\n",
      "batch: 50 ;B loss: 1.786 ;acc: 0.620 ;iou_acc: 0.700 ;time: 0:02:23\n",
      "batch: 100 ;B loss: 2.877 ;acc: 0.310 ;iou_acc: 0.530 ;time: 0:02:29\n",
      "batch: 150 ;B loss: 1.331 ;acc: 0.820 ;iou_acc: 0.890 ;time: 0:02:36\n",
      "batch: 200 ;B loss: 2.595 ;acc: 0.390 ;iou_acc: 0.560 ;time: 0:02:42\n",
      "batch: 250 ;B loss: 2.196 ;acc: 0.550 ;iou_acc: 0.650 ;time: 0:02:49\n",
      "\n",
      "*Test loss: 1.927 ;Test accuracy 0.610 ;IOU accuracy: 0.722 ;Time: 0:02:55\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:15; Training B:False; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 3 ;batch: 0 ;gs: 2673 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.319 ;A loss: 0.289 ;edits ratio: 0.401 ;acc: 0.335 ;iou: 0.475 ;time: 0:00:00\n",
      "Edit: True ;delta: 3 ;batch: 50 ;gs: 2673 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.744 ;A loss: 0.459 ;edits ratio: 0.384 ;acc: 0.560 ;iou: 0.640 ;time: 0:00:09\n",
      "Edit: True ;delta: 3 ;batch: 100 ;gs: 2673 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.866 ;A loss: 0.258 ;edits ratio: 0.333 ;acc: 0.255 ;iou: 0.430 ;time: 0:00:20\n",
      "Edit: True ;delta: 3 ;batch: 150 ;gs: 2673 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.032 ;A loss: 0.299 ;edits ratio: 0.277 ;acc: 0.485 ;iou: 0.580 ;time: 0:00:31\n",
      "Edit: True ;delta: 3 ;batch: 200 ;gs: 2673 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.311 ;A loss: 0.334 ;edits ratio: 0.311 ;acc: 0.410 ;iou: 0.515 ;time: 0:00:43\n",
      "Edit: True ;delta: 3 ;batch: 250 ;gs: 2673 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.897 ;A loss: 0.299 ;edits ratio: 0.342 ;acc: 0.475 ;iou: 0.565 ;time: 0:00:53\n",
      "\n",
      "*B Train loss: 2.104 ;A Train loss: 0.337 ;Edit ratio: 0.321 ;Train accuracy: 0.467 ;IOU accuracy: 0.580 ;Time: 0:01:02 \n",
      "\n",
      "VALIDATION, ephoc: 15\n",
      "---------------------\n",
      "\n",
      "Random choice: False\n",
      "batch: 0 ;edits ratio: 0.443 ;B loss: 1.728 ;A loss: 0.434 ;acc: 0.495 ;iou_acc: 0.580 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.463 ;B loss: 1.804 ;A loss: 0.394 ;acc: 0.495 ;iou_acc: 0.590 ;time: 0:00:10\n",
      "batch: 100 ;edits ratio: 0.502 ;B loss: 2.599 ;A loss: 0.329 ;acc: 0.245 ;iou_acc: 0.385 ;time: 0:00:20\n",
      "\n",
      "*B loss: 2.232 ;A loss: 0.359 ;Edit ratio: 0.449 ;Accuracy 0.383 ;IOU accuracy: 0.493 ;Time: 0:00:30\n",
      "\n",
      "Random choice: True\n",
      "batch: 0 ;edits ratio: 0.385 ;B loss: 1.619 ;A loss: 0.382 ;acc: 0.540 ;iou_acc: 0.690 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.415 ;B loss: 1.758 ;A loss: 0.383 ;acc: 0.550 ;iou_acc: 0.640 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.421 ;B loss: 2.451 ;A loss: 0.366 ;acc: 0.370 ;iou_acc: 0.490 ;time: 0:00:19\n",
      "\n",
      "*B loss: 2.094 ;A loss: 0.403 ;Edit ratio: 0.409 ;Accuracy 0.461 ;IOU accuracy: 0.579 ;Time: 0:00:29\n",
      "\n",
      "*AlrDecreaseNum: 0 ; maxDecreaseNum: 3 ;AGain: 0.138 ;toTrainA: True\n",
      "-------------------------------------------------------------------- \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:16; Training B:True; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 3 ;batch: 0 ;gs: 2674 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.251 ;A loss: 0.370 ;edits ratio: 0.509 ;acc: 0.315 ;iou: 0.390 ;time: 0:00:00\n",
      "Edit: True ;delta: 3 ;batch: 50 ;gs: 2724 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.524 ;A loss: 0.302 ;edits ratio: 0.074 ;acc: 0.720 ;iou: 0.800 ;time: 0:00:15\n",
      "Edit: True ;delta: 3 ;batch: 100 ;gs: 2774 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.591 ;A loss: 0.529 ;edits ratio: 0.492 ;acc: 0.560 ;iou: 0.650 ;time: 0:00:30\n",
      "Edit: True ;delta: 3 ;batch: 150 ;gs: 2824 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.933 ;A loss: 0.393 ;edits ratio: 0.561 ;acc: 0.370 ;iou: 0.505 ;time: 0:00:45\n",
      "Edit: True ;delta: 3 ;batch: 200 ;gs: 2874 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 3.285 ;A loss: 0.239 ;edits ratio: 0.370 ;acc: 0.165 ;iou: 0.375 ;time: 0:01:00\n",
      "Edit: True ;delta: 3 ;batch: 250 ;gs: 2924 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.665 ;A loss: 0.268 ;edits ratio: 0.460 ;acc: 0.265 ;iou: 0.415 ;time: 0:01:15\n",
      "\n",
      "*B Train loss: 2.208 ;A Train loss: 0.353 ;Edit ratio: 0.469 ;Train accuracy: 0.399 ;IOU accuracy: 0.505 ;Time: 0:01:29 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;batch: 0 ;gs: 2970 ;Blr: 0.050 ;B loss: 1.856 ;acc: 0.640 ;iou: 0.725 ;time: 0:01:29\n",
      "Edit: False ;batch: 50 ;gs: 2970 ;Blr: 0.050 ;B loss: 1.415 ;acc: 0.760 ;iou: 0.840 ;time: 0:01:37\n",
      "Edit: False ;batch: 100 ;gs: 2970 ;Blr: 0.050 ;B loss: 1.268 ;acc: 0.830 ;iou: 0.895 ;time: 0:01:44\n",
      "Edit: False ;batch: 150 ;gs: 2970 ;Blr: 0.050 ;B loss: 1.581 ;acc: 0.680 ;iou: 0.780 ;time: 0:01:52\n",
      "Edit: False ;batch: 200 ;gs: 2970 ;Blr: 0.050 ;B loss: 2.950 ;acc: 0.420 ;iou: 0.640 ;time: 0:02:00\n",
      "Edit: False ;batch: 250 ;gs: 2970 ;Blr: 0.050 ;B loss: 2.283 ;acc: 0.460 ;iou: 0.650 ;time: 0:02:08\n",
      "\n",
      "*B Train loss: 1.893 ;Train accuracy: 0.632 ;IOU accuracy: 0.742 ;Time: 0:02:15 \n",
      "\n",
      "TESTING, ephoc: 16\n",
      "# t_data size: 59400\n",
      "# t_data batches: 297\n",
      "batch: 0 ;B loss: 3.009 ;acc: 0.295 ;iou_acc: 0.515 ;time: 0:02:16\n",
      "batch: 50 ;B loss: 1.761 ;acc: 0.675 ;iou_acc: 0.750 ;time: 0:02:23\n",
      "batch: 100 ;B loss: 2.841 ;acc: 0.320 ;iou_acc: 0.560 ;time: 0:02:28\n",
      "batch: 150 ;B loss: 1.327 ;acc: 0.855 ;iou_acc: 0.905 ;time: 0:02:35\n",
      "batch: 200 ;B loss: 2.575 ;acc: 0.400 ;iou_acc: 0.545 ;time: 0:02:41\n",
      "batch: 250 ;B loss: 2.179 ;acc: 0.565 ;iou_acc: 0.675 ;time: 0:02:47\n",
      "\n",
      "*Test loss: 1.910 ;Test accuracy 0.621 ;IOU accuracy: 0.731 ;Time: 0:02:53\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:17; Training B:False; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 3 ;batch: 0 ;gs: 2970 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 1.518 ;A loss: 0.537 ;edits ratio: 0.317 ;acc: 0.660 ;iou: 0.700 ;time: 0:00:01\n",
      "Edit: True ;delta: 3 ;batch: 50 ;gs: 2970 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 2.396 ;A loss: 0.259 ;edits ratio: 0.426 ;acc: 0.380 ;iou: 0.470 ;time: 0:00:11\n",
      "Edit: True ;delta: 3 ;batch: 100 ;gs: 2970 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 2.461 ;A loss: 0.304 ;edits ratio: 0.200 ;acc: 0.450 ;iou: 0.570 ;time: 0:00:22\n",
      "Edit: True ;delta: 3 ;batch: 150 ;gs: 2970 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 3.277 ;A loss: 0.253 ;edits ratio: 0.410 ;acc: 0.190 ;iou: 0.310 ;time: 0:00:32\n",
      "Edit: True ;delta: 3 ;batch: 200 ;gs: 2970 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 1.469 ;A loss: 0.512 ;edits ratio: 0.327 ;acc: 0.660 ;iou: 0.695 ;time: 0:00:42\n",
      "Edit: True ;delta: 3 ;batch: 250 ;gs: 2970 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 2.187 ;A loss: 0.352 ;edits ratio: 0.379 ;acc: 0.405 ;iou: 0.530 ;time: 0:00:52\n",
      "\n",
      "*B Train loss: 2.132 ;A Train loss: 0.342 ;Edit ratio: 0.365 ;Train accuracy: 0.444 ;IOU accuracy: 0.552 ;Time: 0:01:00 \n",
      "\n",
      "VALIDATION, ephoc: 17\n",
      "---------------------\n",
      "\n",
      "Random choice: False\n",
      "batch: 0 ;edits ratio: 0.457 ;B loss: 1.707 ;A loss: 0.436 ;acc: 0.495 ;iou_acc: 0.595 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.459 ;B loss: 1.792 ;A loss: 0.409 ;acc: 0.520 ;iou_acc: 0.600 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.496 ;B loss: 2.562 ;A loss: 0.306 ;acc: 0.265 ;iou_acc: 0.395 ;time: 0:00:18\n",
      "\n",
      "*B loss: 2.216 ;A loss: 0.352 ;Edit ratio: 0.466 ;Accuracy 0.375 ;IOU accuracy: 0.481 ;Time: 0:00:26\n",
      "\n",
      "Random choice: True\n",
      "batch: 0 ;edits ratio: 0.433 ;B loss: 1.632 ;A loss: 0.441 ;acc: 0.575 ;iou_acc: 0.660 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.422 ;B loss: 1.866 ;A loss: 0.413 ;acc: 0.535 ;iou_acc: 0.650 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.425 ;B loss: 2.399 ;A loss: 0.353 ;acc: 0.395 ;iou_acc: 0.475 ;time: 0:00:19\n",
      "\n",
      "*B loss: 2.093 ;A loss: 0.397 ;Edit ratio: 0.423 ;Accuracy 0.464 ;IOU accuracy: 0.570 ;Time: 0:00:29\n",
      "\n",
      "*AlrDecreaseNum: 0 ; maxDecreaseNum: 3 ;AGain: 0.123 ;toTrainA: True\n",
      "-------------------------------------------------------------------- \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:18; Training B:True; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;delta: 3 ;batch: 0 ;gs: 2971 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.277 ;A loss: 0.357 ;edits ratio: 0.508 ;acc: 0.380 ;iou: 0.475 ;time: 0:00:00\n",
      "Edit: True ;delta: 3 ;batch: 50 ;gs: 3021 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.824 ;A loss: 0.239 ;edits ratio: 0.220 ;acc: 0.350 ;iou: 0.510 ;time: 0:00:15\n",
      "Edit: True ;delta: 3 ;batch: 100 ;gs: 3071 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.858 ;A loss: 0.405 ;edits ratio: 0.536 ;acc: 0.450 ;iou: 0.525 ;time: 0:00:29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a5a2b6cdf6cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                 \u001b[0mBlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                 \u001b[0mAlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                 imScale=1/50)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-41a26aee9ccd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trn_data, tst_data, val_data, ephocs_num, edit_reward, startA, activation_ephoc, muteBnum, start_ephoc, dropout_in, rnn_editProb, dropout_out, dropout_img, dropout_q, queries_editProb, addNoise, imScale, Alr, Blr)\u001b[0m\n\u001b[1;32m    966\u001b[0m                                 gs, scores, batch_Alr, batch_Blr, B_loss, batch_edit_ratio, A_loss, _ = sess.run([self.global_step, self.scores, self.A_learning_rate, self.learning_rate,\n\u001b[1;32m    967\u001b[0m                                                                                                                    self.B_loss, self.edit_ratio, self.A_loss, self.A_optimizer], \n\u001b[0;32m--> 968\u001b[0;31m                                                                                                                   feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    969\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m                                 gs, scores, batch_Blr, B_loss, batch_edit_ratio, A_loss, _ = sess.run([self.global_step, self.scores, self.learning_rate, self.B_loss, \n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('params_dir:', params_dir)\n",
    "print('num_hidden:', m.num_hidden)\n",
    "tst, trn, val, AGain = m.train(trainset, testset, valset,\n",
    "                                ephocs_num=100,\n",
    "                                start_ephoc=6,\n",
    "                                startA=1,\n",
    "                                activation_ephoc=3,\n",
    "                                muteBnum=1, \n",
    "                                queries_editProb=0.95,\n",
    "                                edit_reward=-.8,\n",
    "                                rnn_editProb=0.2,\n",
    "                                Blr=.05,\n",
    "                                Alr=.1,\n",
    "                                imScale=1/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params_dir: ../data/training/models/All/RL/testNIXER_hidden:30\n",
      "num_hidden: 30\n",
      "# Train set size: 59400\n",
      "# Training batches: 297\n",
      "# Validation set size: 29800\n",
      "# Validation batches: 149\n",
      "# Test set size: 29600\n",
      "# Testing batches: 148\n",
      "Initializing variables\n",
      "================================================== \n",
      "TRAIN, ephoc:0; Training B:True; A is a know-it-all:False; ActivateA:False\n",
      "Edit: False ;batch: 0 ;gs: 1 ;Blr: 0.050 ;B loss: 1.386 ;acc: 0.530 ;iou: 0.655 ;time: 0:00:01\n",
      "Edit: False ;batch: 50 ;gs: 51 ;Blr: 0.050 ;B loss: 2.050 ;acc: 0.195 ;iou: 0.260 ;time: 0:00:09\n",
      "Edit: False ;batch: 100 ;gs: 101 ;Blr: 0.050 ;B loss: 2.371 ;acc: 0.225 ;iou: 0.270 ;time: 0:00:18\n",
      "Edit: False ;batch: 150 ;gs: 151 ;Blr: 0.050 ;B loss: 2.127 ;acc: 0.235 ;iou: 0.280 ;time: 0:00:27\n",
      "Edit: False ;batch: 200 ;gs: 201 ;Blr: 0.050 ;B loss: 2.113 ;acc: 0.215 ;iou: 0.250 ;time: 0:00:36\n",
      "Edit: False ;batch: 250 ;gs: 251 ;Blr: 0.050 ;B loss: 2.642 ;acc: 0.120 ;iou: 0.210 ;time: 0:00:45\n",
      "\n",
      "*B Train loss: 2.430 ;Train accuracy: 0.200 ;IOU accuracy: 0.284 ;Time: 0:00:53 \n",
      "\n",
      "TESTING, ephoc: 0\n",
      "# t_data size: 59400\n",
      "# t_data batches: 297\n",
      "batch: 0 ;B loss: 3.653 ;acc: 0.020 ;iou_acc: 0.090 ;time: 0:00:54\n",
      "batch: 50 ;B loss: 2.418 ;acc: 0.120 ;iou_acc: 0.165 ;time: 0:01:00\n",
      "batch: 100 ;B loss: 3.427 ;acc: 0.100 ;iou_acc: 0.215 ;time: 0:01:06\n",
      "batch: 150 ;B loss: 1.520 ;acc: 0.325 ;iou_acc: 0.410 ;time: 0:01:13\n",
      "batch: 200 ;B loss: 3.194 ;acc: 0.095 ;iou_acc: 0.190 ;time: 0:01:19\n",
      "batch: 250 ;B loss: 2.870 ;acc: 0.150 ;iou_acc: 0.200 ;time: 0:01:25\n",
      "\n",
      "*Test loss: 2.434 ;Test accuracy 0.207 ;IOU accuracy: 0.292 ;Time: 0:01:31\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:1; Training B:False; A is a know-it-all:False; ActivateA:False\n",
      "Edit: True ;batch: 0 ;gs: 297 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.387 ;A loss: 0.846 ;edits ratio: 0.378 ;acc: 0.580 ;iou: 0.700 ;time: 0:00:00\n",
      "Edit: True ;batch: 50 ;gs: 297 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.990 ;A loss: 0.201 ;edits ratio: 0.101 ;acc: 0.100 ;iou: 0.190 ;time: 0:00:10\n",
      "Edit: True ;batch: 100 ;gs: 297 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 3.569 ;A loss: 0.310 ;edits ratio: 0.172 ;acc: 0.050 ;iou: 0.175 ;time: 0:00:21\n",
      "Edit: True ;batch: 150 ;gs: 297 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 3.195 ;A loss: 0.190 ;edits ratio: 0.102 ;acc: 0.105 ;iou: 0.195 ;time: 0:00:31\n",
      "Edit: True ;batch: 200 ;gs: 297 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.166 ;A loss: 0.117 ;edits ratio: 0.141 ;acc: 0.250 ;iou: 0.320 ;time: 0:00:42\n",
      "Edit: True ;batch: 250 ;gs: 297 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.542 ;A loss: 0.177 ;edits ratio: 0.151 ;acc: 0.390 ;iou: 0.480 ;time: 0:00:52\n",
      "\n",
      "*B Train loss: 2.429 ;A Train loss: 0.197 ;Edit ratio: 0.135 ;Train accuracy: 0.207 ;IOU accuracy: 0.289 ;Time: 0:01:01 \n",
      "\n",
      "VALIDATION, ephoc: 1\n",
      "--------------------\n",
      "\n",
      "Random choice: False\n",
      "batch: 0 ;edits ratio: 0.152 ;B loss: 1.867 ;A loss: 0.092 ;acc: 0.285 ;iou_acc: 0.360 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.160 ;B loss: 2.099 ;A loss: 0.085 ;acc: 0.225 ;iou_acc: 0.290 ;time: 0:00:10\n",
      "batch: 100 ;edits ratio: 0.082 ;B loss: 2.784 ;A loss: 0.150 ;acc: 0.125 ;iou_acc: 0.185 ;time: 0:00:19\n",
      "\n",
      "*B loss: 2.398 ;A loss: 0.122 ;Edit ratio: 0.122 ;Accuracy 0.214 ;IOU accuracy: 0.297 ;Time: 0:00:28\n",
      "\n",
      "Random choice: True\n",
      "batch: 0 ;edits ratio: 0.108 ;B loss: 1.868 ;A loss: 0.146 ;acc: 0.265 ;iou_acc: 0.350 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.097 ;B loss: 2.100 ;A loss: 0.138 ;acc: 0.225 ;iou_acc: 0.300 ;time: 0:00:08\n",
      "batch: 100 ;edits ratio: 0.090 ;B loss: 2.784 ;A loss: 0.158 ;acc: 0.130 ;iou_acc: 0.170 ;time: 0:00:18\n",
      "\n",
      "*B loss: 2.398 ;A loss: 0.155 ;Edit ratio: 0.101 ;Accuracy 0.214 ;IOU accuracy: 0.298 ;Time: 0:00:27\n",
      "\n",
      "*AlrDecreaseNum: 0 ; maxDecreaseNum: 3 ;AGain: -0.000 ;toTrainA: True\n",
      "--------------------------------------------------------------------- \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:2; Training B:True; A is a know-it-all:False; ActivateA:False\n",
      "Edit: False ;batch: 0 ;gs: 298 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.566 ;A loss: 0.114 ;edits ratio: 0.168 ;acc: 0.160 ;iou: 0.250 ;time: 0:00:00\n",
      "Edit: False ;batch: 50 ;gs: 348 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.881 ;A loss: 0.135 ;edits ratio: 0.168 ;acc: 0.260 ;iou: 0.370 ;time: 0:00:12\n",
      "Edit: False ;batch: 100 ;gs: 398 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 3.357 ;A loss: 0.244 ;edits ratio: 0.171 ;acc: 0.055 ;iou: 0.120 ;time: 0:00:25\n",
      "Edit: False ;batch: 150 ;gs: 448 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.580 ;A loss: 0.100 ;edits ratio: 0.149 ;acc: 0.140 ;iou: 0.235 ;time: 0:00:37\n",
      "Edit: False ;batch: 200 ;gs: 498 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.840 ;A loss: 0.148 ;edits ratio: 0.192 ;acc: 0.315 ;iou: 0.380 ;time: 0:00:50\n",
      "Edit: False ;batch: 250 ;gs: 548 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 3.562 ;A loss: 0.248 ;edits ratio: 0.097 ;acc: 0.045 ;iou: 0.130 ;time: 0:01:02\n",
      "\n",
      "*B Train loss: 2.423 ;A Train loss: 0.150 ;Edit ratio: 0.156 ;Train accuracy: 0.222 ;IOU accuracy: 0.306 ;Time: 0:01:13 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;batch: 0 ;gs: 594 ;Blr: 0.050 ;B loss: 2.545 ;acc: 0.160 ;iou: 0.245 ;time: 0:01:13\n",
      "Edit: False ;batch: 50 ;gs: 594 ;Blr: 0.050 ;B loss: 1.868 ;acc: 0.310 ;iou: 0.425 ;time: 0:01:21\n",
      "Edit: False ;batch: 100 ;gs: 594 ;Blr: 0.050 ;B loss: 3.344 ;acc: 0.090 ;iou: 0.145 ;time: 0:01:29\n",
      "Edit: False ;batch: 150 ;gs: 594 ;Blr: 0.050 ;B loss: 2.564 ;acc: 0.195 ;iou: 0.290 ;time: 0:01:37\n",
      "Edit: False ;batch: 200 ;gs: 594 ;Blr: 0.050 ;B loss: 1.828 ;acc: 0.400 ;iou: 0.490 ;time: 0:01:45\n",
      "Edit: False ;batch: 250 ;gs: 594 ;Blr: 0.050 ;B loss: 3.557 ;acc: 0.055 ;iou: 0.150 ;time: 0:01:53\n",
      "\n",
      "*B Train loss: 2.412 ;Train accuracy: 0.269 ;IOU accuracy: 0.358 ;Time: 0:01:59 \n",
      "\n",
      "TESTING, ephoc: 2\n",
      "# t_data size: 59400\n",
      "# t_data batches: 297\n",
      "batch: 0 ;B loss: 3.640 ;acc: 0.045 ;iou_acc: 0.140 ;time: 0:02:00\n",
      "batch: 50 ;B loss: 2.401 ;acc: 0.200 ;iou_acc: 0.255 ;time: 0:02:06\n",
      "batch: 100 ;B loss: 3.415 ;acc: 0.095 ;iou_acc: 0.190 ;time: 0:02:12\n",
      "batch: 150 ;B loss: 1.504 ;acc: 0.490 ;iou_acc: 0.575 ;time: 0:02:19\n",
      "batch: 200 ;B loss: 3.177 ;acc: 0.110 ;iou_acc: 0.195 ;time: 0:02:26\n",
      "batch: 250 ;B loss: 2.846 ;acc: 0.185 ;iou_acc: 0.245 ;time: 0:02:32\n",
      "\n",
      "*Test loss: 2.417 ;Test accuracy 0.263 ;IOU accuracy: 0.354 ;Time: 0:02:39\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:3; Training B:False; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;batch: 0 ;gs: 594 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.687 ;A loss: 0.092 ;edits ratio: 0.072 ;acc: 0.140 ;iou: 0.215 ;time: 0:00:00\n",
      "Edit: True ;batch: 50 ;gs: 594 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 3.323 ;A loss: 0.220 ;edits ratio: 0.107 ;acc: 0.080 ;iou: 0.125 ;time: 0:00:10\n",
      "Edit: True ;batch: 100 ;gs: 594 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.544 ;A loss: 0.118 ;edits ratio: 0.183 ;acc: 0.450 ;iou: 0.575 ;time: 0:00:22\n",
      "Edit: True ;batch: 150 ;gs: 594 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 2.695 ;A loss: 0.139 ;edits ratio: 0.252 ;acc: 0.155 ;iou: 0.205 ;time: 0:00:32\n",
      "Edit: True ;batch: 200 ;gs: 594 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.928 ;A loss: 0.133 ;edits ratio: 0.222 ;acc: 0.395 ;iou: 0.465 ;time: 0:00:42\n",
      "Edit: True ;batch: 250 ;gs: 594 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.5 ;B loss: 1.869 ;A loss: 0.090 ;edits ratio: 0.212 ;acc: 0.305 ;iou: 0.410 ;time: 0:00:52\n",
      "\n",
      "*B Train loss: 2.414 ;A Train loss: 0.134 ;Edit ratio: 0.166 ;Train accuracy: 0.258 ;IOU accuracy: 0.346 ;Time: 0:01:01 \n",
      "\n",
      "VALIDATION, ephoc: 3\n",
      "--------------------\n",
      "\n",
      "Random choice: False\n",
      "batch: 0 ;edits ratio: 0.201 ;B loss: 1.854 ;A loss: 0.052 ;acc: 0.320 ;iou_acc: 0.390 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.264 ;B loss: 2.081 ;A loss: 0.064 ;acc: 0.250 ;iou_acc: 0.360 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.181 ;B loss: 2.766 ;A loss: 0.147 ;acc: 0.165 ;iou_acc: 0.265 ;time: 0:00:18\n",
      "\n",
      "*B loss: 2.382 ;A loss: 0.111 ;Edit ratio: 0.216 ;Accuracy 0.265 ;IOU accuracy: 0.355 ;Time: 0:00:28\n",
      "\n",
      "Random choice: True\n",
      "batch: 0 ;edits ratio: 0.188 ;B loss: 1.855 ;A loss: 0.094 ;acc: 0.325 ;iou_acc: 0.405 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.176 ;B loss: 2.084 ;A loss: 0.089 ;acc: 0.265 ;iou_acc: 0.370 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.183 ;B loss: 2.766 ;A loss: 0.148 ;acc: 0.150 ;iou_acc: 0.245 ;time: 0:00:18\n",
      "\n",
      "*B loss: 2.384 ;A loss: 0.129 ;Edit ratio: 0.194 ;Accuracy 0.259 ;IOU accuracy: 0.349 ;Time: 0:00:28\n",
      "\n",
      "*AlrDecreaseNum: 0 ; maxDecreaseNum: 3 ;AGain: -0.002 ;toTrainA: True\n",
      "--------------------------------------------------------------------- \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:4; Training B:True; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;batch: 0 ;gs: 595 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.494 ;A loss: 0.076 ;edits ratio: 0.306 ;acc: 0.245 ;iou: 0.330 ;time: 0:00:00\n",
      "Edit: True ;batch: 50 ;gs: 645 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.344 ;A loss: 0.028 ;edits ratio: 0.062 ;acc: 0.255 ;iou: 0.345 ;time: 0:00:15\n",
      "Edit: True ;batch: 100 ;gs: 695 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 3.384 ;A loss: 0.221 ;edits ratio: 0.209 ;acc: 0.110 ;iou: 0.190 ;time: 0:00:30\n",
      "Edit: True ;batch: 150 ;gs: 745 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.338 ;A loss: 0.134 ;edits ratio: 0.245 ;acc: 0.325 ;iou: 0.380 ;time: 0:00:44\n",
      "Edit: True ;batch: 200 ;gs: 795 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.453 ;A loss: 0.045 ;edits ratio: 0.235 ;acc: 0.230 ;iou: 0.365 ;time: 0:00:59\n",
      "Edit: True ;batch: 250 ;gs: 845 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 2.995 ;A loss: 0.230 ;edits ratio: 0.312 ;acc: 0.170 ;iou: 0.255 ;time: 0:01:13\n",
      "\n",
      "*B Train loss: 2.382 ;A Train loss: 0.124 ;Edit ratio: 0.110 ;Train accuracy: 0.325 ;IOU accuracy: 0.422 ;Time: 0:01:25 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;batch: 0 ;gs: 891 ;Blr: 0.050 ;B loss: 2.405 ;acc: 0.350 ;iou: 0.485 ;time: 0:01:25\n",
      "Edit: False ;batch: 50 ;gs: 891 ;Blr: 0.050 ;B loss: 2.257 ;acc: 0.400 ;iou: 0.530 ;time: 0:01:32\n",
      "Edit: False ;batch: 100 ;gs: 891 ;Blr: 0.050 ;B loss: 3.303 ;acc: 0.150 ;iou: 0.270 ;time: 0:01:41\n",
      "Edit: False ;batch: 150 ;gs: 891 ;Blr: 0.050 ;B loss: 2.271 ;acc: 0.380 ;iou: 0.465 ;time: 0:01:47\n",
      "Edit: False ;batch: 200 ;gs: 891 ;Blr: 0.050 ;B loss: 2.399 ;acc: 0.295 ;iou: 0.445 ;time: 0:01:55\n",
      "Edit: False ;batch: 250 ;gs: 891 ;Blr: 0.050 ;B loss: 2.937 ;acc: 0.225 ;iou: 0.335 ;time: 0:02:03\n",
      "\n",
      "*B Train loss: 2.327 ;Train accuracy: 0.390 ;IOU accuracy: 0.497 ;Time: 0:02:09 \n",
      "\n",
      "TESTING, ephoc: 4\n",
      "# t_data size: 59400\n",
      "# t_data batches: 297\n",
      "batch: 0 ;B loss: 3.557 ;acc: 0.105 ;iou_acc: 0.215 ;time: 0:02:10\n",
      "batch: 50 ;B loss: 2.306 ;acc: 0.405 ;iou_acc: 0.475 ;time: 0:02:17\n",
      "batch: 100 ;B loss: 3.338 ;acc: 0.130 ;iou_acc: 0.245 ;time: 0:02:23\n",
      "batch: 150 ;B loss: 1.440 ;acc: 0.650 ;iou_acc: 0.745 ;time: 0:02:30\n",
      "batch: 200 ;B loss: 3.101 ;acc: 0.170 ;iou_acc: 0.265 ;time: 0:02:36\n",
      "batch: 250 ;B loss: 2.754 ;acc: 0.295 ;iou_acc: 0.400 ;time: 0:02:43\n",
      "\n",
      "*Test loss: 2.333 ;Test accuracy 0.384 ;IOU accuracy: 0.493 ;Time: 0:02:49\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:5; Training B:False; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;batch: 0 ;gs: 891 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 2.872 ;A loss: 0.142 ;edits ratio: 0.095 ;acc: 0.220 ;iou: 0.325 ;time: 0:00:00\n",
      "Edit: True ;batch: 50 ;gs: 891 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 2.872 ;A loss: 0.137 ;edits ratio: 0.076 ;acc: 0.245 ;iou: 0.355 ;time: 0:00:10\n",
      "Edit: True ;batch: 100 ;gs: 891 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 2.279 ;A loss: 0.101 ;edits ratio: 0.217 ;acc: 0.305 ;iou: 0.380 ;time: 0:00:21\n",
      "Edit: True ;batch: 150 ;gs: 891 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 2.251 ;A loss: 0.063 ;edits ratio: 0.198 ;acc: 0.305 ;iou: 0.420 ;time: 0:00:31\n",
      "Edit: True ;batch: 200 ;gs: 891 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 3.105 ;A loss: 0.162 ;edits ratio: 0.057 ;acc: 0.230 ;iou: 0.315 ;time: 0:00:42\n",
      "Edit: True ;batch: 250 ;gs: 891 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 2.769 ;A loss: 0.130 ;edits ratio: 0.169 ;acc: 0.270 ;iou: 0.390 ;time: 0:00:51\n",
      "\n",
      "*B Train loss: 2.337 ;A Train loss: 0.131 ;Edit ratio: 0.146 ;Train accuracy: 0.366 ;IOU accuracy: 0.471 ;Time: 0:01:00 \n",
      "\n",
      "VALIDATION, ephoc: 5\n",
      "--------------------\n",
      "\n",
      "Random choice: False\n",
      "batch: 0 ;edits ratio: 0.057 ;B loss: 1.780 ;A loss: 0.045 ;acc: 0.440 ;iou_acc: 0.555 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.096 ;B loss: 1.977 ;A loss: 0.045 ;acc: 0.445 ;iou_acc: 0.570 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.004 ;B loss: 2.668 ;A loss: 0.125 ;acc: 0.290 ;iou_acc: 0.400 ;time: 0:00:18\n",
      "\n",
      "*B loss: 2.299 ;A loss: 0.087 ;Edit ratio: 0.038 ;Accuracy 0.389 ;IOU accuracy: 0.497 ;Time: 0:00:27\n",
      "\n",
      "Random choice: True\n",
      "batch: 0 ;edits ratio: 0.039 ;B loss: 1.781 ;A loss: 0.064 ;acc: 0.470 ;iou_acc: 0.575 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.023 ;B loss: 1.978 ;A loss: 0.061 ;acc: 0.455 ;iou_acc: 0.565 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.035 ;B loss: 2.668 ;A loss: 0.130 ;acc: 0.295 ;iou_acc: 0.405 ;time: 0:00:18\n",
      "\n",
      "*B loss: 2.302 ;A loss: 0.113 ;Edit ratio: 0.034 ;Accuracy 0.391 ;IOU accuracy: 0.499 ;Time: 0:00:28\n",
      "\n",
      "*AlrDecreaseNum: 0 ; maxDecreaseNum: 3 ;AGain: -0.003 ;toTrainA: True\n",
      "--------------------------------------------------------------------- \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:6; Training B:True; A is a know-it-all:False; ActivateA:True\n",
      "Edit: False ;batch: 0 ;gs: 892 ;Blr: 0.050 ;B loss: 2.932 ;acc: 0.195 ;iou: 0.290 ;time: 0:00:00\n",
      "Edit: True ;batch: 50 ;gs: 942 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.444 ;A loss: 0.110 ;edits ratio: 0.083 ;acc: 0.625 ;iou: 0.770 ;time: 0:00:14\n",
      "Edit: True ;batch: 100 ;gs: 992 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.448 ;A loss: 0.092 ;edits ratio: 0.130 ;acc: 0.760 ;iou: 0.850 ;time: 0:00:29\n",
      "Edit: True ;batch: 150 ;gs: 1042 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 3.429 ;A loss: 0.236 ;edits ratio: 0.016 ;acc: 0.190 ;iou: 0.295 ;time: 0:00:45\n",
      "Edit: False ;batch: 200 ;gs: 1092 ;Blr: 0.050 ;B loss: 2.560 ;acc: 0.330 ;iou: 0.475 ;time: 0:01:00\n",
      "Edit: True ;batch: 250 ;gs: 1142 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0 ;B loss: 1.605 ;A loss: 0.094 ;edits ratio: 0.153 ;acc: 0.610 ;iou: 0.680 ;time: 0:01:13\n",
      "\n",
      "*B Train loss: 2.275 ;A Train loss: 0.148 ;Edit ratio: 0.157 ;Train accuracy: 0.402 ;IOU accuracy: 0.511 ;Time: 0:01:27 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;batch: 0 ;gs: 1188 ;Blr: 0.050 ;B loss: 2.762 ;acc: 0.280 ;iou: 0.430 ;time: 0:01:27\n",
      "Edit: False ;batch: 50 ;gs: 1188 ;Blr: 0.050 ;B loss: 1.359 ;acc: 0.730 ;iou: 0.845 ;time: 0:01:34\n",
      "Edit: False ;batch: 100 ;gs: 1188 ;Blr: 0.050 ;B loss: 1.543 ;acc: 0.770 ;iou: 0.860 ;time: 0:01:41\n",
      "Edit: False ;batch: 150 ;gs: 1188 ;Blr: 0.050 ;B loss: 3.347 ;acc: 0.200 ;iou: 0.325 ;time: 0:01:49\n",
      "Edit: False ;batch: 200 ;gs: 1188 ;Blr: 0.050 ;B loss: 2.507 ;acc: 0.355 ;iou: 0.520 ;time: 0:01:56\n",
      "Edit: False ;batch: 250 ;gs: 1188 ;Blr: 0.050 ;B loss: 1.576 ;acc: 0.670 ;iou: 0.740 ;time: 0:02:03\n",
      "\n",
      "*B Train loss: 2.181 ;Train accuracy: 0.483 ;IOU accuracy: 0.600 ;Time: 0:02:09 \n",
      "\n",
      "TESTING, ephoc: 6\n",
      "# t_data size: 59400\n",
      "# t_data batches: 297\n",
      "batch: 0 ;B loss: 3.412 ;acc: 0.205 ;iou_acc: 0.330 ;time: 0:02:10\n",
      "batch: 50 ;B loss: 2.113 ;acc: 0.515 ;iou_acc: 0.595 ;time: 0:02:17\n",
      "batch: 100 ;B loss: 3.205 ;acc: 0.170 ;iou_acc: 0.330 ;time: 0:02:22\n",
      "batch: 150 ;B loss: 1.378 ;acc: 0.720 ;iou_acc: 0.805 ;time: 0:02:29\n",
      "batch: 200 ;B loss: 2.955 ;acc: 0.290 ;iou_acc: 0.410 ;time: 0:02:36\n",
      "batch: 250 ;B loss: 2.584 ;acc: 0.360 ;iou_acc: 0.490 ;time: 0:02:42\n",
      "\n",
      "*Test loss: 2.187 ;Test accuracy 0.477 ;IOU accuracy: 0.597 ;Time: 0:02:48\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:7; Training B:False; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;batch: 0 ;gs: 1188 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 2.689 ;A loss: 0.157 ;edits ratio: 0.259 ;acc: 0.290 ;iou: 0.450 ;time: 0:00:01\n",
      "Edit: True ;batch: 50 ;gs: 1188 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 1.436 ;A loss: 0.392 ;edits ratio: 0.211 ;acc: 0.600 ;iou: 0.695 ;time: 0:00:11\n",
      "Edit: True ;batch: 100 ;gs: 1188 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 1.413 ;A loss: 0.323 ;edits ratio: 0.128 ;acc: 0.695 ;iou: 0.810 ;time: 0:00:22\n",
      "Edit: True ;batch: 150 ;gs: 1188 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 3.036 ;A loss: 0.224 ;edits ratio: 0.224 ;acc: 0.190 ;iou: 0.375 ;time: 0:00:32\n",
      "Edit: True ;batch: 200 ;gs: 1188 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 2.901 ;A loss: 0.236 ;edits ratio: 0.273 ;acc: 0.180 ;iou: 0.300 ;time: 0:00:43\n",
      "Edit: True ;batch: 250 ;gs: 1188 ;Blr: 0.050 ;Alr: 0.100 ;editRandomlyProb: 0.3333333333333333 ;B loss: 3.223 ;A loss: 0.165 ;edits ratio: 0.076 ;acc: 0.195 ;iou: 0.370 ;time: 0:00:54\n",
      "\n",
      "*B Train loss: 2.237 ;A Train loss: 0.227 ;Edit ratio: 0.225 ;Train accuracy: 0.417 ;IOU accuracy: 0.531 ;Time: 0:01:02 \n",
      "\n",
      "VALIDATION, ephoc: 7\n",
      "--------------------\n",
      "\n",
      "Random choice: False\n",
      "batch: 0 ;edits ratio: 0.178 ;B loss: 1.677 ;A loss: 0.184 ;acc: 0.510 ;iou_acc: 0.630 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.213 ;B loss: 1.806 ;A loss: 0.150 ;acc: 0.480 ;iou_acc: 0.575 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.103 ;B loss: 2.501 ;A loss: 0.112 ;acc: 0.370 ;iou_acc: 0.490 ;time: 0:00:19\n",
      "\n",
      "*B loss: 2.192 ;A loss: 0.188 ;Edit ratio: 0.145 ;Accuracy 0.442 ;IOU accuracy: 0.561 ;Time: 0:00:28\n",
      "\n",
      "Random choice: True\n",
      "batch: 0 ;edits ratio: 0.135 ;B loss: 1.686 ;A loss: 0.214 ;acc: 0.515 ;iou_acc: 0.655 ;time: 0:00:00\n",
      "batch: 50 ;edits ratio: 0.139 ;B loss: 1.844 ;A loss: 0.211 ;acc: 0.515 ;iou_acc: 0.595 ;time: 0:00:09\n",
      "batch: 100 ;edits ratio: 0.147 ;B loss: 2.521 ;A loss: 0.165 ;acc: 0.370 ;iou_acc: 0.505 ;time: 0:00:18\n",
      "\n",
      "*B loss: 2.184 ;A loss: 0.202 ;Edit ratio: 0.129 ;Accuracy 0.454 ;IOU accuracy: 0.574 ;Time: 0:00:27\n",
      "\n",
      "*AlrDecreaseNum: 1 ; maxDecreaseNum: 3 ;AGain: 0.008 ;toTrainA: True\n",
      "-------------------------------------------------------------------- \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "TRAIN, ephoc:8; Training B:True; A is a know-it-all:False; ActivateA:True\n",
      "Edit: True ;batch: 0 ;gs: 1189 ;Blr: 0.050 ;Alr: 0.080 ;editRandomlyProb: 0 ;B loss: 2.412 ;A loss: 0.058 ;edits ratio: 0.137 ;acc: 0.325 ;iou: 0.465 ;time: 0:00:01\n",
      "Edit: True ;batch: 50 ;gs: 1239 ;Blr: 0.050 ;Alr: 0.080 ;editRandomlyProb: 0 ;B loss: 1.825 ;A loss: 0.368 ;edits ratio: 0.456 ;acc: 0.420 ;iou: 0.485 ;time: 0:00:16\n",
      "Edit: True ;batch: 100 ;gs: 1289 ;Blr: 0.050 ;Alr: 0.080 ;editRandomlyProb: 0 ;B loss: 2.065 ;A loss: 0.310 ;edits ratio: 0.576 ;acc: 0.305 ;iou: 0.385 ;time: 0:00:29\n",
      "Edit: True ;batch: 150 ;gs: 1339 ;Blr: 0.050 ;Alr: 0.080 ;editRandomlyProb: 0 ;B loss: 2.277 ;A loss: 0.241 ;edits ratio: 0.541 ;acc: 0.230 ;iou: 0.360 ;time: 0:00:43\n",
      "Edit: True ;batch: 200 ;gs: 1389 ;Blr: 0.050 ;Alr: 0.080 ;editRandomlyProb: 0 ;B loss: 2.279 ;A loss: 0.254 ;edits ratio: 0.606 ;acc: 0.280 ;iou: 0.360 ;time: 0:00:55\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9a117a42d895>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                                 \u001b[0mBlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                 \u001b[0mAlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                 imScale=1/50)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-cefa8e3f02e0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trn_data, tst_data, val_data, ephocs_num, edit_reward, startA, activation_ephoc, muteBnum, start_ephoc, dropout_in, rnn_editProb, dropout_out, dropout_img, dropout_q, queries_editProb, addNoise, imScale, Alr, Blr)\u001b[0m\n\u001b[1;32m    997\u001b[0m                                 batch_Blr, gs, _ = sess.run(\n\u001b[1;32m    998\u001b[0m                                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB_optimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                                     feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1000\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mtoTrainA\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('params_dir:', params_dir)\n",
    "print('num_hidden:', m.num_hidden)\n",
    "tst, trn, val, AGain = m.train(trainset, testset, valset,\n",
    "                                ephocs_num=100,\n",
    "                                start_ephoc=0,\n",
    "                                startA=1,\n",
    "                                activation_ephoc=3,\n",
    "                                muteBnum=1, \n",
    "                                queries_editProb=0.95,\n",
    "                                edit_reward=-.8,\n",
    "                                rnn_editProb=0.2,\n",
    "                                Blr=.05,\n",
    "                                Alr=.1,\n",
    "                                imScale=1/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden=200\n",
    "edit_rewards = [-1.5, -1., -0.9, -0.8]\n",
    "edit_rewards_tst, edit_rewards_trn, edit_rewards_val, edit_rewards_AGain = [], [], [], []\n",
    "\n",
    "for edit_reward in edit_rewards:\n",
    "    params_dir = params_dir_tmp+'MIXER/edit_rewards_'+str(edit_rewards)+'hidden_'+str(num_hidden)\n",
    "    tf.reset_default_graph()\n",
    "    m = Model(\n",
    "        batch_size=200, \n",
    "        num_hidden=num_hidden,\n",
    "        embed_size=embed_vecs.shape[1],\n",
    "        img_dims=trainset[0][0][1][0].shape[1], \n",
    "        bbox_dims=trainset[0][0][1][1].shape[1], \n",
    "        vocab=vocab, \n",
    "        decay_steps=10000, \n",
    "        decay_rate=0.9, \n",
    "        bnorm=False,\n",
    "        toQscale=True\n",
    "    )\n",
    "\n",
    "    print('params_dir:', params_dir)\n",
    "    print('num_hidden:', m.num_hidden)\n",
    "    tst, trn, val, AGain = m.train(trainset, testset, valset,\n",
    "                                    ephocs_num=200,\n",
    "                                    start_ephoc=0,\n",
    "                                    startA=10,\n",
    "                                    activation_ephoc=13,\n",
    "                                    muteBnum=2, \n",
    "                                    queries_editProb=0.95,\n",
    "                                    edit_reward=edit_reward,\n",
    "                                    rnn_editProb=0.2,\n",
    "                                    Blr=.05,\n",
    "                                    Alr=.2,\n",
    "                                    imScale=1/50)\n",
    "    edit_rewards_tst.append(tst) \n",
    "    edit_rewards_trn.append(trn)\n",
    "    edit_rewards_val.append(val) \n",
    "    edit_rewards_AGain.append(AGain)\n",
    "    \n",
    "    print('\\n'+'*'*100)\n",
    "    print('*'*100)\n",
    "    print('*'*100,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(open('../data/training/results/edit_rewards_tst.bin', 'wb'), edit_rewards_tst)\n",
    "np.save(open('../data/training/results/edit_rewards_trn.bin', 'wb'), edit_rewards_trn)\n",
    "np.save(open('../data/training/results/edit_rewards_val.bin', 'wb'), edit_rewards_val)\n",
    "np.save(open('../data/training/results/edit_rewards_trn.bin', 'wb'), edit_rewards_AGain)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
