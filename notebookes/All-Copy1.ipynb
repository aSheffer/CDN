{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import retriever\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset_file = '../data/training/order_train_data.bin'\n",
    "testset_file = '../data/training/order_test_data.bin'\n",
    "vocab_file =  '../data/metadata/w2v_vocab.json'\n",
    "params_dir_tmp = '../data/training/models/All/'\n",
    "embed_path =  '../data/metadata/w2v.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Each entry in this list has the following structure:</h3>\n",
    "<ul>\n",
    "<li>entry[0]: query indexes </li>\n",
    "<li>entry[1:n]: n items where each item is [bounding box vector, bounding box spaital features]. Note that different enteries might have different 'n' </li>\n",
    "<li>entry[n+1]: integer, entry[ 1 + entry[n+1]] is the ture bbox </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset barches #: 297\n",
      "test barches #: 297\n"
     ]
    }
   ],
   "source": [
    "trainset = np.load(open(trainset_file, 'rb'))\n",
    "trainset = [item for item in trainset if len(item)>2 and len(item[0])>0]\n",
    "print('trainset barches #:', len(trainset))\n",
    "\n",
    "testset = np.load(open(testset_file, 'rb'))\n",
    "testset = [item for item in testset if len(item)>2 and len(item[0])>0]\n",
    "print('test barches #:', len(testset))\n",
    "\n",
    "with open(vocab_file, 'r') as f:\n",
    "    vocab = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab['<unk>'] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8242, 8241)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), vocab['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_vecs = np.load(open(embed_path, 'rb')).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> augment_data function </h3>\n",
    "<br>We try sevral regularization methods. One of the things I've tried is to add data points where for each data I pick a query from a random data point and a set of bbox from a different random point. We build the labels (bboxes) distribution by giving an equal probability to each label. <br><br>\n",
    "The augment_data function does just that but the label of each added poind is writen as -1*number of bboxes. When we build the batch it self (function build_data in class Model), if we see a negative label index, we know its an added point and we know the number of bboxes so we can build the correct distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_data(data, ratio=0.5, addNoise=False):\n",
    "        '''\n",
    "        The function add data points. \n",
    "        We pick a query from a random data point,\n",
    "        and a set of bbox from a different random point and we join them\n",
    "        to build a new data point. The label distribution of the new data point will \n",
    "        uniform, that is, all labels will have equal probability. \n",
    "        \n",
    "        \n",
    "        Params:\n",
    "            data: a list of data entries\n",
    "                                                \n",
    "        Returns: a list of augmented data\n",
    "            \n",
    "                        \n",
    "        '''\n",
    "                          \n",
    "        q_idx = np.random.choice(range(len(data)), int(len(data)*ratio), replace=False)\n",
    "        im_idx = np.random.choice([i for i in range(len(data)) if i not in q_idx], int(len(data)*ratio))\n",
    "        for i in range(len(q_idx)):\n",
    "            q, im = data[q_idx[i]][0], data[im_idx[i]][1:-1]\n",
    "            item = [q]\n",
    "            for im_tmp in im:\n",
    "                item.append(im_tmp)\n",
    "            item.append(-len(im))\n",
    "            data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stats(test, train, ephocs=100, title=None, params=[50, 100, 150, 200]):\n",
    "    '''\n",
    "    Plot metrics graphs and print some stats.\n",
    "    \n",
    "    Params:\n",
    "        test: list. \n",
    "              Each item is a tuple, [test accuracy, test IOU, test loss]\n",
    "        \n",
    "        train: list. \n",
    "               Each item is a tuple, [train accuracy, train IOU, train loss, 0]\n",
    "               For now we can ignore the last part in the tuple (zero)\n",
    "               \n",
    "        params: The hyper-parameters to iterate over, defult to number of rnn's hidden units.\n",
    "    '''\n",
    "    \n",
    "    ephocs = range(ephocs)\n",
    "    test_res = np.array(test)\n",
    "    train_res = np.array(train)\n",
    "    test_Glabels = ['test accuracy', 'test IOU', 'test loss']\n",
    "    train_Glabels = ['train accuracy', 'train IOU', 'train loss']\n",
    "\n",
    "    for j, param in enumerate(params):\n",
    "        print('num_hidden:', param)\n",
    "        print('='*(len('num_hidden:')+3))\n",
    "        for i in range(len(train_Glabels)):\n",
    "            plt.plot(ephocs, test_res[j][:,i])\n",
    "            plt.plot(ephocs, train_res[j][:,i])\n",
    "            plt.legend([test_Glabels[i], train_Glabels[i]], loc='upper left')\n",
    "            if title is not None:\n",
    "                plt.title('%s: %d'%(title, param))\n",
    "            plt.show()\n",
    "\n",
    "            metric = ''.join(train_Glabels[i][len('train')+1:])\n",
    "            if metric=='loss':\n",
    "                print('Train min %s:%.3f'%(metric, min(train_res[j][:,i])))\n",
    "                print('Test min %s:%.3f'%(metric, min(test_res[j][:,i])))\n",
    "            else:\n",
    "                print('Train max %s:%.3f'%(metric, max(train_res[j][:,i])))\n",
    "                print('Test max %s:%.3f'%(metric, max(test_res[j][:,i])))\n",
    "        print('-'*100,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALSTM\n",
    "\n",
    "This RNN cell has two LSTM cells, Bcell (for player B) and Acell (for Player A) and work as follow:\n",
    "<ol> \n",
    "<li>We run B's cells with the true query input word, getting the un-edited state. If use_worsAttn=True we add attention on the images vectors for each word.</li>\n",
    "<li>We run B's cells with the edited input word - 'unk', getting the edited state. If use_worsAttn=True we add attention on the images vectors for each word.</li>\n",
    "<li>We feed the un-edited state to A's cell.</li>\n",
    "<li>We run A's cells. A's input are:\n",
    "<ul><li>B's un-edited state</li><li>The reward for editing a word</li><li>B's loss having no edited words.</li></ul></li>\n",
    "<li>A's output is then goes throw a transformation which yields two values, one for editing a word and another for not.<br>If the the value for edit the word is higher, we pass B's edited state to the next time step, else we pass B's un_edit state.</li><br>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ALSTM(tf.nn.rnn_cell.LSTMCell):\n",
    "    def __init__(self, \n",
    "                 batch_size, \n",
    "                 num_units, \n",
    "                 \n",
    "                 # Size of A's attention vector.\n",
    "                 words_attn_dim, \n",
    "                 # B's outputs (for each time step).\n",
    "                 words_attn_states, \n",
    "                 # Inicates whether attentionvec is a padding (0) or not (1).\n",
    "                 words_attn_idx, \n",
    "                 \n",
    "                 # Size of B's attention vector ([image vector, spital features] size) .\n",
    "                 img_attn_dim, \n",
    "                 # B's outputs (for each time step).\n",
    "                 img_attn_states, \n",
    "                 \n",
    "                 \n",
    "                 # Inicates whether attentionvec is a padding (0) or not (1).\n",
    "                 img_attn_idx, \n",
    "                 \n",
    "                 unk, #'unk' word vector\n",
    "                 \n",
    "                 # Probabilty for edit a word in rnn, \n",
    "                 # when decision are taken randomly.\n",
    "                 rnn_editProb, \n",
    "                 \n",
    "                 \n",
    "                 # Whehter to edit the query or not.\n",
    "                 isEdit, \n",
    "                 \n",
    "                 # Whether B uses words levlel attention or not.\n",
    "                 use_wordAttn,\n",
    "                 \n",
    "                 # If True add noise instead of using 'unk'\n",
    "                 useNoise=False,\n",
    "                 \n",
    "                 # If useNoise is true word vec = alpha*word_vector+(1-alpha)*noise\n",
    "                 alpha=.3,\n",
    "                 \n",
    "                 # when isEdit=True, whether or not to use A's \n",
    "                 # output inorder to edit or to do it randomly.\n",
    "                 activateA=False,\n",
    "                 \n",
    "                 #this holds A's rewards and B's losses to\n",
    "                 # be add to A's feature vectors.\n",
    "                 reward_loss=None,\n",
    "                 state_is_tuple=True,):\n",
    "        \n",
    "        \n",
    "        # When useing A, the cell state will contain the concatenation \n",
    "        # of both B and A states. Therefore we set the unit number to be\n",
    "        # 2*(A and B unit size).\n",
    "        super().__init__(2*num_units, state_is_tuple=state_is_tuple)\n",
    "    \n",
    "        self.words_attn_states = words_attn_states\n",
    "        self.words_attn_idx = words_attn_idx\n",
    "        self.words_attn_dim = words_attn_dim\n",
    "        \n",
    "        self.img_attn_states = img_attn_states\n",
    "        self.img_attn_idx = img_attn_idx\n",
    "        self.img_attn_dim = img_attn_dim\n",
    "        \n",
    "        self.num_units = num_units\n",
    "        self.batch_size = batch_size\n",
    "        self.unk = unk \n",
    "        self.isEdit = isEdit\n",
    "        self.activateA = activateA\n",
    "        self.use_wordAttn=use_wordAttn\n",
    "        \n",
    "        \n",
    "        self.Acell = tf.nn.rnn_cell.LSTMCell(self.num_units, state_is_tuple=True)\n",
    "        self.Bcell = tf.nn.rnn_cell.LSTMCell(self.num_units, state_is_tuple=True)\n",
    "        \n",
    "        self.rnn_editProb = rnn_editProb\n",
    "        self.reward_loss = reward_loss\n",
    "        self.useNoise=useNoise\n",
    "        self.alpha=alpha\n",
    "        \n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        '''\n",
    "        Params:\n",
    "            inputs: word embadding.\n",
    "            state:  [B's state form privious state, A's state form privious state]\n",
    "        '''\n",
    "        # takse B's state from state[:self.num_units]\n",
    "        Bstate_c = tf.slice(state[0], [0, 0], [-1, self.num_units])\n",
    "        Bstate_h = tf.slice(state[1], [0, 0], [-1, self.num_units])\n",
    "        self.Bstate =  tf.nn.rnn_cell.LSTMStateTuple(c=Bstate_c, h=Bstate_h)\n",
    "         \n",
    "        # If B's cell uses attention\n",
    "        if self.use_wordAttn:\n",
    "            words_attn = self.attention(Bstate_h, self.img_attn_states, self.img_attn_dim, self.img_attn_idx)\n",
    "            new_input = tf.concat([inputs, words_attn], -1)\n",
    "            Boutputs, Bnew_state =  self.Bcell(new_input, self.Bstate, 'Bcell')\n",
    "        else:\n",
    "            Boutputs, Bnew_state =  self.Bcell(inputs, self.Bstate, 'Bcell')\n",
    "\n",
    "        \n",
    "        # If isEdit==True \n",
    "        def f1(): \n",
    "            # takse A's state from state[self.num_units: 2*self.num_units]\n",
    "            Astate_c = tf.slice(state[0], [0, self.num_units], [-1, self.num_units])\n",
    "            Astate_h = tf.slice(state[1], [0, self.num_units], [-1, self.num_units])\n",
    "            self.Astate =  tf.nn.rnn_cell.LSTMStateTuple(c=Astate_c, h=Astate_h)\n",
    "            \n",
    "            if self.useNoise: # just add noise to the edited words\n",
    "                new_unk_vecs = self.alpha*inputs + (1-alpha)*tf.random_normal(shape=inputs.get_shape(), stddev=0.1)\n",
    "            else: # change the edited words by 'unk'\n",
    "                unk_vecs = tf.concat([self.unk for _ in range(self.batch_size)], 0) # shape: self.batch_size x 1 x embed_size\n",
    "                new_unk_vecs = tf.squeeze(unk_vecs) # shape: self.batch_size x embed_size\n",
    "            \n",
    "            # run B's cell with unk_batch\n",
    "            if self.use_wordAttn:\n",
    "                new_unk = tf.concat([new_unk_vecs, words_attn], -1)\n",
    "            else:\n",
    "                new_unk = new_unk_vecs\n",
    "            edit_output, edit_new_state = self.Bcell(new_unk, self.Bstate, 'Bcell')  \n",
    "            out1, state1 = self.runCell(Boutputs, Bnew_state, edit_new_state)\n",
    "            return out1, state1\n",
    "        \n",
    "        def f2(): \n",
    "            outs = tf.concat([Boutputs, tf.zeros((self.batch_size, self.num_units))], 1)\n",
    "            new_state_c = tf.concat([Bnew_state[0], tf.zeros_like(Bnew_state[0])], 1)\n",
    "            new_state_h = tf.concat([Bnew_state[1], tf.zeros_like(Bnew_state[1])], 1)\n",
    "            return outs, tf.nn.rnn_cell.LSTMStateTuple(c=new_state_c, h=new_state_h)\n",
    "        \n",
    "        new_output, new_state = tf.cond(self.isEdit, f1, f2)\n",
    "        \n",
    "        return new_output, new_state\n",
    "    \n",
    "    def runCell(self, Boutputs, Bnew_state, edit_new_state):\n",
    "        '''\n",
    "        Run B's cell after editing.\n",
    "        \n",
    "        params:\n",
    "            Boutputs: output vector without editing.\n",
    "            Bnew_state: state vector without editing.\n",
    "            edit_new_state: state vector after editing the input word to 'unk'. \n",
    "        '''\n",
    "        \n",
    "        with tf.variable_scope('runcell'):\n",
    "            # get action values according to B's hidden state\n",
    "            Aout, Anew_state, actions_vals = self.action_vals(Boutputs) \n",
    "            \n",
    "            def f1(): \n",
    "                '''\n",
    "                If self.activateA==True, edit word if actions_vals[0]<actions_vals[1].\n",
    "                Note: if we edit the word cond=1, else cond=0.\n",
    "                '''\n",
    "                a1, a2 = tf.split(value=actions_vals, num_or_size_splits=2, axis=1)\n",
    "                cond = tf.less(a1, a2)\n",
    "                return cond\n",
    "\n",
    "            def f2(): \n",
    "                '''\n",
    "                If self.activateA==False, choose randomly whether to edit a word.\n",
    "                We edit a word with 'rnn_editProb' probability (~U[0,1]).\n",
    "                Note: if we edit the word cond=1, else cond=0.\n",
    "                '''\n",
    "                rand = tf.multinomial(tf.log([[self.rnn_editProb, 1-self.rnn_editProb]]), self.batch_size)\n",
    "                cond = tf.less(tf.transpose(rand), 1) \n",
    "                return cond\n",
    "\n",
    "            # a list of A's decisions for each batch.  1->edit, 0-> do not edit.\n",
    "            cond = tf.cast(tf.cond(self.activateA, f1, f2), tf.float32)\n",
    "\n",
    "            # We'd like to know the action values and decision for each word,\n",
    "            # therefore theses info are placed on the first 3 dimensions of the \n",
    "            # output vector. Note that this vector is not passed to the \n",
    "            # next tiee step so it won't affect the model. \n",
    "            outs = tf.concat([actions_vals, cond], 1)\n",
    "\n",
    "            # B's i state is replaced by the edited state if cond[i]=1 (i =1, 2, ..., batch_size)\n",
    "            new_edit_state_c = (1-cond)*Bnew_state[0] + cond*edit_new_state[0]\n",
    "            new_edit_state_h = (1-cond)*Bnew_state[1] + cond*edit_new_state[1]\n",
    "\n",
    "\n",
    "            new_outs = tf.concat([outs, tf.zeros((self.batch_size, 2*self.num_units-3))], 1)\n",
    "            new_state_c = tf.concat([new_edit_state_c, Anew_state[0]], 1)\n",
    "            new_state_h = tf.concat([new_edit_state_h, Anew_state[1]], 1)\n",
    "            return new_outs, tf.nn.rnn_cell.LSTMStateTuple(c=new_state_c, h=new_state_h)\n",
    "    \n",
    "    \n",
    "    def action_vals(self, Boutputs):\n",
    "        '''\n",
    "        Get values for editing/not editing the input word.\n",
    "        \n",
    "        Params:\n",
    "            Boutputs: output vector without editing.\n",
    "            \n",
    "        Returns vals where:\n",
    "            Aout: A's cell output\n",
    "            Anew_state: A's cell state\n",
    "            vals: Tensor where vals[0] is the value for not editing the word \n",
    "                    and vals[1] is the value for editing the word.\n",
    "        '''\n",
    "        \n",
    "        with tf.variable_scope('action_vals') as scope:\n",
    "            Aattn = self.attention(self.Astate[1], self.words_attn_states, self.words_attn_dim, self.words_attn_idx)\n",
    "            \n",
    "            # A's input: [input, B's output, attntion state, reward, B's loss with no edits]\n",
    "            Anew_inputs = tf.concat([Boutputs, Aattn, self.reward_loss], 1)\n",
    "            \n",
    "            Aout, Anew_state = self.Acell(Anew_inputs, self.Astate, 'Acell')\n",
    "            vals = tf.nn.relu(self.linear(Aout, 2))\n",
    "\n",
    "            # Save the variables that only A uses. \n",
    "            # These variables will be trained separately from B's model.\n",
    "            self.Avars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)\n",
    "            \n",
    "            return Aout, Anew_state, vals\n",
    "            \n",
    "        \n",
    "    def linear(self, inputs, output_dim, scope='linear', bias=True, reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=False):\n",
    "            W = tf.get_variable('W', initializer=tf.random_uniform_initializer(maxval=1., minval=-1.),\n",
    "                                shape=(inputs.get_shape()[-1], output_dim))\n",
    "            if bias:\n",
    "                b = tf.get_variable('b', initializer=tf.constant_initializer(0.1),\n",
    "                               shape=[1, output_dim])\n",
    "                return tf.matmul(inputs, W) + b\n",
    "\n",
    "        return tf.matmul(inputs, W)\n",
    "    \n",
    "    \n",
    "    def attention(self, state, attn_states, attn_dim, attn_idx, relu=False):\n",
    "        '''\n",
    "        Attention mechanism (see https://arxiv.org/pdf/1409.0473.pdf)\n",
    "        \n",
    "        state: State from previous time step.\n",
    "        attn_states: Attetntion states. \n",
    "                     Tensor of shape (batch_size x max([len(attention_vectors[i]) for i in range(bach_size)]) x attn_dim)\n",
    "        attn_dim: Attention vector size.\n",
    "        attn_idx,: Tensor used for masking of shape (batch_size x max([len(attention_vectors[i]) for i in range(bach_size)]). \n",
    "                   attn_idx[i, j]=1 if the j's attention vcctior of sample i  is not padding, else its equat to 0.\n",
    "        '''\n",
    "        \n",
    "        self.attn_length = tf.shape(attn_states)[1]  \n",
    "        \n",
    "        # Computing... hidden_attn = W*v_att (use tf.nn.conv2d for efficiency)\n",
    "        attn_vecs = tf.reshape(attn_states, [self.batch_size, self.attn_length, 1, attn_dim])\n",
    "        W = tf.get_variable(\"attn_W\", [1, 1, attn_dim, self.num_units])\n",
    "        hidden_attn = tf.nn.conv2d(attn_vecs, W, [1, 1, 1, 1], \"SAME\")\n",
    "\n",
    "        # Computing... hidden_s = U*v_state\n",
    "        hidden_s = tf.reshape(\n",
    "            self.linear(\n",
    "                tf.cast(state, tf.float32), output_dim=self.num_units, scope='hidden_s_linear'), [-1, 1, 1,  self.num_units], name='hidden_s')\n",
    "\n",
    "        # Computing alpha\n",
    "        v = tf.get_variable(\"attn_v\", [self.num_units])\n",
    "        if relu:\n",
    "            logits = tf.reduce_sum(v * tf.nn.relu(hidden_attn + hidden_s), [2, 3])\n",
    "        else:\n",
    "            logits = tf.reduce_sum(v * tf.nn.tanh(hidden_attn + hidden_s), [2, 3])\n",
    "\n",
    "        # Masked softmax\n",
    "        max_logits = tf.reduce_max(logits, axis=-1)\n",
    "        masked_logits = tf.exp(logits-tf.expand_dims(max_logits, axis=1))*attn_idx\n",
    "        alpha = masked_logits/tf.reduce_sum(masked_logits, axis=-1, keep_dims=True)\n",
    "\n",
    "        a = tf.reduce_sum(tf.reshape(alpha, [-1, self.attn_length, 1, 1]) * attn_vecs, [1, 2])\n",
    "        b = tf.contrib.layers.fully_connected(a, num_outputs=self.num_units)\n",
    "                                               \n",
    "\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "When A joins the game, each iteration is completed via three steps:\n",
    "<ul>\n",
    "<li>We feed the query as it is to B and run it alone (no optimization nor training is done in this step)</li>\n",
    "<li>We run A and B together (no optimization nor training is done in this step). At this step we get A's decition and B's loss on the edited query. We use B's loss to calculate the Bellman's value for each time step. </li>\n",
    "<li>We again run A and B together, since now we know the real values for each time step and the real action for each time step (these will be the same as in the previous step since we did not train optimize the parameters yet), we finaly train the model</li>\n",
    "</ul>\n",
    "<br>\n",
    "But first we check the model's performance with out A. Class Model has a set of conditioning variables that set a different ragularization methods in the model. We start by checking them with out A's interference. We can also make the model RNN become bidirectional (by setting useBidirectionalRnn to True) and use words level attention (by setting use_wordAttn to True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self,\n",
    "                 batch_size, \n",
    "                 num_hidden, \n",
    "                 \n",
    "                 #Image's vector size.\n",
    "                 img_dims, \n",
    "                 \n",
    "                 #Spaital features length.\n",
    "                 bbox_dims, \n",
    "                 \n",
    "                 #Probabilty for edit a word in rnn, when decision are taken randomly.\n",
    "                 rnn_editProb,  \n",
    "                 vocab, \n",
    "                 lr, #  B's learning rate.\n",
    "                 decay_steps, \n",
    "                 decay_rate, \n",
    "                 \n",
    "                 # A's leanring rate = B's learning rate x coefAlr.\n",
    "                 coefAlr,\n",
    "                 \n",
    "                 # whether to use bach normaliztion for the last attention layer\n",
    "                 bnorm,\n",
    "                 embed_size=embed_vecs.shape[1],\n",
    "                 # Whether B uses words levlel attention or not.\n",
    "                 use_wordAttn=False,\n",
    "                 \n",
    "                 # Whther to use bidirectional rnn\n",
    "                 useBidirectionalRnn=False,\n",
    "                 \n",
    "                 # Urnn_norm: Whether to use batch normalization for the queries.\n",
    "                 # Uatt_norm: Whether to use batch normalization for the VGG outputs.\n",
    "                 Urnn_norm=True, \n",
    "                 Uatt_norm=True,\n",
    "                \n",
    "                 # Whether to scale and normelize queries \n",
    "                 # embedding to have zero mean and QSTD std\n",
    "                 toQscale = False,\n",
    "                 Qstd = 0.1):\n",
    "        \n",
    "#         with tf.device('/cpu:0'):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_dims = img_dims\n",
    "        self.bbox_dims = bbox_dims \n",
    "        self.num_hidden = num_hidden\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab = vocab\n",
    "        L2Reg = 0.0005 # L2 regulizerr coefficient\n",
    "        self.lr = lr\n",
    "        self.toQscale=toQscale\n",
    "        self.Qstd=Qstd\n",
    "\n",
    "\n",
    "        self.queries = tf.placeholder(tf.int32, [None, None], name='queries')\n",
    "        self.img  = tf.placeholder(tf.float32, [None, None, self.img_dims], name='img')# VGG output vectors\n",
    "        self.bboxes = tf.placeholder(tf.float32, [None, None, self.bbox_dims], name='bboxes')# spatial bbox's features.\n",
    "\n",
    "        # attn_idx: inicates whether attention box is a dummy (0) or not (1).\n",
    "        self.attn_idx = tf.placeholder(tf.float32, [None, None], name='attn_idx')\n",
    "\n",
    "        self.labels = tf.placeholder(tf.float32, [None, None], name='labels')\n",
    "        self.isEdit = tf.placeholder(tf.bool, name='isEdit') # whehter to edit the query or not.\n",
    "\n",
    "        # when isEdit=True, whether to use A's output in order to edit or to do it randomly.\n",
    "        self.activateA = tf.placeholder(tf.bool, name='activateA') \n",
    "\n",
    "\n",
    "        # this holds  A's rewards and B losses to be add to A's feature vectors.\n",
    "        self.reward_loss = tf.placeholder(tf.float32, [None,2], name='rewards_loss')\n",
    "\n",
    "        # Dropout ratio for rnn's inputs and outpouts\n",
    "        self.dropout_in = tf.placeholder(tf.float32, name='dropoutIn_holder')\n",
    "        self.dropout_out = tf.placeholder(tf.float32, name='dropoutOut_holder')\n",
    "\n",
    "        # Dropout ratio for attention vector (for the final attention layer before the loss function)\n",
    "        self.dropout_img = tf.placeholder(tf.float32, name='dropoutImg_holder')\n",
    "        # Dropout ratio for query vector (for the final attention layer before the loss function)\n",
    "        self.dropout_q = tf.placeholder(tf.float32, name='dropoutImg_holder')\n",
    "\n",
    "        # B outputs vectors (with no words edits), These are A's attention vectors\n",
    "        # which it uses to decide whter to edit a word.\n",
    "        self.Aattn_vecs = tf.placeholder(tf.float32, [None, None, None], name='Aattn_vecs_holder')    \n",
    "        self.unk = tf.constant([[vocab['<unk>']]], tf.int32)\n",
    "\n",
    "        self.isTrain = tf.placeholder(tf.bool, name='isTrain_holder') \n",
    "        self.queries_lens = self.length(self.queries) # list of all the lengths  of the batch's queriey \n",
    "\n",
    "        # Concatinate images vectors and their spaital features. \n",
    "        # These vectors wlll be used for attenionn when \n",
    "        # we calculate the loss function.\n",
    "        attn_vecs = tf.concat([self.img, self.bboxes], 2) \n",
    "        voc_size = len(self.vocab)\n",
    "\n",
    "        # Load pre-trained word imaddings.\n",
    "        # w2v_embed is not trainable.\n",
    "        with tf.variable_scope('w2v'):\n",
    "            w2v_embed = tf.get_variable('w2v_embed', initializer=embed_vecs, trainable=False)\n",
    "            w2v_queries = tf.nn.embedding_lookup(w2v_embed, self.queries, name='w2v_queries')\n",
    "\n",
    "        with tf.variable_scope('embed'):\n",
    "            embed = tf.get_variable('embed', shape=[voc_size, self.embed_size], \n",
    "                                    initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "            embed_queries_tmp = tf.nn.embedding_lookup(embed, self.queries, name='embed_queries')\n",
    "\n",
    "        embed_queries = embed_queries_tmp+w2v_queries\n",
    "\n",
    "        with tf.variable_scope('rnn'):\n",
    "            Aattn_idx = tf.cast(tf.abs(tf.sign(self.queries)), tf.float32)\n",
    "\n",
    "\n",
    "            cell_tmp = ALSTM(num_units=self.num_hidden, \n",
    "                            words_attn_dim=self.num_hidden, \n",
    "                            words_attn_states=self.Aattn_vecs, \n",
    "                            words_attn_idx=Aattn_idx,\n",
    "                            img_attn_dim=self.img_dims+self.bbox_dims,\n",
    "                            img_attn_states=attn_vecs,\n",
    "                            img_attn_idx=self.attn_idx,\n",
    "                            batch_size=self.batch_size, \n",
    "                            unk=tf.nn.embedding_lookup(embed, self.unk), rnn_editProb=rnn_editProb,\n",
    "                            activateA=self.activateA, isEdit=self.isEdit,\n",
    "                            reward_loss=self.reward_loss, use_wordAttn=use_wordAttn)\n",
    "\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(\n",
    "                cell_tmp, input_keep_prob=self.dropout_in, output_keep_prob=self.dropout_out)\n",
    "\n",
    "            if useBidirectionalRnn:\n",
    "                self.outputs, self.last_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw=cell,\n",
    "                    cell_bw=cell,\n",
    "                    dtype=tf.float32,\n",
    "                    sequence_length=self.queries_lens,\n",
    "                    inputs=embed_queries)\n",
    "\n",
    "                # self.last_states contain both forward state\n",
    "                # and backward state.\n",
    "                # We don't use A with bidirectional rnn\n",
    "                # so no need for self.values (action values as calculated by A)\n",
    "                Bstate = tf.concat(\n",
    "                    [tf.slice(self.last_states[0][1], [0,0], [-1, self.num_hidden]), \n",
    "                     tf.slice(self.last_states[1][1], [0,0], [-1, self.num_hidden])], -1)\n",
    "            else:\n",
    "                self.outputs, self.last_states = tf.nn.dynamic_rnn(\n",
    "                    cell=cell,\n",
    "                    dtype=tf.float32,\n",
    "                    sequence_length=self.queries_lens,\n",
    "                    inputs=embed_queries)\n",
    "\n",
    "                # self.values[0]=value for not editing, self.values[1]=value for editing\n",
    "                self.values = tf.slice(self.outputs, [0,0,0], [-1,-1,2])\n",
    "                Bstate = tf.slice(self.last_states[1], [0,0], [-1, self.num_hidden])  \n",
    "\n",
    "\n",
    "        Avars = cell_tmp.Avars\n",
    "        self.Avars = {var.name:var for var in Avars}\n",
    "\n",
    "        if bnorm:\n",
    "            self.scores = self.bnorm_attention(Bstate, Urnn_norm=Urnn_norm, Uatt_norm=Uatt_norm) \n",
    "        else:\n",
    "            self.scores = self.attention(Bstate) \n",
    "\n",
    "\n",
    "        # Cross entophy loss for each of the queries in the batch.\n",
    "        self.B_ce = -tf.reduce_sum(\n",
    "                        self.labels*tf.log(self.scores+0.00000001)+\n",
    "                            (1-self.labels)*tf.log((1-self.scores)+0.00000001), \n",
    "                            axis=-1)\n",
    "\n",
    "\n",
    "        # We don't use A with bidirectional rnn\n",
    "        if not useBidirectionalRnn:\n",
    "            # A's decision for each word.\n",
    "            self.idx = tf.squeeze(tf.slice(self.outputs, [0,0,2], [-1,-1,1]))\n",
    "\n",
    "            self.edit_num = tf.reduce_mean(tf.reduce_sum(\n",
    "                tf.cast(self.idx, tf.float32)*tf.expand_dims(\n",
    "                        1/tf.cast(self.queries_lens, tf.float32), axis=1), axis=1))\n",
    "\n",
    "            # After running A for the first time, we get A's decisions and their values.\n",
    "            # we then calulate the following tensors:\n",
    "            # actions_idx[j,i] = 1 if the word i in query j was edited or 0 otherwise.     \n",
    "            # bell_vall holds the values for each decision by the bellman function. \n",
    "\n",
    "            #  self.actions_idx will be used to get the values of the action that were taken by A.                 \n",
    "            self.actions_idx = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"actions_idx\")\n",
    "            self.bell_val = tf.placeholder(shape=[None, None], dtype=tf.float32, name=\"bell_val\")\n",
    "\n",
    "            # pyrite_val: holds the values for each of A's decisions, claculated by A. \n",
    "            # see https://en.wikipedia.org/wiki/Pyrite.\n",
    "            self.pyrite_val = tf.reshape(tf.gather_nd(tf.reshape(self.values, (-1,2)), self.actions_idx), (self.batch_size, -1))\n",
    "\n",
    "            # RMSE loss\n",
    "            self.A_loss = tf.reduce_mean(tf.reduce_sum(\n",
    "                tf.sqrt(tf.square((self.bell_val-self.pyrite_val+0.000001)*tf.cast(\n",
    "                    tf.sign(tf.abs(self.queries)), tf.float32)))/tf.expand_dims(\n",
    "                                                    tf.cast(self.queries_lens, tf.float32), axis=1), axis=-1))\n",
    "\n",
    "        self.B_loss = tf.reduce_mean(self.B_ce)\n",
    "\n",
    "        ##############\n",
    "        # Optimizers #\n",
    "        ##############\n",
    "\n",
    "        starter_learning_rate = self.lr\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                       decay_steps=decay_steps, decay_rate=decay_rate, staircase=True)\n",
    "\n",
    "        if not useBidirectionalRnn:\n",
    "            # Train only A variables \n",
    "            self.A_optimizer =  tf.train.GradientDescentOptimizer(\n",
    "                    learning_rate=coefAlr*self.learning_rate).minimize(self.A_loss, var_list=Avars)  \n",
    "\n",
    "        # Train only B variables \n",
    "        Bvars = [var for var in tf.trainable_variables() if var not in Avars]\n",
    "        self.B_optimizer =  tf.train.GradientDescentOptimizer(\n",
    "                    learning_rate=self.learning_rate).minimize(self.B_loss, global_step=self.global_step, var_list=Bvars)  \n",
    "\n",
    "        if not os.path.exists(params_dir):\n",
    "                os.makedirs(params_dir)\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "    def length(self, seq):\n",
    "        '''\n",
    "        Retruns real lengths (before addings) of all queries in seq  .\n",
    "        '''\n",
    "        return tf.cast(tf.reduce_sum(tf.sign(tf.abs(seq)), reduction_indices=1), tf.int32)\n",
    "       \n",
    "\n",
    "    def linear(self, inputs, output_dim, scope='linear', bias=True, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            W = tf.get_variable('W', initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                shape=(inputs.get_shape()[-1], output_dim))\n",
    "            if bias:\n",
    "                b = tf.get_variable('b', initializer=tf.constant_initializer(0.1),\n",
    "                               shape=[1, output_dim])\n",
    "                return tf.matmul(inputs, W) + b\n",
    "            \n",
    "            return tf.matmul(inputs, W)\n",
    "\n",
    "            \n",
    "    def Qscale(self, qVecs):\n",
    "        '''\n",
    "        Scale queries embedding vectors to have zero mean and std STD.\n",
    "        \n",
    "        Params:\n",
    "            qVecs: Tensor (shape: batch_size x number_of_hidden_units) \n",
    "                   holding the queries vectors, \n",
    "        '''\n",
    "        qVecs_min = tf.reduce_min(qVecs, axis=-1, keep_dims=True)\n",
    "        qVecs_max = tf.reduce_max(qVecs, axis=-1, keep_dims=True)\n",
    "        qVecs_scale = (qVecs-qVecs_min)/(qVecs_max-qVecs_min) # Scale to 0-1\n",
    "        s = tf.contrib.keras.backend.std(qVecs_scale, axis=-1, keepdims=True)\n",
    "        tmp = self.Qstd * qVecs_scale/s #Scale to have lf.Qstd std\n",
    "        new_qVecs = tmp - tf.reduce_mean(tmp, axis=-1, keep_dims=True) # zero mean\n",
    "        \n",
    "        return new_qVecs\n",
    "    \n",
    "    def attention(self, q_embed):\n",
    "        '''\n",
    "        Given B's output vector, calculate the attention over \n",
    "        all the query's bounding boxes vectors, That is, calculate:\n",
    "        \n",
    "        probs = softmax(relu(context(Sq+Satt+b)))\n",
    "        \n",
    "        Where:\n",
    "        Sq = <Wq, queries_states>\n",
    "        Sattn = <Wattn, attention_bboxes_vectors>\n",
    "        \n",
    "        The  bounding box with the highest attention score will be chosen as the correct bounding box.\n",
    "        \n",
    "        Params:\n",
    "            q_embed: Tensor of shape (batch size x num_hidden)B's outputs. \n",
    "            \n",
    "        Returns:\n",
    "            probs: Tensor of shape (batch_size x max bbox number for query).\n",
    "                   Attention score for each bbox.\n",
    "        '''\n",
    "        # concatenate img vectors with spaical features\n",
    "        attn_vecs = tf.concat([self.img, self.bboxes], 2)\n",
    "        \n",
    "        # B's outputs, shape: (batch size x num_hidden)\n",
    "        if self.toQscale:\n",
    "            Urnn = self.Qscale(q_embed)\n",
    "        else:\n",
    "            Urnn = q_embed\n",
    "        \n",
    "        # Attention vectors, \n",
    "        # shape: (batch size x max bbox number for query x attention vector size)\n",
    "        Uatt = attn_vecs\n",
    "           \n",
    "        with tf.variable_scope('l1'):\n",
    "            b = tf.get_variable(\n",
    "                    'b', \n",
    "                    initializer=tf.constant_initializer(0.1), \n",
    "                    shape=[1, self.num_hidden])\n",
    "\n",
    "            context = tf.get_variable(\n",
    "                    'context', \n",
    "                    initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1), \n",
    "                    shape=[self.num_hidden, 1])\n",
    "\n",
    "\n",
    "            Sq = tf.nn.dropout(\n",
    "                self.linear(Urnn, self.num_hidden, bias=False, scope='Sq'), \n",
    "                self.dropout_q)\n",
    "            \n",
    "            Sattn = tf.nn.dropout(\n",
    "                tf.reshape(\n",
    "                    self.linear(\n",
    "                        tf.reshape(Uatt, (-1, self.img_dims+self.bbox_dims)), \n",
    "                        self.num_hidden, \n",
    "                        bias=False, scope='Sattn'), \n",
    "                    [self.batch_size, -1, self.num_hidden]),\n",
    "                self.dropout_img)\n",
    "\n",
    "        out = tf.nn.relu(tf.expand_dims(Sq, 1) + Sattn + b)\n",
    "        logits = tf.reshape(tf.matmul(tf.reshape(out, (-1, tf.shape(out)[-1])),  context), (tf.shape(out)[0], -1))\n",
    "\n",
    "        # Calculate logits's masked softmax (we use self.attn_idx to mas\n",
    "        max_logits = tf.reduce_max(logits, axis=-1)\n",
    "        masked_logits = tf.exp(logits-tf.expand_dims(max_logits, axis=1))*self.attn_idx\n",
    "        probs = self.attn_idx*masked_logits/tf.reduce_sum(masked_logits, axis=-1, keep_dims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    \n",
    "    def bnorm_attention(self, q_embed, Urnn_norm=True, Uatt_norm=True):\n",
    "        '''\n",
    "        Given B's output vector, calculate the attention over \n",
    "        all the query's bounding boxes vectors, That is, calculate:\n",
    "        \n",
    "        probs = softmax(relu(context(Sq+Satt+b)))\n",
    "        \n",
    "        Where:\n",
    "        Sq = <Wq, queries_states>\n",
    "        Sattn = <Wattn, attention_bboxes_vectors>\n",
    "        \n",
    "        The  bounding box with the highest attention score will be chosen as the correct bounding box.\n",
    "        This function uses batch normalization. \n",
    "        \n",
    "        Params:\n",
    "            q_embed: Tensor of shape (batch size x num_hidden)B's outputs. \n",
    "            Urnn_norm: Whether to use batch normalization for the queries.\n",
    "            Uatt_norm: Whether to use batch normalization for the VGG outputs.\n",
    "            \n",
    "        Returns:\n",
    "            probs: Tensor of shape (batch_size x max bbox number for query).\n",
    "                   Attention score for each bbox.\n",
    "        '''\n",
    "        # concatenate img vectors with with spaical features\n",
    "        attn_vecs = tf.concat([self.img, self.bboxes], 2)\n",
    "        Urnn = q_embed\n",
    "        Uatt = attn_vecs\n",
    "        \n",
    "        if Urnn_norm:\n",
    "            # B's outputs with bath normalization. \n",
    "            # shape: (batch size x num_hidden)\n",
    "            Urnn = tf.contrib.layers.batch_norm(\n",
    "                q_embed, center=True, scale=True, is_training=self.isTrain) \n",
    "        \n",
    "        if Uatt_norm:\n",
    "            # Attention vectors with bath normalization. \n",
    "            # shape: (batch size x max bbox number for query x attention vector size)\n",
    "            Uatt = tf.contrib.layers.batch_norm(\n",
    "                attn_vecs, center=True, scale=True, is_training=self.isTrain)\n",
    "           \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            with tf.variable_scope('bnorm_l1'):\n",
    "                b = tf.get_variable(\n",
    "                        'b', \n",
    "                        initializer=tf.constant_initializer(0.1), \n",
    "                        shape=[1, self.num_hidden])\n",
    "\n",
    "                context = tf.get_variable(\n",
    "                        'context', \n",
    "                        initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1), \n",
    "                        shape=[self.num_hidden, 1])\n",
    "\n",
    "\n",
    "                Sq = tf.nn.dropout(\n",
    "                    self.linear(Urnn, self.num_hidden, bias=False, scope='Sq'), \n",
    "                    self.dropout_q)\n",
    "                \n",
    "                Sattn = tf.nn.dropout(\n",
    "                            tf.reshape(\n",
    "                                self.linear(\n",
    "                                    tf.reshape(Uatt, (-1, self.img_dims+self.bbox_dims)), \n",
    "                                    self.num_hidden, \n",
    "                                    bias=False, scope='Sattn'), \n",
    "                                 [self.batch_size, -1, self.num_hidden]),\n",
    "                            self.dropout_img)\n",
    "                    \n",
    "                   \n",
    "            out = tf.nn.relu(tf.expand_dims(Sq, 1) + Sattn + b)\n",
    "            logits = tf.reshape(tf.matmul(tf.reshape(out, (-1, tf.shape(out)[-1])),  context), (tf.shape(out)[0], -1))\n",
    "\n",
    "            # Calculate logits's masked softmax (we use self.attn_idx to mas\n",
    "            max_logits = tf.reduce_max(logits, axis=-1)\n",
    "            masked_logits = tf.exp(logits-tf.expand_dims(max_logits, axis=1))*self.attn_idx\n",
    "            probs = self.attn_idx*masked_logits/tf.reduce_sum(masked_logits, axis=-1, keep_dims=True)\n",
    "\n",
    "            return probs\n",
    "  \n",
    "        \n",
    "    def q_padding(self, seq, max_length):\n",
    "        '''\n",
    "        Pad  seq with vocab['<pad>'] (0) to max_length length.\n",
    "        '''                  \n",
    "        return seq + [self.vocab['<pad>']]*(max_length-len(seq))\n",
    "\n",
    "    \n",
    "    def build_data(self, data, start, end, imScale=None, addNoise=False):\n",
    "        '''\n",
    "        Build batch.\n",
    "        ------------\n",
    "        \n",
    "        Params:\n",
    "            data: each entry in this list has the following structure:\n",
    "                  [query indexes, [bounding box vector (VGG), bounding box spaital features], ..., \n",
    "                  [bounding box vector (VGG), bounding box spaital features], index of the true label]\n",
    "                  \n",
    "            start/end: batch data is built from data[start:end]\n",
    "            \n",
    "        Returns:\n",
    "            attn_idx: attn_idx[i, j]=1 if the j'th bbox in the i'th query is not padding, else equals to 0. \n",
    "            \n",
    "            padded_queries: list of queries, padded to the length of the longest query in the batch.\n",
    "                            Note: vocab['pad']=0\n",
    "                            \n",
    "            padded_im: list of bounding boxes vectors, padded to the maximum number of bbox per query.\n",
    "                       Note: padded vector is vector of zeros. \n",
    "                            \n",
    "            padded_bbox: list of bounding boxes spatial features, padded to the maximum number of bbox per query.\n",
    "                         Note: padded vector is vector of zeros.  \n",
    "        \n",
    "            onehot_labels: onehot_labels[i][j]=1 if j is the true bbox for query i, else  onehot_labels[i][j]=0\n",
    "            \n",
    "            addNoise: Boolean. Whether to add normal noise to the images.\n",
    "            \n",
    "            imScale: If not None, scale the image vectors (VGG16 outputs) to have 0 mean and 1/imScale std\n",
    "                        \n",
    "        '''\n",
    "                          \n",
    "        qlen = max([len(data[i][0]) for i in range(start, end)]) # Length fo the longest query\n",
    "        imlen = max([len(data[i]) for i in range(start, end)])-2 # Maximum number of bbox per query.\n",
    "        padded_queries, padded_im, padded_bbox, attn_idx = [], [], [], []\n",
    "        \n",
    "        # Build one hot labels from the labels index, given in the data.                  \n",
    "        labels = [item[-1] for item in data[start:end]] #data[i][-1]=index of the true bbox of query i\n",
    "        dist_labels = np.zeros((end-start, imlen)) #label distribution\n",
    "        \n",
    "        # Real data points\n",
    "        dist_labels[[i for i in np.arange(end-start) if labels[i]>0], [l for l in labels if l>0]]=1\n",
    "        \n",
    "        # augmented data points\n",
    "        for i in np.arange(end-start):\n",
    "            if labels[i]<0:\n",
    "                dist_labels[i] = [-1/labels[i] for _ in range(-labels[i])]+[0. for _ in range(imlen+labels[i])]\n",
    "                          \n",
    "        im_dim, bbox_dim = data[0][1][0].shape[1], data[0][1][1].shape[1]\n",
    "        for i in range(start, end):\n",
    "            padded_queries.append(self.q_padding(data[i][0], qlen))\n",
    "            \n",
    "            attn_idx.append([1 for _ in range(len(data[i])-2)]+[0 for _ in range(imlen-(len(data[i])-2))])\n",
    "            \n",
    "            padded_im.append(np.concatenate([data[i][j][0] for j in range(1, len(data[i])-1)] + \n",
    "                                       [np.full((imlen-(len(data[i])-2), im_dim), vocab['<pad>'], dtype=np.float32)], axis=0))\n",
    "            \n",
    "            padded_bbox.append(np.concatenate([data[i][j][1] for j in range(1, len(data[i])-1)] + \n",
    "                                       [np.full((imlen-(len(data[i])-2),bbox_dim), vocab['<pad>'], dtype=np.float32)], axis=0))\n",
    "           \n",
    "        \n",
    "        \n",
    "        if addNoise:\n",
    "            padded_im+=(padded_im+np.random.normal(0, .1, np.array(padded_im).shape))*np.expand_dims(attn_idx, 2)\n",
    "        else:\n",
    "            padded_im=np.array(padded_im)\n",
    "            \n",
    "        if imScale is not None:\n",
    "            # Smoothing factor\n",
    "            halper = (1-np.expand_dims(np.array(attn_idx).astype(np.float32), 2))\n",
    "            img_min = np.min(padded_im, axis=-1, keepdims=True)\n",
    "            img_max = np.max(padded_im, axis=-1, keepdims=True)\n",
    "            img_scale = (padded_im-img_min)/((img_max-img_min)+halper) # Scale to 0-1\n",
    "            s = np.std(img_scale, axis=-1, keepdims=True)\n",
    "            padded_im = img_scale/(imScale*s+halper)  #Scale to have 1/imScale std\n",
    "            padded_im = padded_im - np.mean(padded_im, axis=-1, keepdims=True) # scale to zero mean\n",
    "            \n",
    "        return np.array(attn_idx), np.array(padded_queries, dtype=np.int32), padded_im, np.array(padded_bbox), np.array(dist_labels)\n",
    "            \n",
    "   \n",
    "    def ground(self, data=None, start=None, end=None, sess=None, feed_dict = None, isEdit=True, imScale=False):\n",
    "        '''\n",
    "        Given a query and a list of bboxes, the function returns the index of the referred bbox.\n",
    "        '''\n",
    "        isSess = (sess==None)\n",
    "        if isSess:\n",
    "            sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            if isSess:\n",
    "                tf.global_variables_initializer().run()\n",
    "                ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "                else:\n",
    "                    print('Initializing variables')\n",
    "            if feed_dict is None:\n",
    "                attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(data, start, end, imScale=imScale)\n",
    "                feed_dict = {\n",
    "                        self.queries:padded_queries,\n",
    "                        self.img:padded_im,\n",
    "                        self.bboxes:padded_bbox,\n",
    "                        self.labels:labels,\n",
    "                        self.attn_idx:attn_idx\n",
    "                    }\n",
    "            feed_dict[self.isTrain]=False\n",
    "            feed_dict[self.isEdit] = isEdit\n",
    "            feed_dict[self.dropout_in]=1.\n",
    "            feed_dict[self.dropout_out]=1.\n",
    "            feed_dict[self.dropout_img]=1.\n",
    "            feed_dict[self.dropout_q]=1.\n",
    "            scores = sess.run(self.scores, feed_dict=feed_dict) # get score for each bbox\n",
    "\n",
    "        return np.argmax(scores, axis=1), np.argmax(feed_dict[self.labels], axis=1)\n",
    "        \n",
    "        \n",
    "    def iou_accuracy(self, data, start, end, imScale, sess=None, feed_dict = None, threshold=0.5, test=False, isEdit=True):\n",
    "        '''\n",
    "        Calculate the IOU score between the Model bbox and the true bbox.\n",
    "        ''' \n",
    "                          \n",
    "        # Get score for each bbox (labels) and th true bbox index (gt_idx)                  \n",
    "        if feed_dict is None:\n",
    "            labels, gt_idx = self.ground(data, start, end, sess=sess, feed_dict=feed_dict, \n",
    "                                         isEdit=isEdit, imScale=imScale)\n",
    "        else: labels, gt_idx = self.ground(sess=sess, feed_dict=feed_dict, \n",
    "                                           isEdit=isEdit, imScale=imScale)\n",
    "        acc = 0\n",
    "        \n",
    "        for i in range(start, end):\n",
    "            gt = data[i][gt_idx[i-start]+1][1][0] # ground truth bbox\n",
    "            crops = np.expand_dims(data[i][labels[i-start]+1][1][0], axis=0) #Model chosen bbox\n",
    "            acc += (retriever.compute_iou(crops, gt)[0]>threshold) #IOU for the i sample.\n",
    "            \n",
    "        return acc/(end-start)\n",
    "        \n",
    "    def accuracy(self, imScale, data=None, start=None, end=None, sess=None, feed_dict = None, isEdit=True):\n",
    "        isSess = (sess==None)\n",
    "        if isSess:\n",
    "            print('Building sess')\n",
    "            sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            if isSess:\n",
    "                print('Building sess used')\n",
    "                tf.global_variables_initializer().run()\n",
    "                ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    print('3')\n",
    "                    self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "                else:\n",
    "                    print('Initializing variables')\n",
    "            if feed_dict is None:\n",
    "                print('Building feed_dict')\n",
    "                attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(data, start, end, imScale=imScale)\n",
    "                feed_dict = {\n",
    "                        self.queries:padded_queries,\n",
    "                        self.img:padded_im,\n",
    "                        self.bboxes:padded_bbox,\n",
    "                        self.attn_idx:attn_idx,\n",
    "                        self.labels:labels,\n",
    "                    }\n",
    "                \n",
    "            feed_dict[self.isTrain]=False\n",
    "            feed_dict[self.isEdit] = isEdit\n",
    "            feed_dict[self.dropout_in]=1.\n",
    "            feed_dict[self.dropout_out]=1.\n",
    "            feed_dict[self.dropout_img]=1.\n",
    "            feed_dict[self.dropout_q]=1.\n",
    "            scores = sess.run(self.scores, feed_dict=feed_dict)\n",
    "            acc = sum(np.equal(np.argmax(scores, axis=1), np.argmax(feed_dict[self.labels], axis=1))/len(feed_dict[self.labels]))\n",
    "\n",
    "                    \n",
    "        return acc\n",
    "    \n",
    "    def discount_rewards(self, r, last_r, gamma=1.0):\n",
    "        \"\"\" \n",
    "        take 1D float array of rewards and compute discounted reward \n",
    "        using bellman function.\n",
    "        \n",
    "        params:\n",
    "            r: r[i,j]=1 reward of action A did on word j at query i.\n",
    "            last_r: B's loss for query i with no edits. This is the final reward.\n",
    "            gamma: discount factor.\n",
    "        \"\"\"\n",
    "                          \n",
    "        discounted_r = np.zeros(r.shape)\n",
    "        running_add = last_r # B loss\n",
    "        discounted_r = [i for i in range(r.shape[-1])]\n",
    "        for t in reversed(range(0, r.shape[-1])):\n",
    "            running_add = running_add * gamma + r[:,t]\n",
    "            discounted_r[t] = running_add\n",
    "        return np.array(discounted_r).T\n",
    "        \n",
    "    def train(self, trn_data, tst_data, ephocs_num, edit_reward, startA=3, \n",
    "              activation_ephoc=10, muteB=3, start_ephoc=0, dropout_in=1.,\n",
    "              dropout_out=1., dropout_img=1., dropout_q=1., editProb=0.5, max_activateAProb=0.8, \n",
    "              activateAProb = 0.5, addNoise=False, imScale=None):\n",
    "                          \n",
    "        '''\n",
    "        Params:\n",
    "             trn_data: list, train set. \n",
    "             \n",
    "             tst_data: list, test set. \n",
    "             \n",
    "             ephocs_num: number of ephocs\n",
    "             \n",
    "             start_ephoc: number of first ephoc.\n",
    "             \n",
    "             edit_reward: int, coefficient to multiply the reward by when editing a word.\n",
    "             \n",
    "             startA: int, Start competition only at ephoc # startA.\n",
    "             \n",
    "             activation_ephoc: at ephoc numer \"activation_ephoc\", A will be activate.\n",
    "                               That is, for (activation_ephoc-startA) number of ephocs, \n",
    "                               A will chooce an action randomly.\n",
    "             \n",
    "            muteB: After A starts, for each ephoc which A & B trains, \n",
    "                   only A will be trained for this amount of ephocs.\n",
    "                   \n",
    "            editProb: robabilty for editing a query.\n",
    "            \n",
    "            activateAProb: when running A, we can choos an action randomly or taknig A decision. \n",
    "                            This is the starting probabilty for NOT choocing an action ranomdly.\n",
    "            \n",
    "            max_activateAProb: Final probabilty for NOT choocing an action ranomdly.\n",
    "            \n",
    "            dropout_in: dropout ratio of B's rnn inputs.\n",
    "            \n",
    "            dropout_output: dropout ratio of B's rnn output.\n",
    "            \n",
    "            dropout_img: dropout ratio of images vectors before the last attention layer .\n",
    "            \n",
    "            addNoise: Boolean. Whether to add normal noise to the images (see build_data).\n",
    "            \n",
    "            imScale: If not None, scale the image vectors (VGG16 outputs) to have 0 mean and 1imScale std\n",
    "                               \n",
    "        '''                  \n",
    "        \n",
    "        trn_nbatch = len(trn_data)\n",
    "        tst_nbatch = len(tst_data)\n",
    "        print('# Train set size:', len([len(batch) for batch in trn_data]))\n",
    "        print('# Training batches:', trn_nbatch)\n",
    "        print('# Test set size:', len([len(batch) for batch in tst_data]))\n",
    "        print('# Testing batches:', tst_nbatch)\n",
    "        self.test_res, self.train_res = [], [] #list to hold accuracy of test set\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            tf.global_variables_initializer().run()\n",
    "            ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                print('Loading parameters from', ckpt.model_checkpoint_path)\n",
    "                self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "            else:\n",
    "                print('Initializing variables')\n",
    "                \n",
    "            for ephoc in range(start_ephoc, ephocs_num):\n",
    "                startTime = datetime.now().replace(microsecond=0)\n",
    "                          \n",
    "                # Train B only after competition start (ephoc<startA). \n",
    "                # Each time we train B, we train only B for muteB ephocs.\n",
    "                if startA>0:\n",
    "                    toTrainB = ephoc<startA or (ephoc-(startA-1))%(muteB+1)==0 # Whether to train only B \n",
    "                else:\n",
    "                    toTrainB = ephoc==0 or ephoc%(muteB+1)==0 # Whether to train only B \n",
    "                    \n",
    "                # Every 2 times we train B and A (+ only A for muteB ephocs), the\n",
    "                # probability for choosing an action using A and not randomly, increases by 1.1.\n",
    "                if activateAProb>0 and activateAProb<max_activateAProb and muteB>0 and (ephoc-activation_ephoc)%((muteB+1)*6)==0 and ephoc>activation_ephoc:\n",
    "                    activateAProb=activateAProb*1.1\n",
    "                        \n",
    "                itrInEphoc=1 # When we train B, we show B results without editing (on train set)\n",
    "                if activateAProb>0 and ephoc>=startA and toTrainB:\n",
    "                    itrInEphoc=2\n",
    "                    \n",
    "                print('='*50,'\\nTrain, ephoc:',ephoc, '; Training B:', toTrainB)\n",
    "                np.random.shuffle(trn_data)\n",
    "                for ep in range(itrInEphoc):\n",
    "                    trn_ce_loss, A_trn_loss, B_trn_loss = 0, 0, 0\n",
    "                    trn_acc, trn_iou, edits_count = 0, 0, 0\n",
    "                    tst_ce_loss, tst_loss, tst_acc, tst_iou = 0, 0, 0, 0\n",
    "                    if ep==1:\n",
    "                        print('No Edit')\n",
    "                        print('ooooooo')\n",
    "                    edit_num = 0 # number of edited words\n",
    "                    editRount_count = 0 # number of iterations with competition.\n",
    "                    for b in range(trn_nbatch):\n",
    "                        if ep==1:\n",
    "                            '''\n",
    "                            If ep=1, we're in round 2 in which we only show B's \n",
    "                            results with out editing (on the train set).\n",
    "                            '''\n",
    "                            isEdit=False\n",
    "                            isActivateA=False\n",
    "                        else:\n",
    "                            isEdit = ephoc>=startA and (toTrainB==False or np.random.rand(1)[0]<editProb)\n",
    "                            # Whether to choose an action using A or randomly.\n",
    "                            isActivateA = (toTrainB==True and activateAProb>0.) or (isEdit and \n",
    "                                                                                    ephoc>=activation_ephoc and \n",
    "                                                                                    np.random.rand(1)[0]<activateAProb)\n",
    "            \n",
    "                        attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(trn_data[b], \n",
    "                                                                                            0, \n",
    "                                                                                            self.batch_size, \n",
    "                                                                                            addNoise=addNoise,\n",
    "                                                                                            imScale=imScale)\n",
    "                \n",
    "                        reward_loss = np.array([[0., 0.] for _ in range(self.batch_size)]) # dummy holder \n",
    "                    \n",
    "\n",
    "                        feed_dict = {\n",
    "                            self.queries:padded_queries,\n",
    "                            self.img:padded_im,\n",
    "                            self.bboxes:padded_bbox,\n",
    "                            self.attn_idx:attn_idx,\n",
    "                            self.labels: labels,\n",
    "                            self.unk:np.array([[vocab['<unk>']]]),\n",
    "                            self.isEdit:isEdit,\n",
    "                            self.activateA:isActivateA,\n",
    "                            self.reward_loss:reward_loss,\n",
    "                            self.Aattn_vecs:[[[]]],# dummy holder \n",
    "                            self.dropout_in:dropout_in,\n",
    "                            self.dropout_out:dropout_out,\n",
    "                            self.dropout_img:dropout_img,\n",
    "                            self.dropout_q:dropout_q,\n",
    "                            self.isTrain:True\n",
    "                        }\n",
    "                        \n",
    "                        \n",
    "                        if isEdit:\n",
    "                            editRount_count+=1\n",
    "                          \n",
    "                            '''We first run B with no edit in order to get it's loss and outputs.'''\n",
    "                          \n",
    "                            feed_dict[self.isEdit] = False\n",
    "                            queries_lens, B_ce, outputs = sess.run([self.queries_lens, self.B_ce, self.outputs], feed_dict=feed_dict)\n",
    "                            feed_dict[self.isEdit] = True\n",
    "                            \n",
    "                            # B_ce contain the loss for each query.\n",
    "                            # We scale all the losses to have mean 1 and std 0.5.\n",
    "                            Bce_min = min(B_ce)\n",
    "                            Bce_max = max(B_ce)\n",
    "                            Bce_scale = (B_ce-Bce_min)/(Bce_max-Bce_min)\n",
    "                            s = np.std(Bce_scale)\n",
    "                            BCE = Bce_scale/(2.*s)\n",
    "                            BCE = 1 + BCE - np.mean(BCE)\n",
    "                          \n",
    "                            # reward for edit a word = edit_reward*BCE/queries_lens.\n",
    "                            # A's get the reward and loss per query  as 2 of its features.\n",
    "                            rewards_loss = np.concatenate(\n",
    "                                [np.expand_dims(edit_reward*BCE/queries_lens, 1), np.expand_dims(BCE, 1)], 1)\n",
    "                            \n",
    "                            feed_dict[self.reward_loss]=rewards_loss\n",
    "                            \n",
    "                            # A's attention vectors are B's outputs with no edited words.\n",
    "                            Aattn_vecs = outputs[:,:,:self.num_hidden]\n",
    "                            feed_dict[self.Aattn_vecs]=Aattn_vecs\n",
    "                           \n",
    "                            '''\n",
    "                            We then run B with A edits. \n",
    "                            We get idx (A's decision [0,1] for each words), edit_num: the ratio\n",
    "                            of edited words and B_ce, B_loss: loss poer query and B_vce mean.\n",
    "                            '''\n",
    "                            \n",
    "                            idx, B_ce, B_loss, edit_num = sess.run(\n",
    "                                [self.idx, self.B_ce, self.B_loss, self.edit_num], \n",
    "                                 feed_dict=feed_dict)\n",
    "                            \n",
    "                            \n",
    "                            # Get rewurd per word.\n",
    "                            # If word i in query j was edited idx[j,i]=1 --> rewards[i,j] = 1*reward\n",
    "                            # If word i in query j was not edited idx[j,i]=0 --> rewards[i,j] = 0*reward\n",
    "                            rewards = idx*np.expand_dims(\n",
    "                                edit_reward*BCE/queries_lens, axis=1)\n",
    "                            \n",
    "                            # Scale new losses\n",
    "                            Bce_min = min(B_ce)\n",
    "                            Bce_max = max(B_ce)\n",
    "                            Bce_scale = (B_ce-Bce_min)/(Bce_max-Bce_min)\n",
    "                            s = np.std(Bce_scale)\n",
    "                            BCE1 = Bce_scale/(2.*s)\n",
    "                            BCE1 = 1 + BCE1 - np.mean(BCE1)\n",
    "                            \n",
    "                            feed_dict[self.actions_idx] = np.abs(list(zip(range(\n",
    "                                        self.batch_size*idx.shape[-1]), idx.reshape(-1))))\n",
    "                            feed_dict[self.bell_val] = self.discount_rewards(rewards, BCE1)\n",
    "                                                         \n",
    "                            A_loss, _ = sess.run(\n",
    "                                [self.A_loss, self.A_optimizer], feed_dict=feed_dict)\n",
    "                            \n",
    "                            if toTrainB:\n",
    "                                lr, gs, _ = sess.run(\n",
    "                                    [self.learning_rate, self.global_step, self.B_optimizer], \n",
    "                                    feed_dict=feed_dict)\n",
    "                \n",
    "                    \n",
    "                            acc = self.accuracy(sess=sess, feed_dict=feed_dict, isEdit=isEdit, imScale=imScale)  \n",
    "                            iou_acc = self.iou_accuracy(\n",
    "                                trn_data[b], 0, self.batch_size, imScale=imScale,\n",
    "                                sess=sess, feed_dict=feed_dict, isEdit=isEdit)\n",
    "\n",
    "                            trn_acc += acc/trn_nbatch\n",
    "                            A_trn_loss += A_loss\n",
    "                            B_trn_loss += B_loss/trn_nbatch\n",
    "                            trn_iou += iou_acc/trn_nbatch\n",
    "                            edits_count += edit_num\n",
    "                            \n",
    "                            if b%50==0:\n",
    "                                if toTrainB: \n",
    "                                    print('Edit:', feed_dict[self.isEdit], ';A:',isActivateA, \n",
    "                                          ';ephoc:',ephoc, ';batch:', b, \n",
    "                                          ';gs:', gs, ';lr: %.4f'%(lr), ';B loss:%.3f'%(B_loss), \n",
    "                                          ';A loss:%.3f'%(A_loss), ';edits:%.2f'%(edit_num),  ';activateAProb:%.1f'%(activateAProb),\n",
    "                                          ';acc: %.3f'%(acc), ';iou: %.3f'%(iou_acc), ';time:', datetime.now().replace(microsecond=0)-startTime)    \n",
    "                                else:\n",
    "                                    print('Edit:', feed_dict[self.isEdit], ';A:',isActivateA, \n",
    "                                          ';ephoc:',ephoc, ';batch:', b, \n",
    "                                          ';Bloss:%.3f'%(B_loss), ';Aloss:%.3f'%(A_loss), \n",
    "                                          ';edits: %.2f'%(edit_num), ';activateAProb:%.1f'%(activateAProb), ';acc: %.3f'%(acc),\n",
    "                                          ';iou: %.3f'%(iou_acc), ';time:', datetime.now().replace(microsecond=0)-startTime)  \n",
    "                                \n",
    "\n",
    "                               \n",
    "                        else: # isEdit=False\n",
    "                            if ep==0:\n",
    "                                B_loss, lr, gs, _ = sess.run([self.B_loss, self.learning_rate, \n",
    "                                                                self.global_step, self.B_optimizer], feed_dict=feed_dict)\n",
    "                            else: \n",
    "                                # just show results of B with no edits\n",
    "                                B_loss, lr, gs = sess.run([self.B_loss, self.learning_rate, self.global_step], feed_dict=feed_dict)\n",
    "\n",
    "                            acc = self.accuracy(sess=sess, feed_dict=feed_dict, isEdit=isEdit, imScale=imScale)  \n",
    "                            iou_acc = self.iou_accuracy(trn_data[b], 0, self.batch_size, imScale=imScale,\n",
    "                                                        sess=sess, feed_dict=feed_dict, isEdit=isEdit)\n",
    "\n",
    "                            trn_acc += acc/trn_nbatch\n",
    "                            B_trn_loss += B_loss/trn_nbatch\n",
    "                            trn_iou += iou_acc/trn_nbatch\n",
    "\n",
    "                            if b%50==0:\n",
    "                                print('Edit:', feed_dict[self.isEdit],  ';A:',isActivateA, ';batch:', b, ';gs:', gs, ';lr: %.4f'%(lr), \n",
    "                                      ';B loss: %.3f'%(B_loss),  ';acc: %.3f'%(acc), \n",
    "                                      ';iou: %.3f'%(iou_acc),\n",
    "                                      ';time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "\n",
    "                    if editRount_count>0:\n",
    "                        print('\\n*Training B:', ephoc%3==0, ';B Train loss: %.3f'%(B_trn_loss), ';A Train loss: %.3f'%(A_trn_loss/editRount_count),                                                                                             \n",
    "                          ';Edit num: %.3f'%(edits_count/editRount_count), \n",
    "                          ';Train accuracy: %.3f'%(trn_acc),  ';IOU accuracy: %.3f'%(trn_iou), \n",
    "                          ';Time:', datetime.now().replace(microsecond=0)-startTime, '\\n')\n",
    "                    else:\n",
    "                        print('\\n*Training B:', ephoc%3==0, ';B Train loss: %.3f'%(B_trn_loss),';Train accuracy: %.3f'%(trn_acc), \n",
    "                          ';IOU accuracy: %.3f'%(trn_iou),  \n",
    "                          ';Time:', datetime.now().replace(microsecond=0)-startTime, '\\n')\n",
    "                          \n",
    "                if editRount_count>0:\n",
    "                    self.train_res.append([trn_acc, trn_iou, B_trn_loss, A_trn_loss/editRount_count])\n",
    "                else: \n",
    "                    self.train_res.append([trn_acc, trn_iou, B_trn_loss, 0])\n",
    "                self.saver.save(sess, params_dir + \"/model.ckpt\", global_step=ephoc)    \n",
    "                if ephoc<startA or toTrainB:\n",
    "                    print('Testing, ephoc:',ephoc)\n",
    "                    tstTime = datetime.now().replace(microsecond=0)\n",
    "                    tst_loss, tst_acc, tst_iou = 0, 0, 0\n",
    "                    for b in range(tst_nbatch):\n",
    "                        attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(tst_data[b],\n",
    "                                                                                    0, self.batch_size, imScale=imScale)\n",
    "                        rewards_loss = np.array([[0., 0.] for _ in range(self.batch_size)])\n",
    "                        feed_dict = {\n",
    "                            self.queries:padded_queries,\n",
    "                            self.img:padded_im,\n",
    "                            self.bboxes:padded_bbox,\n",
    "                            self.attn_idx:attn_idx,\n",
    "                            self.labels: labels,\n",
    "                            self.unk:np.array([[vocab['<unk>']]]),\n",
    "                            self.reward_loss:reward_loss,\n",
    "                            self.isEdit:False,\n",
    "                            self.activateA:False,\n",
    "                            self.Aattn_vecs:[[[]]],\n",
    "                            self.dropout_in:1.,\n",
    "                            self.dropout_out:1.,\n",
    "                            self.dropout_img:1.,\n",
    "                            self.dropout_q:1.,\n",
    "                            self.isTrain:False\n",
    "                        }\n",
    "                        B_loss = sess.run(self.B_loss, feed_dict=feed_dict)\n",
    "\n",
    "                        acc = self.accuracy(sess=sess, feed_dict=feed_dict, isEdit=False, imScale=imScale)\n",
    "                        iou_acc = self.iou_accuracy(\n",
    "                            tst_data[b], 0, self.batch_size, sess=sess, \n",
    "                            feed_dict=feed_dict, isEdit=False, imScale=imScale)\n",
    "                        \n",
    "                        tst_acc += acc/tst_nbatch\n",
    "                        tst_loss += B_loss/tst_nbatch\n",
    "                        tst_iou += iou_acc/tst_nbatch\n",
    "                        if b%50==0:\n",
    "                            print('batch:', b, ';B loss: %.3f'%(B_loss), ';acc: %.3f'%(acc), \n",
    "                                   ';iou_acc: %.3f'%(iou_acc), ';time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "                    print('\\n*BTrain:', ephoc%3==0, ';Test loss: %.3f'%(tst_loss), ';Test accuracy %.3f'%(tst_acc), \n",
    "                          ';IOU accuracy: %.3f'%(tst_iou), ';Time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "                    self.test_res.append([tst_acc, tst_iou, tst_loss])\n",
    "                print('='*50,'\\n')\n",
    "            return self.test_res, self.train_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We start by runnging the Grounder in its simplest form\n",
    "<p> We test the model with different state sizes: 50, 100, 150 and 200. We run each test for 100 ephocs</p>\n",
    "<p> We get about 60% IOU in all the test, about the same as the baseline.<br> On the other hand, the train set IOU gets bigger as the hidden state gets bigger, starting from about 89%, for 50 hidden units, to about 97% with 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params_dir: ../data/training/models/All/RL/all_hidden:200\n",
      "num_hidden: 200\n",
      "learning rate: 0.05\n",
      "# Train set size: 297\n",
      "# Training batches: 297\n",
      "# Test set size: 297\n",
      "# Testing batches: 297\n",
      "Loading parameters from ../data/training/models/All/RL/all_hidden:200/model.ckpt-38\n",
      "INFO:tensorflow:Restoring parameters from ../data/training/models/All/RL/all_hidden:200/model.ckpt-38\n",
      "================================================== \n",
      "Train, ephoc: 39 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 39 ;batch: 0 ;Bloss:2.819 ;Aloss:0.218 ;edits: 0.44 ;activateAProb:1.0 ;acc: 0.185 ;iou: 0.345 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 39 ;batch: 50 ;Bloss:1.908 ;Aloss:0.211 ;edits: 0.28 ;activateAProb:1.0 ;acc: 0.555 ;iou: 0.685 ;time: 0:00:15\n",
      "Edit: True ;A: True ;ephoc: 39 ;batch: 100 ;Bloss:1.812 ;Aloss:0.308 ;edits: 0.21 ;activateAProb:1.0 ;acc: 0.615 ;iou: 0.705 ;time: 0:00:27\n",
      "Edit: True ;A: True ;ephoc: 39 ;batch: 150 ;Bloss:1.574 ;Aloss:0.317 ;edits: 0.35 ;activateAProb:1.0 ;acc: 0.735 ;iou: 0.810 ;time: 0:00:41\n",
      "Edit: True ;A: True ;ephoc: 39 ;batch: 200 ;Bloss:1.724 ;Aloss:0.308 ;edits: 0.43 ;activateAProb:1.0 ;acc: 0.635 ;iou: 0.715 ;time: 0:00:54\n",
      "Edit: True ;A: True ;ephoc: 39 ;batch: 250 ;Bloss:1.512 ;Aloss:0.188 ;edits: 0.24 ;activateAProb:1.0 ;acc: 0.735 ;iou: 0.785 ;time: 0:01:07\n",
      "\n",
      "*Training B: True ;B Train loss: 1.982 ;A Train loss: 0.224 ;Edit num: 0.257 ;Train accuracy: 0.553 ;IOU accuracy: 0.654 ;Time: 0:01:19 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 40 ; Training B: True\n",
      "Edit: False ;A: True ;batch: 0 ;gs: 3862 ;lr: 0.0500 ;B loss: 1.893 ;acc: 0.670 ;iou: 0.775 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 40 ;batch: 50 ;gs: 3912 ;lr: 0.0500 ;B loss:1.913 ;A loss:0.125 ;edits:0.06 ;activateAProb:1.0 ;acc: 0.580 ;iou: 0.685 ;time: 0:00:16\n",
      "Edit: True ;A: True ;ephoc: 40 ;batch: 100 ;gs: 3962 ;lr: 0.0500 ;B loss:2.549 ;A loss:0.192 ;edits:0.27 ;activateAProb:1.0 ;acc: 0.455 ;iou: 0.625 ;time: 0:00:33\n",
      "Edit: True ;A: True ;ephoc: 40 ;batch: 150 ;gs: 4012 ;lr: 0.0500 ;B loss:1.540 ;A loss:0.316 ;edits:0.21 ;activateAProb:1.0 ;acc: 0.750 ;iou: 0.810 ;time: 0:00:48\n",
      "Edit: True ;A: True ;ephoc: 40 ;batch: 200 ;gs: 4062 ;lr: 0.0500 ;B loss:2.986 ;A loss:0.289 ;edits:0.42 ;activateAProb:1.0 ;acc: 0.235 ;iou: 0.380 ;time: 0:01:04\n",
      "Edit: True ;A: True ;ephoc: 40 ;batch: 250 ;gs: 4112 ;lr: 0.0500 ;B loss:1.567 ;A loss:0.217 ;edits:0.19 ;activateAProb:1.0 ;acc: 0.670 ;iou: 0.740 ;time: 0:01:20\n",
      "\n",
      "*Training B: False ;B Train loss: 1.969 ;A Train loss: 0.217 ;Edit num: 0.272 ;Train accuracy: 0.585 ;IOU accuracy: 0.684 ;Time: 0:01:34 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 4158 ;lr: 0.0500 ;B loss: 1.818 ;acc: 0.630 ;iou: 0.755 ;time: 0:01:34\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 4158 ;lr: 0.0500 ;B loss: 1.723 ;acc: 0.665 ;iou: 0.765 ;time: 0:01:40\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 4158 ;lr: 0.0500 ;B loss: 2.102 ;acc: 0.530 ;iou: 0.715 ;time: 0:01:47\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 4158 ;lr: 0.0500 ;B loss: 1.267 ;acc: 0.850 ;iou: 0.900 ;time: 0:01:53\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 4158 ;lr: 0.0500 ;B loss: 2.341 ;acc: 0.520 ;iou: 0.650 ;time: 0:01:59\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 4158 ;lr: 0.0500 ;B loss: 1.283 ;acc: 0.810 ;iou: 0.870 ;time: 0:02:06\n",
      "\n",
      "*Training B: False ;B Train loss: 1.645 ;Train accuracy: 0.708 ;IOU accuracy: 0.803 ;Time: 0:02:12 \n",
      "\n",
      "Testing, ephoc: 40\n",
      "batch: 0 ;B loss: 1.936 ;acc: 0.855 ;iou_acc: 0.925 ;time: 0:02:12\n",
      "batch: 50 ;B loss: 1.201 ;acc: 0.840 ;iou_acc: 0.875 ;time: 0:02:16\n",
      "batch: 100 ;B loss: 1.404 ;acc: 0.755 ;iou_acc: 0.820 ;time: 0:02:21\n",
      "batch: 150 ;B loss: 1.473 ;acc: 0.720 ;iou_acc: 0.790 ;time: 0:02:26\n",
      "batch: 200 ;B loss: 1.719 ;acc: 0.610 ;iou_acc: 0.750 ;time: 0:02:33\n",
      "batch: 250 ;B loss: 2.146 ;acc: 0.550 ;iou_acc: 0.690 ;time: 0:02:40\n",
      "\n",
      "*BTrain: False ;Test loss: 1.692 ;Test accuracy 0.686 ;IOU accuracy: 0.784 ;Time: 0:02:50\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 41 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 41 ;batch: 0 ;Bloss:1.924 ;Aloss:0.271 ;edits: 0.38 ;activateAProb:1.0 ;acc: 0.590 ;iou: 0.700 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 41 ;batch: 50 ;Bloss:1.653 ;Aloss:0.289 ;edits: 0.31 ;activateAProb:1.0 ;acc: 0.720 ;iou: 0.765 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 41 ;batch: 100 ;Bloss:1.849 ;Aloss:0.158 ;edits: 0.22 ;activateAProb:1.0 ;acc: 0.560 ;iou: 0.650 ;time: 0:00:28\n",
      "Edit: True ;A: True ;ephoc: 41 ;batch: 150 ;Bloss:2.739 ;Aloss:0.279 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.265 ;iou: 0.480 ;time: 0:00:41\n",
      "Edit: True ;A: True ;ephoc: 41 ;batch: 200 ;Bloss:1.718 ;Aloss:0.179 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.615 ;iou: 0.685 ;time: 0:00:54\n",
      "Edit: True ;A: True ;ephoc: 41 ;batch: 250 ;Bloss:2.034 ;Aloss:0.184 ;edits: 0.31 ;activateAProb:1.0 ;acc: 0.375 ;iou: 0.465 ;time: 0:01:07\n",
      "\n",
      "*Training B: False ;B Train loss: 2.063 ;A Train loss: 0.220 ;Edit num: 0.309 ;Train accuracy: 0.511 ;IOU accuracy: 0.611 ;Time: 0:01:19 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 42 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 42 ;batch: 0 ;Bloss:1.981 ;Aloss:0.195 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.535 ;iou: 0.630 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 42 ;batch: 50 ;Bloss:2.089 ;Aloss:0.164 ;edits: 0.32 ;activateAProb:1.0 ;acc: 0.490 ;iou: 0.570 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 42 ;batch: 100 ;Bloss:1.668 ;Aloss:0.161 ;edits: 0.28 ;activateAProb:1.0 ;acc: 0.645 ;iou: 0.710 ;time: 0:00:26\n",
      "Edit: True ;A: True ;ephoc: 42 ;batch: 150 ;Bloss:2.892 ;Aloss:0.206 ;edits: 0.23 ;activateAProb:1.0 ;acc: 0.320 ;iou: 0.480 ;time: 0:00:39\n",
      "Edit: True ;A: True ;ephoc: 42 ;batch: 200 ;Bloss:1.536 ;Aloss:0.124 ;edits: 0.13 ;activateAProb:1.0 ;acc: 0.620 ;iou: 0.720 ;time: 0:00:52\n",
      "Edit: True ;A: True ;ephoc: 42 ;batch: 250 ;Bloss:1.746 ;Aloss:0.226 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.700 ;iou: 0.765 ;time: 0:01:05\n",
      "\n",
      "*Training B: True ;B Train loss: 2.041 ;A Train loss: 0.212 ;Edit num: 0.293 ;Train accuracy: 0.521 ;IOU accuracy: 0.621 ;Time: 0:01:19 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 43 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 43 ;batch: 0 ;gs: 4159 ;lr: 0.0500 ;B loss:1.785 ;A loss:0.172 ;edits:0.18 ;activateAProb:1.0 ;acc: 0.710 ;iou: 0.775 ;time: 0:00:01\n",
      "Edit: True ;A: True ;ephoc: 43 ;batch: 50 ;gs: 4209 ;lr: 0.0500 ;B loss:2.652 ;A loss:0.203 ;edits:0.34 ;activateAProb:1.0 ;acc: 0.245 ;iou: 0.350 ;time: 0:00:17\n",
      "Edit: True ;A: True ;ephoc: 43 ;batch: 100 ;gs: 4259 ;lr: 0.0500 ;B loss:1.648 ;A loss:0.246 ;edits:0.22 ;activateAProb:1.0 ;acc: 0.665 ;iou: 0.700 ;time: 0:00:33\n",
      "Edit: True ;A: True ;ephoc: 43 ;batch: 150 ;gs: 4309 ;lr: 0.0500 ;B loss:1.647 ;A loss:0.203 ;edits:0.32 ;activateAProb:1.0 ;acc: 0.705 ;iou: 0.785 ;time: 0:00:48\n",
      "Edit: False ;A: True ;batch: 200 ;gs: 4359 ;lr: 0.0500 ;B loss: 1.203 ;acc: 0.905 ;iou: 0.915 ;time: 0:01:04\n",
      "Edit: True ;A: True ;ephoc: 43 ;batch: 250 ;gs: 4409 ;lr: 0.0500 ;B loss:1.354 ;A loss:0.305 ;edits:0.17 ;activateAProb:1.0 ;acc: 0.755 ;iou: 0.800 ;time: 0:01:22\n",
      "\n",
      "*Training B: False ;B Train loss: 1.999 ;A Train loss: 0.209 ;Edit num: 0.298 ;Train accuracy: 0.569 ;IOU accuracy: 0.669 ;Time: 0:01:36 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 4455 ;lr: 0.0500 ;B loss: 1.526 ;acc: 0.755 ;iou: 0.825 ;time: 0:01:37\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 4455 ;lr: 0.0500 ;B loss: 2.176 ;acc: 0.575 ;iou: 0.710 ;time: 0:01:43\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 4455 ;lr: 0.0500 ;B loss: 1.274 ;acc: 0.820 ;iou: 0.870 ;time: 0:01:50\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 4455 ;lr: 0.0500 ;B loss: 1.270 ;acc: 0.855 ;iou: 0.910 ;time: 0:01:56\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 4455 ;lr: 0.0500 ;B loss: 1.200 ;acc: 0.890 ;iou: 0.910 ;time: 0:02:03\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 4455 ;lr: 0.0500 ;B loss: 1.142 ;acc: 0.880 ;iou: 0.915 ;time: 0:02:09\n",
      "\n",
      "*Training B: False ;B Train loss: 1.655 ;Train accuracy: 0.711 ;IOU accuracy: 0.805 ;Time: 0:02:15 \n",
      "\n",
      "Testing, ephoc: 43\n",
      "batch: 0 ;B loss: 1.849 ;acc: 0.855 ;iou_acc: 0.930 ;time: 0:02:16\n",
      "batch: 50 ;B loss: 1.224 ;acc: 0.810 ;iou_acc: 0.850 ;time: 0:02:20\n",
      "batch: 100 ;B loss: 1.431 ;acc: 0.755 ;iou_acc: 0.845 ;time: 0:02:25\n",
      "batch: 150 ;B loss: 1.491 ;acc: 0.710 ;iou_acc: 0.795 ;time: 0:02:31\n",
      "batch: 200 ;B loss: 1.742 ;acc: 0.605 ;iou_acc: 0.745 ;time: 0:02:37\n",
      "batch: 250 ;B loss: 2.151 ;acc: 0.535 ;iou_acc: 0.685 ;time: 0:02:45\n",
      "\n",
      "*BTrain: False ;Test loss: 1.703 ;Test accuracy 0.687 ;IOU accuracy: 0.785 ;Time: 0:02:55\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 44 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 44 ;batch: 0 ;Bloss:1.885 ;Aloss:0.297 ;edits: 0.47 ;activateAProb:1.0 ;acc: 0.490 ;iou: 0.540 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 44 ;batch: 50 ;Bloss:1.864 ;Aloss:0.188 ;edits: 0.35 ;activateAProb:1.0 ;acc: 0.540 ;iou: 0.655 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 44 ;batch: 100 ;Bloss:1.587 ;Aloss:0.202 ;edits: 0.33 ;activateAProb:1.0 ;acc: 0.755 ;iou: 0.785 ;time: 0:00:27\n",
      "Edit: True ;A: True ;ephoc: 44 ;batch: 150 ;Bloss:2.149 ;Aloss:0.239 ;edits: 0.24 ;activateAProb:1.0 ;acc: 0.420 ;iou: 0.555 ;time: 0:00:39\n",
      "Edit: True ;A: True ;ephoc: 44 ;batch: 200 ;Bloss:1.512 ;Aloss:0.239 ;edits: 0.20 ;activateAProb:1.0 ;acc: 0.695 ;iou: 0.755 ;time: 0:00:51\n",
      "Edit: True ;A: True ;ephoc: 44 ;batch: 250 ;Bloss:2.147 ;Aloss:0.276 ;edits: 0.43 ;activateAProb:1.0 ;acc: 0.475 ;iou: 0.575 ;time: 0:01:03\n",
      "\n",
      "*Training B: False ;B Train loss: 2.001 ;A Train loss: 0.204 ;Edit num: 0.281 ;Train accuracy: 0.546 ;IOU accuracy: 0.645 ;Time: 0:01:16 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 45 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 45 ;batch: 0 ;Bloss:2.011 ;Aloss:0.165 ;edits: 0.26 ;activateAProb:1.0 ;acc: 0.405 ;iou: 0.530 ;time: 0:00:01\n",
      "Edit: True ;A: True ;ephoc: 45 ;batch: 50 ;Bloss:1.914 ;Aloss:0.182 ;edits: 0.07 ;activateAProb:1.0 ;acc: 0.520 ;iou: 0.640 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 45 ;batch: 100 ;Bloss:2.098 ;Aloss:0.226 ;edits: 0.40 ;activateAProb:1.0 ;acc: 0.480 ;iou: 0.560 ;time: 0:00:28\n",
      "Edit: True ;A: True ;ephoc: 45 ;batch: 150 ;Bloss:1.682 ;Aloss:0.163 ;edits: 0.24 ;activateAProb:1.0 ;acc: 0.590 ;iou: 0.675 ;time: 0:00:42\n",
      "Edit: True ;A: True ;ephoc: 45 ;batch: 200 ;Bloss:1.689 ;Aloss:0.176 ;edits: 0.32 ;activateAProb:1.0 ;acc: 0.625 ;iou: 0.690 ;time: 0:00:55\n",
      "Edit: True ;A: True ;ephoc: 45 ;batch: 250 ;Bloss:1.709 ;Aloss:0.212 ;edits: 0.24 ;activateAProb:1.0 ;acc: 0.580 ;iou: 0.650 ;time: 0:01:08\n",
      "\n",
      "*Training B: True ;B Train loss: 2.026 ;A Train loss: 0.202 ;Edit num: 0.292 ;Train accuracy: 0.533 ;IOU accuracy: 0.632 ;Time: 0:01:20 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 46 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 46 ;batch: 0 ;gs: 4456 ;lr: 0.0500 ;B loss:2.070 ;A loss:0.173 ;edits:0.31 ;activateAProb:1.0 ;acc: 0.535 ;iou: 0.640 ;time: 0:00:01\n",
      "Edit: True ;A: True ;ephoc: 46 ;batch: 50 ;gs: 4506 ;lr: 0.0500 ;B loss:2.459 ;A loss:0.146 ;edits:0.26 ;activateAProb:1.0 ;acc: 0.455 ;iou: 0.630 ;time: 0:00:16\n",
      "Edit: True ;A: True ;ephoc: 46 ;batch: 100 ;gs: 4556 ;lr: 0.0500 ;B loss:1.774 ;A loss:0.093 ;edits:0.07 ;activateAProb:1.0 ;acc: 0.615 ;iou: 0.720 ;time: 0:00:32\n",
      "Edit: True ;A: True ;ephoc: 46 ;batch: 150 ;gs: 4606 ;lr: 0.0500 ;B loss:2.765 ;A loss:0.243 ;edits:0.48 ;activateAProb:1.0 ;acc: 0.350 ;iou: 0.525 ;time: 0:00:48\n",
      "Edit: True ;A: True ;ephoc: 46 ;batch: 200 ;gs: 4656 ;lr: 0.0500 ;B loss:1.956 ;A loss:0.354 ;edits:0.34 ;activateAProb:1.0 ;acc: 0.910 ;iou: 0.950 ;time: 0:01:05\n",
      "Edit: True ;A: True ;ephoc: 46 ;batch: 250 ;gs: 4706 ;lr: 0.0500 ;B loss:2.244 ;A loss:0.232 ;edits:0.44 ;activateAProb:1.0 ;acc: 0.345 ;iou: 0.470 ;time: 0:01:22\n",
      "\n",
      "*Training B: False ;B Train loss: 2.003 ;A Train loss: 0.212 ;Edit num: 0.308 ;Train accuracy: 0.568 ;IOU accuracy: 0.667 ;Time: 0:01:37 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 4752 ;lr: 0.0500 ;B loss: 1.708 ;acc: 0.690 ;iou: 0.805 ;time: 0:01:37\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 4752 ;lr: 0.0500 ;B loss: 2.099 ;acc: 0.555 ;iou: 0.735 ;time: 0:01:43\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 4752 ;lr: 0.0500 ;B loss: 1.620 ;acc: 0.705 ;iou: 0.795 ;time: 0:01:50\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 4752 ;lr: 0.0500 ;B loss: 1.998 ;acc: 0.630 ;iou: 0.805 ;time: 0:01:56\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 4752 ;lr: 0.0500 ;B loss: 1.935 ;acc: 0.920 ;iou: 0.965 ;time: 0:02:03\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 4752 ;lr: 0.0500 ;B loss: 1.700 ;acc: 0.695 ;iou: 0.805 ;time: 0:02:09\n",
      "\n",
      "*Training B: False ;B Train loss: 1.638 ;Train accuracy: 0.718 ;IOU accuracy: 0.811 ;Time: 0:02:15 \n",
      "\n",
      "Testing, ephoc: 46\n",
      "batch: 0 ;B loss: 1.887 ;acc: 0.855 ;iou_acc: 0.925 ;time: 0:02:16\n",
      "batch: 50 ;B loss: 1.218 ;acc: 0.830 ;iou_acc: 0.865 ;time: 0:02:19\n",
      "batch: 100 ;B loss: 1.418 ;acc: 0.760 ;iou_acc: 0.840 ;time: 0:02:24\n",
      "batch: 150 ;B loss: 1.481 ;acc: 0.720 ;iou_acc: 0.800 ;time: 0:02:30\n",
      "batch: 200 ;B loss: 1.726 ;acc: 0.655 ;iou_acc: 0.785 ;time: 0:02:37\n",
      "batch: 250 ;B loss: 2.127 ;acc: 0.555 ;iou_acc: 0.695 ;time: 0:02:45\n",
      "\n",
      "*BTrain: False ;Test loss: 1.691 ;Test accuracy 0.692 ;IOU accuracy: 0.790 ;Time: 0:02:54\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 47 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 47 ;batch: 0 ;Bloss:1.706 ;Aloss:0.306 ;edits: 0.42 ;activateAProb:1.0 ;acc: 0.725 ;iou: 0.795 ;time: 0:00:01\n",
      "Edit: True ;A: True ;ephoc: 47 ;batch: 50 ;Bloss:1.827 ;Aloss:0.188 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.570 ;iou: 0.645 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 47 ;batch: 100 ;Bloss:1.612 ;Aloss:0.258 ;edits: 0.36 ;activateAProb:1.0 ;acc: 0.740 ;iou: 0.790 ;time: 0:00:28\n",
      "Edit: True ;A: True ;ephoc: 47 ;batch: 150 ;Bloss:1.880 ;Aloss:0.175 ;edits: 0.33 ;activateAProb:1.0 ;acc: 0.525 ;iou: 0.580 ;time: 0:00:42\n",
      "Edit: True ;A: True ;ephoc: 47 ;batch: 200 ;Bloss:1.869 ;Aloss:0.232 ;edits: 0.29 ;activateAProb:1.0 ;acc: 0.455 ;iou: 0.545 ;time: 0:00:55\n",
      "Edit: True ;A: True ;ephoc: 47 ;batch: 250 ;Bloss:1.704 ;Aloss:0.141 ;edits: 0.28 ;activateAProb:1.0 ;acc: 0.635 ;iou: 0.685 ;time: 0:01:09\n",
      "\n",
      "*Training B: False ;B Train loss: 2.060 ;A Train loss: 0.220 ;Edit num: 0.329 ;Train accuracy: 0.517 ;IOU accuracy: 0.619 ;Time: 0:01:21 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 48 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 48 ;batch: 0 ;Bloss:2.770 ;Aloss:0.142 ;edits: 0.12 ;activateAProb:1.0 ;acc: 0.320 ;iou: 0.550 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 48 ;batch: 50 ;Bloss:2.831 ;Aloss:0.192 ;edits: 0.33 ;activateAProb:1.0 ;acc: 0.325 ;iou: 0.490 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 48 ;batch: 100 ;Bloss:2.058 ;Aloss:0.174 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.490 ;iou: 0.550 ;time: 0:00:27\n",
      "Edit: True ;A: True ;ephoc: 48 ;batch: 150 ;Bloss:2.272 ;Aloss:0.332 ;edits: 0.54 ;activateAProb:1.0 ;acc: 0.405 ;iou: 0.505 ;time: 0:00:41\n",
      "Edit: True ;A: True ;ephoc: 48 ;batch: 200 ;Bloss:1.683 ;Aloss:0.167 ;edits: 0.32 ;activateAProb:1.0 ;acc: 0.590 ;iou: 0.670 ;time: 0:00:54\n",
      "Edit: True ;A: True ;ephoc: 48 ;batch: 250 ;Bloss:2.081 ;Aloss:0.246 ;edits: 0.36 ;activateAProb:1.0 ;acc: 0.340 ;iou: 0.440 ;time: 0:01:07\n",
      "\n",
      "*Training B: True ;B Train loss: 2.057 ;A Train loss: 0.215 ;Edit num: 0.325 ;Train accuracy: 0.519 ;IOU accuracy: 0.620 ;Time: 0:01:18 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 49 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 49 ;batch: 0 ;gs: 4753 ;lr: 0.0500 ;B loss:1.745 ;A loss:0.217 ;edits:0.15 ;activateAProb:1.0 ;acc: 0.550 ;iou: 0.670 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 49 ;batch: 50 ;gs: 4803 ;lr: 0.0500 ;B loss:1.561 ;A loss:0.177 ;edits:0.12 ;activateAProb:1.0 ;acc: 0.670 ;iou: 0.730 ;time: 0:00:17\n",
      "Edit: True ;A: True ;ephoc: 49 ;batch: 100 ;gs: 4853 ;lr: 0.0500 ;B loss:1.973 ;A loss:0.235 ;edits:0.41 ;activateAProb:1.0 ;acc: 0.575 ;iou: 0.660 ;time: 0:00:32\n",
      "Edit: True ;A: True ;ephoc: 49 ;batch: 150 ;gs: 4903 ;lr: 0.0500 ;B loss:1.853 ;A loss:0.205 ;edits:0.01 ;activateAProb:1.0 ;acc: 0.875 ;iou: 0.925 ;time: 0:00:47\n",
      "Edit: True ;A: True ;ephoc: 49 ;batch: 200 ;gs: 4953 ;lr: 0.0500 ;B loss:1.679 ;A loss:0.172 ;edits:0.25 ;activateAProb:1.0 ;acc: 0.590 ;iou: 0.665 ;time: 0:01:04\n",
      "Edit: True ;A: True ;ephoc: 49 ;batch: 250 ;gs: 5003 ;lr: 0.0500 ;B loss:1.737 ;A loss:0.172 ;edits:0.25 ;activateAProb:1.0 ;acc: 0.430 ;iou: 0.530 ;time: 0:01:20\n",
      "\n",
      "*Training B: False ;B Train loss: 1.998 ;A Train loss: 0.208 ;Edit num: 0.311 ;Train accuracy: 0.571 ;IOU accuracy: 0.669 ;Time: 0:01:35 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 5049 ;lr: 0.0500 ;B loss: 1.417 ;acc: 0.715 ;iou: 0.800 ;time: 0:01:35\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 5049 ;lr: 0.0500 ;B loss: 1.373 ;acc: 0.765 ;iou: 0.835 ;time: 0:01:41\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 5049 ;lr: 0.0500 ;B loss: 1.449 ;acc: 0.725 ;iou: 0.805 ;time: 0:01:48\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 5049 ;lr: 0.0500 ;B loss: 1.855 ;acc: 0.870 ;iou: 0.920 ;time: 0:01:54\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 5049 ;lr: 0.0500 ;B loss: 1.261 ;acc: 0.805 ;iou: 0.855 ;time: 0:02:01\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 5049 ;lr: 0.0500 ;B loss: 1.312 ;acc: 0.780 ;iou: 0.870 ;time: 0:02:07\n",
      "\n",
      "*Training B: False ;B Train loss: 1.628 ;Train accuracy: 0.719 ;IOU accuracy: 0.812 ;Time: 0:02:13 \n",
      "\n",
      "Testing, ephoc: 49\n",
      "batch: 0 ;B loss: 1.898 ;acc: 0.860 ;iou_acc: 0.925 ;time: 0:02:14\n",
      "batch: 50 ;B loss: 1.217 ;acc: 0.835 ;iou_acc: 0.875 ;time: 0:02:18\n",
      "batch: 100 ;B loss: 1.408 ;acc: 0.755 ;iou_acc: 0.825 ;time: 0:02:22\n",
      "batch: 150 ;B loss: 1.475 ;acc: 0.740 ;iou_acc: 0.800 ;time: 0:02:28\n",
      "batch: 200 ;B loss: 1.685 ;acc: 0.620 ;iou_acc: 0.765 ;time: 0:02:34\n",
      "batch: 250 ;B loss: 2.141 ;acc: 0.535 ;iou_acc: 0.690 ;time: 0:02:42\n",
      "\n",
      "*BTrain: False ;Test loss: 1.684 ;Test accuracy 0.693 ;IOU accuracy: 0.790 ;Time: 0:02:52\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 50 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 50 ;batch: 0 ;Bloss:1.630 ;Aloss:0.166 ;edits: 0.32 ;activateAProb:1.0 ;acc: 0.695 ;iou: 0.725 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 50 ;batch: 50 ;Bloss:2.774 ;Aloss:0.212 ;edits: 0.43 ;activateAProb:1.0 ;acc: 0.315 ;iou: 0.455 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 50 ;batch: 100 ;Bloss:1.937 ;Aloss:0.458 ;edits: 0.42 ;activateAProb:1.0 ;acc: 0.855 ;iou: 0.920 ;time: 0:00:25\n",
      "Edit: True ;A: True ;ephoc: 50 ;batch: 150 ;Bloss:2.196 ;Aloss:0.206 ;edits: 0.41 ;activateAProb:1.0 ;acc: 0.470 ;iou: 0.565 ;time: 0:00:39\n",
      "Edit: True ;A: True ;ephoc: 50 ;batch: 200 ;Bloss:3.079 ;Aloss:0.206 ;edits: 0.35 ;activateAProb:1.0 ;acc: 0.270 ;iou: 0.445 ;time: 0:00:52\n",
      "Edit: True ;A: True ;ephoc: 50 ;batch: 250 ;Bloss:1.856 ;Aloss:0.315 ;edits: 0.31 ;activateAProb:1.0 ;acc: 0.870 ;iou: 0.920 ;time: 0:01:06\n",
      "\n",
      "*Training B: False ;B Train loss: 2.039 ;A Train loss: 0.211 ;Edit num: 0.331 ;Train accuracy: 0.526 ;IOU accuracy: 0.628 ;Time: 0:01:17 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 51 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 51 ;batch: 0 ;Bloss:2.025 ;Aloss:0.235 ;edits: 0.42 ;activateAProb:1.0 ;acc: 0.460 ;iou: 0.585 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 51 ;batch: 50 ;Bloss:1.679 ;Aloss:0.129 ;edits: 0.25 ;activateAProb:1.0 ;acc: 0.630 ;iou: 0.755 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 51 ;batch: 100 ;Bloss:2.663 ;Aloss:0.223 ;edits: 0.42 ;activateAProb:1.0 ;acc: 0.315 ;iou: 0.520 ;time: 0:00:26\n",
      "Edit: True ;A: True ;ephoc: 51 ;batch: 150 ;Bloss:2.104 ;Aloss:0.263 ;edits: 0.49 ;activateAProb:1.0 ;acc: 0.445 ;iou: 0.545 ;time: 0:00:40\n",
      "Edit: True ;A: True ;ephoc: 51 ;batch: 200 ;Bloss:2.030 ;Aloss:0.210 ;edits: 0.41 ;activateAProb:1.0 ;acc: 0.480 ;iou: 0.600 ;time: 0:00:53\n",
      "Edit: True ;A: True ;ephoc: 51 ;batch: 250 ;Bloss:1.790 ;Aloss:0.174 ;edits: 0.17 ;activateAProb:1.0 ;acc: 0.540 ;iou: 0.650 ;time: 0:01:06\n",
      "\n",
      "*Training B: True ;B Train loss: 2.038 ;A Train loss: 0.210 ;Edit num: 0.325 ;Train accuracy: 0.527 ;IOU accuracy: 0.629 ;Time: 0:01:18 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 52 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 52 ;batch: 0 ;gs: 5050 ;lr: 0.0500 ;B loss:1.878 ;A loss:0.214 ;edits:0.42 ;activateAProb:1.0 ;acc: 0.555 ;iou: 0.650 ;time: 0:00:00\n",
      "Edit: False ;A: True ;batch: 50 ;gs: 5100 ;lr: 0.0500 ;B loss: 2.852 ;acc: 0.340 ;iou: 0.545 ;time: 0:00:16\n",
      "Edit: True ;A: True ;ephoc: 52 ;batch: 100 ;gs: 5150 ;lr: 0.0500 ;B loss:2.830 ;A loss:0.188 ;edits:0.32 ;activateAProb:1.0 ;acc: 0.315 ;iou: 0.530 ;time: 0:00:32\n",
      "Edit: True ;A: True ;ephoc: 52 ;batch: 150 ;gs: 5200 ;lr: 0.0500 ;B loss:1.648 ;A loss:0.179 ;edits:0.32 ;activateAProb:1.0 ;acc: 0.760 ;iou: 0.815 ;time: 0:00:48\n",
      "Edit: True ;A: True ;ephoc: 52 ;batch: 200 ;gs: 5250 ;lr: 0.0500 ;B loss:1.574 ;A loss:0.133 ;edits:0.14 ;activateAProb:1.0 ;acc: 0.635 ;iou: 0.695 ;time: 0:01:03\n",
      "Edit: True ;A: True ;ephoc: 52 ;batch: 250 ;gs: 5300 ;lr: 0.0500 ;B loss:1.856 ;A loss:0.193 ;edits:0.29 ;activateAProb:1.0 ;acc: 0.495 ;iou: 0.560 ;time: 0:01:19\n",
      "\n",
      "*Training B: False ;B Train loss: 1.985 ;A Train loss: 0.204 ;Edit num: 0.309 ;Train accuracy: 0.577 ;IOU accuracy: 0.673 ;Time: 0:01:33 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 5346 ;lr: 0.0500 ;B loss: 1.318 ;acc: 0.790 ;iou: 0.835 ;time: 0:01:34\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 5346 ;lr: 0.0500 ;B loss: 2.769 ;acc: 0.405 ;iou: 0.615 ;time: 0:01:40\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 5346 ;lr: 0.0500 ;B loss: 2.373 ;acc: 0.430 ;iou: 0.655 ;time: 0:01:47\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 5346 ;lr: 0.0500 ;B loss: 1.247 ;acc: 0.860 ;iou: 0.905 ;time: 0:01:53\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 5346 ;lr: 0.0500 ;B loss: 1.339 ;acc: 0.775 ;iou: 0.830 ;time: 0:02:00\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 5346 ;lr: 0.0500 ;B loss: 1.313 ;acc: 0.780 ;iou: 0.850 ;time: 0:02:06\n",
      "\n",
      "*Training B: False ;B Train loss: 1.617 ;Train accuracy: 0.721 ;IOU accuracy: 0.813 ;Time: 0:02:12 \n",
      "\n",
      "Testing, ephoc: 52\n",
      "batch: 0 ;B loss: 1.926 ;acc: 0.855 ;iou_acc: 0.920 ;time: 0:02:13\n",
      "batch: 50 ;B loss: 1.199 ;acc: 0.825 ;iou_acc: 0.875 ;time: 0:02:17\n",
      "batch: 100 ;B loss: 1.389 ;acc: 0.765 ;iou_acc: 0.845 ;time: 0:02:22\n",
      "batch: 150 ;B loss: 1.459 ;acc: 0.710 ;iou_acc: 0.785 ;time: 0:02:28\n",
      "batch: 200 ;B loss: 1.704 ;acc: 0.630 ;iou_acc: 0.755 ;time: 0:02:34\n",
      "batch: 250 ;B loss: 2.097 ;acc: 0.565 ;iou_acc: 0.705 ;time: 0:02:43\n",
      "\n",
      "*BTrain: False ;Test loss: 1.676 ;Test accuracy 0.693 ;IOU accuracy: 0.789 ;Time: 0:02:52\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 53 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 53 ;batch: 0 ;Bloss:1.811 ;Aloss:0.169 ;edits: 0.35 ;activateAProb:1.0 ;acc: 0.540 ;iou: 0.620 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 53 ;batch: 50 ;Bloss:1.639 ;Aloss:0.150 ;edits: 0.26 ;activateAProb:1.0 ;acc: 0.645 ;iou: 0.705 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 53 ;batch: 100 ;Bloss:1.770 ;Aloss:0.228 ;edits: 0.43 ;activateAProb:1.0 ;acc: 0.635 ;iou: 0.705 ;time: 0:00:28\n",
      "Edit: True ;A: True ;ephoc: 53 ;batch: 150 ;Bloss:2.423 ;Aloss:0.253 ;edits: 0.45 ;activateAProb:1.0 ;acc: 0.370 ;iou: 0.495 ;time: 0:00:40\n",
      "Edit: True ;A: True ;ephoc: 53 ;batch: 200 ;Bloss:1.881 ;Aloss:0.073 ;edits: 0.01 ;activateAProb:1.0 ;acc: 0.880 ;iou: 0.925 ;time: 0:00:54\n",
      "Edit: True ;A: True ;ephoc: 53 ;batch: 250 ;Bloss:1.879 ;Aloss:0.177 ;edits: 0.08 ;activateAProb:1.0 ;acc: 0.610 ;iou: 0.710 ;time: 0:01:07\n",
      "\n",
      "*Training B: False ;B Train loss: 2.066 ;A Train loss: 0.208 ;Edit num: 0.341 ;Train accuracy: 0.513 ;IOU accuracy: 0.614 ;Time: 0:01:19 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 54 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 54 ;batch: 0 ;Bloss:2.166 ;Aloss:0.187 ;edits: 0.34 ;activateAProb:1.0 ;acc: 0.410 ;iou: 0.505 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 54 ;batch: 50 ;Bloss:1.947 ;Aloss:0.245 ;edits: 0.47 ;activateAProb:1.0 ;acc: 0.535 ;iou: 0.600 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 54 ;batch: 100 ;Bloss:2.036 ;Aloss:0.192 ;edits: 0.36 ;activateAProb:1.0 ;acc: 0.400 ;iou: 0.520 ;time: 0:00:28\n",
      "Edit: True ;A: True ;ephoc: 54 ;batch: 150 ;Bloss:2.198 ;Aloss:0.164 ;edits: 0.33 ;activateAProb:1.0 ;acc: 0.370 ;iou: 0.500 ;time: 0:00:42\n",
      "Edit: True ;A: True ;ephoc: 54 ;batch: 200 ;Bloss:1.453 ;Aloss:0.127 ;edits: 0.14 ;activateAProb:1.0 ;acc: 0.745 ;iou: 0.790 ;time: 0:00:55\n",
      "Edit: True ;A: True ;ephoc: 54 ;batch: 250 ;Bloss:2.713 ;Aloss:0.182 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.230 ;iou: 0.375 ;time: 0:01:08\n",
      "\n",
      "*Training B: True ;B Train loss: 2.054 ;A Train loss: 0.208 ;Edit num: 0.331 ;Train accuracy: 0.518 ;IOU accuracy: 0.618 ;Time: 0:01:20 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 55 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 55 ;batch: 0 ;gs: 5347 ;lr: 0.0500 ;B loss:1.649 ;A loss:0.154 ;edits:0.26 ;activateAProb:1.0 ;acc: 0.745 ;iou: 0.800 ;time: 0:00:01\n",
      "Edit: True ;A: True ;ephoc: 55 ;batch: 50 ;gs: 5397 ;lr: 0.0500 ;B loss:2.313 ;A loss:0.249 ;edits:0.44 ;activateAProb:1.0 ;acc: 0.420 ;iou: 0.500 ;time: 0:00:17\n",
      "Edit: True ;A: True ;ephoc: 55 ;batch: 100 ;gs: 5447 ;lr: 0.0500 ;B loss:2.026 ;A loss:0.216 ;edits:0.37 ;activateAProb:1.0 ;acc: 0.515 ;iou: 0.620 ;time: 0:00:32\n",
      "Edit: True ;A: True ;ephoc: 55 ;batch: 150 ;gs: 5497 ;lr: 0.0500 ;B loss:1.673 ;A loss:0.188 ;edits:0.32 ;activateAProb:1.0 ;acc: 0.740 ;iou: 0.790 ;time: 0:00:48\n",
      "Edit: True ;A: True ;ephoc: 55 ;batch: 200 ;gs: 5547 ;lr: 0.0500 ;B loss:1.616 ;A loss:0.178 ;edits:0.27 ;activateAProb:1.0 ;acc: 0.735 ;iou: 0.790 ;time: 0:01:04\n",
      "Edit: True ;A: True ;ephoc: 55 ;batch: 250 ;gs: 5597 ;lr: 0.0500 ;B loss:1.970 ;A loss:0.339 ;edits:0.30 ;activateAProb:1.0 ;acc: 0.905 ;iou: 0.945 ;time: 0:01:21\n",
      "\n",
      "*Training B: False ;B Train loss: 1.989 ;A Train loss: 0.206 ;Edit num: 0.318 ;Train accuracy: 0.577 ;IOU accuracy: 0.674 ;Time: 0:01:36 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 5643 ;lr: 0.0500 ;B loss: 1.322 ;acc: 0.890 ;iou: 0.935 ;time: 0:01:36\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 5643 ;lr: 0.0500 ;B loss: 1.756 ;acc: 0.630 ;iou: 0.710 ;time: 0:01:42\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 5643 ;lr: 0.0500 ;B loss: 1.546 ;acc: 0.715 ;iou: 0.800 ;time: 0:01:48\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 5643 ;lr: 0.0500 ;B loss: 1.214 ;acc: 0.895 ;iou: 0.920 ;time: 0:01:55\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 5643 ;lr: 0.0500 ;B loss: 1.272 ;acc: 0.830 ;iou: 0.885 ;time: 0:02:01\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 5643 ;lr: 0.0500 ;B loss: 1.961 ;acc: 0.910 ;iou: 0.950 ;time: 0:02:08\n",
      "\n",
      "*Training B: False ;B Train loss: 1.612 ;Train accuracy: 0.726 ;IOU accuracy: 0.816 ;Time: 0:02:14 \n",
      "\n",
      "Testing, ephoc: 55\n",
      "batch: 0 ;B loss: 1.903 ;acc: 0.880 ;iou_acc: 0.940 ;time: 0:02:14\n",
      "batch: 50 ;B loss: 1.201 ;acc: 0.825 ;iou_acc: 0.865 ;time: 0:02:18\n",
      "batch: 100 ;B loss: 1.406 ;acc: 0.760 ;iou_acc: 0.830 ;time: 0:02:23\n",
      "batch: 150 ;B loss: 1.479 ;acc: 0.710 ;iou_acc: 0.790 ;time: 0:02:29\n",
      "batch: 200 ;B loss: 1.716 ;acc: 0.625 ;iou_acc: 0.780 ;time: 0:02:34\n",
      "batch: 250 ;B loss: 2.105 ;acc: 0.550 ;iou_acc: 0.680 ;time: 0:02:41\n",
      "\n",
      "*BTrain: False ;Test loss: 1.675 ;Test accuracy 0.694 ;IOU accuracy: 0.791 ;Time: 0:02:50\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 56 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 56 ;batch: 0 ;Bloss:1.841 ;Aloss:0.129 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.635 ;iou: 0.710 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 56 ;batch: 50 ;Bloss:2.157 ;Aloss:0.291 ;edits: 0.47 ;activateAProb:1.0 ;acc: 0.470 ;iou: 0.580 ;time: 0:00:11\n",
      "Edit: True ;A: True ;ephoc: 56 ;batch: 100 ;Bloss:2.721 ;Aloss:0.186 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.245 ;iou: 0.375 ;time: 0:00:24\n",
      "Edit: True ;A: True ;ephoc: 56 ;batch: 150 ;Bloss:1.848 ;Aloss:0.166 ;edits: 0.31 ;activateAProb:1.0 ;acc: 0.530 ;iou: 0.615 ;time: 0:00:37\n",
      "Edit: True ;A: True ;ephoc: 56 ;batch: 200 ;Bloss:1.497 ;Aloss:0.189 ;edits: 0.32 ;activateAProb:1.0 ;acc: 0.700 ;iou: 0.750 ;time: 0:00:49\n",
      "Edit: True ;A: True ;ephoc: 56 ;batch: 250 ;Bloss:2.182 ;Aloss:0.197 ;edits: 0.38 ;activateAProb:1.0 ;acc: 0.485 ;iou: 0.580 ;time: 0:01:01\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b4a92e214226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0meditProb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0medit_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         imScale=50)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-f89a65b2ec98>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trn_data, tst_data, ephocs_num, edit_reward, startA, activation_ephoc, muteB, start_ephoc, dropout_in, dropout_out, dropout_img, dropout_q, editProb, max_activateAProb, activateAProb, addNoise, imScale)\u001b[0m\n\u001b[1;32m    716\u001b[0m                                                                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                                                                                             \u001b[0maddNoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maddNoise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                                                                                             imScale=imScale)\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                         \u001b[0mreward_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dummy holder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-f89a65b2ec98>\u001b[0m in \u001b[0;36mbuild_data\u001b[0;34m(self, data, start, end, imScale, addNoise)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mimg_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpadded_im\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mimg_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_max\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mimg_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mhalper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Scale to 0-1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mpadded_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_scale\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimScale\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mhalper\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#Scale to have 1/imScale std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mpadded_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadded_im\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_im\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# scale to zero mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_hidden=200\n",
    "params_dir = params_dir_tmp+'RL/all_hidden:'+str(num_hidden)\n",
    "tf.reset_default_graph()\n",
    "m = Model(\n",
    "    batch_size=200, \n",
    "    num_hidden=num_hidden,\n",
    "    embed_size=embed_vecs.shape[1],\n",
    "    img_dims=trainset[0][0][1][0].shape[1], \n",
    "    bbox_dims=trainset[0][0][1][1].shape[1], \n",
    "    lr=.05,\n",
    "    vocab=vocab, \n",
    "    decay_steps=10000, \n",
    "    decay_rate=0.9, \n",
    "    rnn_editProb=0.2,\n",
    "    coefAlr=3,\n",
    "    bnorm=False,\n",
    "    toQscale=True\n",
    ")\n",
    "\n",
    "\n",
    "# start comp\n",
    "print('params_dir:', params_dir)\n",
    "print('num_hidden:', m.num_hidden)\n",
    "print('learning rate:', m.lr)\n",
    "tst, trn = m.train(trainset, testset,\n",
    "        ephocs_num=200,\n",
    "        start_ephoc=39,\n",
    "        startA=5,\n",
    "        activation_ephoc=7,\n",
    "        muteB=2, \n",
    "        activateAProb=1.,\n",
    "        max_activateAProb=1.,\n",
    "        editProb=0.9,\n",
    "        edit_reward=-0.9,\n",
    "        imScale=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params_dir: ../data/training/models/All/RL/all_hidden:200\n",
      "num_hidden: 200\n",
      "learning rate: 0.05\n",
      "# Train set size: 297\n",
      "# Training batches: 297\n",
      "# Test set size: 297\n",
      "# Testing batches: 297\n",
      "Loading parameters from ../data/training/models/All/RL/all_hidden:200/model.ckpt-5\n",
      "INFO:tensorflow:Restoring parameters from ../data/training/models/All/RL/all_hidden:200/model.ckpt-5\n",
      "================================================== \n",
      "Train, ephoc: 6 ; Training B: False\n",
      "Edit: True ;A: False ;ephoc: 6 ;batch: 0 ;Bloss:2.274 ;Aloss:0.209 ;edits: 0.22 ;activateAProb:1.0 ;acc: 0.490 ;iou: 0.600 ;time: 0:00:00\n",
      "Edit: True ;A: False ;ephoc: 6 ;batch: 50 ;Bloss:2.754 ;Aloss:0.139 ;edits: 0.21 ;activateAProb:1.0 ;acc: 0.320 ;iou: 0.420 ;time: 0:00:13\n",
      "Edit: True ;A: False ;ephoc: 6 ;batch: 100 ;Bloss:2.664 ;Aloss:0.174 ;edits: 0.19 ;activateAProb:1.0 ;acc: 0.320 ;iou: 0.500 ;time: 0:00:26\n",
      "Edit: True ;A: False ;ephoc: 6 ;batch: 150 ;Bloss:1.691 ;Aloss:0.155 ;edits: 0.23 ;activateAProb:1.0 ;acc: 0.745 ;iou: 0.845 ;time: 0:00:39\n",
      "Edit: True ;A: False ;ephoc: 6 ;batch: 200 ;Bloss:1.622 ;Aloss:0.212 ;edits: 0.19 ;activateAProb:1.0 ;acc: 0.570 ;iou: 0.660 ;time: 0:00:53\n",
      "Edit: True ;A: False ;ephoc: 6 ;batch: 250 ;Bloss:2.350 ;Aloss:0.145 ;edits: 0.18 ;activateAProb:1.0 ;acc: 0.390 ;iou: 0.525 ;time: 0:01:08\n",
      "\n",
      "*Training B: True ;B Train loss: 2.048 ;A Train loss: 0.174 ;Edit num: 0.197 ;Train accuracy: 0.512 ;IOU accuracy: 0.627 ;Time: 0:01:19 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 7 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 7 ;batch: 0 ;gs: 595 ;lr: 0.0500 ;B loss:2.357 ;A loss:0.120 ;edits:0.12 ;activateAProb:1.0 ;acc: 0.400 ;iou: 0.525 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 7 ;batch: 50 ;gs: 645 ;lr: 0.0500 ;B loss:2.223 ;A loss:0.291 ;edits:0.41 ;activateAProb:1.0 ;acc: 0.275 ;iou: 0.380 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 7 ;batch: 100 ;gs: 695 ;lr: 0.0500 ;B loss:1.656 ;A loss:0.270 ;edits:0.29 ;activateAProb:1.0 ;acc: 0.615 ;iou: 0.695 ;time: 0:00:29\n",
      "Edit: True ;A: True ;ephoc: 7 ;batch: 150 ;gs: 745 ;lr: 0.0500 ;B loss:2.426 ;A loss:0.298 ;edits:0.51 ;activateAProb:1.0 ;acc: 0.280 ;iou: 0.390 ;time: 0:00:47\n",
      "Edit: True ;A: True ;ephoc: 7 ;batch: 200 ;gs: 795 ;lr: 0.0500 ;B loss:1.952 ;A loss:0.366 ;edits:0.56 ;activateAProb:1.0 ;acc: 0.395 ;iou: 0.515 ;time: 0:01:03\n",
      "Edit: False ;A: True ;batch: 250 ;gs: 845 ;lr: 0.0500 ;B loss: 2.374 ;acc: 0.500 ;iou: 0.655 ;time: 0:01:19\n",
      "\n",
      "*Training B: False ;B Train loss: 2.177 ;A Train loss: 0.246 ;Edit num: 0.338 ;Train accuracy: 0.451 ;IOU accuracy: 0.560 ;Time: 0:01:34 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 891 ;lr: 0.0500 ;B loss: 2.187 ;acc: 0.545 ;iou: 0.675 ;time: 0:01:34\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 891 ;lr: 0.0500 ;B loss: 1.805 ;acc: 0.650 ;iou: 0.730 ;time: 0:01:40\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 891 ;lr: 0.0500 ;B loss: 1.314 ;acc: 0.785 ;iou: 0.840 ;time: 0:01:47\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 891 ;lr: 0.0500 ;B loss: 1.916 ;acc: 0.540 ;iou: 0.670 ;time: 0:01:54\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 891 ;lr: 0.0500 ;B loss: 1.511 ;acc: 0.690 ;iou: 0.805 ;time: 0:02:00\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 891 ;lr: 0.0500 ;B loss: 2.384 ;acc: 0.500 ;iou: 0.625 ;time: 0:02:07\n",
      "\n",
      "*Training B: False ;B Train loss: 1.890 ;Train accuracy: 0.613 ;IOU accuracy: 0.722 ;Time: 0:02:12 \n",
      "\n",
      "Testing, ephoc: 7\n",
      "batch: 0 ;B loss: 1.655 ;acc: 0.815 ;iou_acc: 0.910 ;time: 0:02:13\n",
      "batch: 50 ;B loss: 1.346 ;acc: 0.765 ;iou_acc: 0.815 ;time: 0:02:17\n",
      "batch: 100 ;B loss: 1.665 ;acc: 0.690 ;iou_acc: 0.740 ;time: 0:02:22\n",
      "batch: 150 ;B loss: 1.674 ;acc: 0.655 ;iou_acc: 0.745 ;time: 0:02:27\n",
      "batch: 200 ;B loss: 2.031 ;acc: 0.500 ;iou_acc: 0.660 ;time: 0:02:34\n",
      "batch: 250 ;B loss: 2.484 ;acc: 0.435 ;iou_acc: 0.575 ;time: 0:02:42\n",
      "\n",
      "*BTrain: False ;Test loss: 1.901 ;Test accuracy 0.607 ;IOU accuracy: 0.717 ;Time: 0:02:52\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 8 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 8 ;batch: 0 ;Bloss:1.680 ;Aloss:0.179 ;edits: 0.17 ;activateAProb:1.0 ;acc: 0.625 ;iou: 0.690 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 8 ;batch: 50 ;Bloss:1.781 ;Aloss:0.180 ;edits: 0.17 ;activateAProb:1.0 ;acc: 0.645 ;iou: 0.710 ;time: 0:00:16\n",
      "Edit: True ;A: True ;ephoc: 8 ;batch: 100 ;Bloss:1.714 ;Aloss:0.161 ;edits: 0.20 ;activateAProb:1.0 ;acc: 0.850 ;iou: 0.920 ;time: 0:00:31\n",
      "Edit: True ;A: True ;ephoc: 8 ;batch: 150 ;Bloss:2.464 ;Aloss:0.272 ;edits: 0.49 ;activateAProb:1.0 ;acc: 0.285 ;iou: 0.420 ;time: 0:00:43\n",
      "Edit: True ;A: True ;ephoc: 8 ;batch: 200 ;Bloss:1.815 ;Aloss:0.220 ;edits: 0.46 ;activateAProb:1.0 ;acc: 0.495 ;iou: 0.590 ;time: 0:00:57\n",
      "Edit: True ;A: True ;ephoc: 8 ;batch: 250 ;Bloss:2.183 ;Aloss:0.273 ;edits: 0.33 ;activateAProb:1.0 ;acc: 0.500 ;iou: 0.590 ;time: 0:01:07\n",
      "\n",
      "*Training B: False ;B Train loss: 2.186 ;A Train loss: 0.243 ;Edit num: 0.356 ;Train accuracy: 0.427 ;IOU accuracy: 0.533 ;Time: 0:01:19 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 9 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 9 ;batch: 0 ;Bloss:2.712 ;Aloss:0.233 ;edits: 0.36 ;activateAProb:1.0 ;acc: 0.185 ;iou: 0.320 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 9 ;batch: 50 ;Bloss:2.196 ;Aloss:0.191 ;edits: 0.23 ;activateAProb:1.0 ;acc: 0.440 ;iou: 0.535 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 9 ;batch: 100 ;Bloss:1.820 ;Aloss:0.145 ;edits: 0.24 ;activateAProb:1.0 ;acc: 0.575 ;iou: 0.635 ;time: 0:00:26\n",
      "Edit: True ;A: True ;ephoc: 9 ;batch: 150 ;Bloss:2.173 ;Aloss:0.334 ;edits: 0.51 ;activateAProb:1.0 ;acc: 0.325 ;iou: 0.440 ;time: 0:00:38\n",
      "Edit: True ;A: True ;ephoc: 9 ;batch: 200 ;Bloss:2.083 ;Aloss:0.265 ;edits: 0.50 ;activateAProb:1.0 ;acc: 0.370 ;iou: 0.475 ;time: 0:00:50\n",
      "Edit: True ;A: True ;ephoc: 9 ;batch: 250 ;Bloss:2.423 ;Aloss:0.242 ;edits: 0.47 ;activateAProb:1.0 ;acc: 0.315 ;iou: 0.365 ;time: 0:01:03\n",
      "\n",
      "*Training B: True ;B Train loss: 2.216 ;A Train loss: 0.246 ;Edit num: 0.387 ;Train accuracy: 0.410 ;IOU accuracy: 0.516 ;Time: 0:01:15 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 10 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 10 ;batch: 0 ;gs: 892 ;lr: 0.0500 ;B loss:2.007 ;A loss:0.344 ;edits:0.61 ;activateAProb:1.0 ;acc: 0.405 ;iou: 0.500 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 10 ;batch: 50 ;gs: 942 ;lr: 0.0500 ;B loss:2.240 ;A loss:0.183 ;edits:0.27 ;activateAProb:1.0 ;acc: 0.385 ;iou: 0.515 ;time: 0:00:16\n",
      "Edit: True ;A: True ;ephoc: 10 ;batch: 100 ;gs: 992 ;lr: 0.0500 ;B loss:1.568 ;A loss:0.317 ;edits:0.23 ;activateAProb:1.0 ;acc: 0.690 ;iou: 0.765 ;time: 0:00:30\n",
      "Edit: False ;A: True ;batch: 150 ;gs: 1042 ;lr: 0.0500 ;B loss: 1.338 ;acc: 0.800 ;iou: 0.865 ;time: 0:00:46\n",
      "Edit: True ;A: True ;ephoc: 10 ;batch: 200 ;gs: 1092 ;lr: 0.0500 ;B loss:2.052 ;A loss:0.291 ;edits:0.46 ;activateAProb:1.0 ;acc: 0.500 ;iou: 0.580 ;time: 0:01:01\n",
      "Edit: False ;A: True ;batch: 250 ;gs: 1142 ;lr: 0.0500 ;B loss: 1.478 ;acc: 0.710 ;iou: 0.780 ;time: 0:01:17\n",
      "\n",
      "*Training B: False ;B Train loss: 2.144 ;A Train loss: 0.235 ;Edit num: 0.360 ;Train accuracy: 0.476 ;IOU accuracy: 0.584 ;Time: 0:01:33 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 1188 ;lr: 0.0500 ;B loss: 1.518 ;acc: 0.690 ;iou: 0.780 ;time: 0:01:33\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 1188 ;lr: 0.0500 ;B loss: 1.998 ;acc: 0.530 ;iou: 0.670 ;time: 0:01:39\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 1188 ;lr: 0.0500 ;B loss: 1.313 ;acc: 0.785 ;iou: 0.865 ;time: 0:01:46\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 1188 ;lr: 0.0500 ;B loss: 1.361 ;acc: 0.770 ;iou: 0.850 ;time: 0:01:53\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 1188 ;lr: 0.0500 ;B loss: 1.705 ;acc: 0.685 ;iou: 0.765 ;time: 0:01:59\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 1188 ;lr: 0.0500 ;B loss: 1.486 ;acc: 0.680 ;iou: 0.755 ;time: 0:02:05\n",
      "\n",
      "*Training B: False ;B Train loss: 1.859 ;Train accuracy: 0.618 ;IOU accuracy: 0.726 ;Time: 0:02:11 \n",
      "\n",
      "Testing, ephoc: 10\n",
      "batch: 0 ;B loss: 1.721 ;acc: 0.840 ;iou_acc: 0.920 ;time: 0:02:12\n",
      "batch: 50 ;B loss: 1.338 ;acc: 0.740 ;iou_acc: 0.810 ;time: 0:02:15\n",
      "batch: 100 ;B loss: 1.626 ;acc: 0.670 ;iou_acc: 0.715 ;time: 0:02:20\n",
      "batch: 150 ;B loss: 1.639 ;acc: 0.640 ;iou_acc: 0.730 ;time: 0:02:25\n",
      "batch: 200 ;B loss: 1.996 ;acc: 0.510 ;iou_acc: 0.650 ;time: 0:02:32\n",
      "batch: 250 ;B loss: 2.451 ;acc: 0.415 ;iou_acc: 0.560 ;time: 0:02:40\n",
      "\n",
      "*BTrain: False ;Test loss: 1.875 ;Test accuracy 0.608 ;IOU accuracy: 0.718 ;Time: 0:02:50\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 11 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 11 ;batch: 0 ;Bloss:2.613 ;Aloss:0.229 ;edits: 0.47 ;activateAProb:1.0 ;acc: 0.305 ;iou: 0.405 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 11 ;batch: 50 ;Bloss:1.796 ;Aloss:0.167 ;edits: 0.16 ;activateAProb:1.0 ;acc: 0.515 ;iou: 0.610 ;time: 0:00:12\n",
      "Edit: True ;A: True ;ephoc: 11 ;batch: 100 ;Bloss:1.611 ;Aloss:0.184 ;edits: 0.29 ;activateAProb:1.0 ;acc: 0.620 ;iou: 0.705 ;time: 0:00:24\n",
      "Edit: True ;A: True ;ephoc: 11 ;batch: 150 ;Bloss:2.388 ;Aloss:0.235 ;edits: 0.41 ;activateAProb:1.0 ;acc: 0.355 ;iou: 0.475 ;time: 0:00:36\n",
      "Edit: True ;A: True ;ephoc: 11 ;batch: 200 ;Bloss:1.768 ;Aloss:0.244 ;edits: 0.42 ;activateAProb:1.0 ;acc: 0.580 ;iou: 0.650 ;time: 0:00:49\n",
      "Edit: True ;A: True ;ephoc: 11 ;batch: 250 ;Bloss:1.926 ;Aloss:0.265 ;edits: 0.43 ;activateAProb:1.0 ;acc: 0.350 ;iou: 0.460 ;time: 0:01:03\n",
      "\n",
      "*Training B: False ;B Train loss: 2.182 ;A Train loss: 0.221 ;Edit num: 0.346 ;Train accuracy: 0.428 ;IOU accuracy: 0.538 ;Time: 0:01:15 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 12 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 12 ;batch: 0 ;Bloss:2.156 ;Aloss:0.285 ;edits: 0.45 ;activateAProb:1.0 ;acc: 0.370 ;iou: 0.475 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 12 ;batch: 50 ;Bloss:2.487 ;Aloss:0.203 ;edits: 0.38 ;activateAProb:1.0 ;acc: 0.230 ;iou: 0.345 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 12 ;batch: 100 ;Bloss:1.764 ;Aloss:0.247 ;edits: 0.45 ;activateAProb:1.0 ;acc: 0.570 ;iou: 0.655 ;time: 0:00:26\n",
      "Edit: True ;A: True ;ephoc: 12 ;batch: 150 ;Bloss:1.911 ;Aloss:0.277 ;edits: 0.50 ;activateAProb:1.0 ;acc: 0.510 ;iou: 0.630 ;time: 0:00:38\n",
      "Edit: True ;A: True ;ephoc: 12 ;batch: 200 ;Bloss:1.725 ;Aloss:0.214 ;edits: 0.21 ;activateAProb:1.0 ;acc: 0.550 ;iou: 0.620 ;time: 0:00:51\n",
      "Edit: True ;A: True ;ephoc: 12 ;batch: 250 ;Bloss:2.423 ;Aloss:0.231 ;edits: 0.44 ;activateAProb:1.0 ;acc: 0.295 ;iou: 0.450 ;time: 0:01:04\n",
      "\n",
      "*Training B: True ;B Train loss: 2.192 ;A Train loss: 0.224 ;Edit num: 0.352 ;Train accuracy: 0.420 ;IOU accuracy: 0.528 ;Time: 0:01:16 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 13 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 13 ;batch: 0 ;gs: 1189 ;lr: 0.0500 ;B loss:2.422 ;A loss:0.204 ;edits:0.32 ;activateAProb:1.0 ;acc: 0.360 ;iou: 0.490 ;time: 0:00:00\n",
      "Edit: False ;A: True ;batch: 50 ;gs: 1239 ;lr: 0.0500 ;B loss: 2.152 ;acc: 0.500 ;iou: 0.655 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 13 ;batch: 100 ;gs: 1289 ;lr: 0.0500 ;B loss:2.898 ;A loss:0.279 ;edits:0.41 ;activateAProb:1.0 ;acc: 0.245 ;iou: 0.355 ;time: 0:00:29\n",
      "Edit: True ;A: True ;ephoc: 13 ;batch: 150 ;gs: 1339 ;lr: 0.0500 ;B loss:3.165 ;A loss:0.182 ;edits:0.32 ;activateAProb:1.0 ;acc: 0.205 ;iou: 0.375 ;time: 0:00:44\n",
      "Edit: True ;A: True ;ephoc: 13 ;batch: 200 ;gs: 1389 ;lr: 0.0500 ;B loss:2.594 ;A loss:0.204 ;edits:0.32 ;activateAProb:1.0 ;acc: 0.320 ;iou: 0.490 ;time: 0:01:01\n",
      "Edit: True ;A: True ;ephoc: 13 ;batch: 250 ;gs: 1439 ;lr: 0.0500 ;B loss:2.582 ;A loss:0.207 ;edits:0.37 ;activateAProb:1.0 ;acc: 0.285 ;iou: 0.445 ;time: 0:01:17\n",
      "\n",
      "*Training B: False ;B Train loss: 2.111 ;A Train loss: 0.219 ;Edit num: 0.324 ;Train accuracy: 0.499 ;IOU accuracy: 0.605 ;Time: 0:01:32 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 1485 ;lr: 0.0500 ;B loss: 1.991 ;acc: 0.550 ;iou: 0.685 ;time: 0:01:32\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 1485 ;lr: 0.0500 ;B loss: 2.116 ;acc: 0.500 ;iou: 0.665 ;time: 0:01:39\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 1485 ;lr: 0.0500 ;B loss: 2.499 ;acc: 0.480 ;iou: 0.600 ;time: 0:01:45\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 1485 ;lr: 0.0500 ;B loss: 2.790 ;acc: 0.360 ;iou: 0.540 ;time: 0:01:52\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 1485 ;lr: 0.0500 ;B loss: 2.237 ;acc: 0.470 ;iou: 0.650 ;time: 0:01:58\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 1485 ;lr: 0.0500 ;B loss: 2.216 ;acc: 0.525 ;iou: 0.665 ;time: 0:02:05\n",
      "\n",
      "*Training B: False ;B Train loss: 1.806 ;Train accuracy: 0.648 ;IOU accuracy: 0.753 ;Time: 0:02:11 \n",
      "\n",
      "Testing, ephoc: 13\n",
      "batch: 0 ;B loss: 1.755 ;acc: 0.840 ;iou_acc: 0.915 ;time: 0:02:12\n",
      "batch: 50 ;B loss: 1.287 ;acc: 0.780 ;iou_acc: 0.830 ;time: 0:02:16\n",
      "batch: 100 ;B loss: 1.584 ;acc: 0.740 ;iou_acc: 0.795 ;time: 0:02:21\n",
      "batch: 150 ;B loss: 1.592 ;acc: 0.690 ;iou_acc: 0.755 ;time: 0:02:26\n",
      "batch: 200 ;B loss: 1.958 ;acc: 0.535 ;iou_acc: 0.670 ;time: 0:02:33\n",
      "batch: 250 ;B loss: 2.365 ;acc: 0.490 ;iou_acc: 0.625 ;time: 0:02:41\n",
      "\n",
      "*BTrain: False ;Test loss: 1.823 ;Test accuracy 0.638 ;IOU accuracy: 0.744 ;Time: 0:02:51\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 14 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 14 ;batch: 0 ;Bloss:1.703 ;Aloss:0.175 ;edits: 0.29 ;activateAProb:1.0 ;acc: 0.620 ;iou: 0.710 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 14 ;batch: 50 ;Bloss:1.885 ;Aloss:0.254 ;edits: 0.47 ;activateAProb:1.0 ;acc: 0.515 ;iou: 0.600 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 14 ;batch: 100 ;Bloss:2.226 ;Aloss:0.313 ;edits: 0.56 ;activateAProb:1.0 ;acc: 0.315 ;iou: 0.425 ;time: 0:00:26\n",
      "Edit: True ;A: True ;ephoc: 14 ;batch: 150 ;Bloss:2.449 ;Aloss:0.177 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.360 ;iou: 0.530 ;time: 0:00:39\n",
      "Edit: True ;A: True ;ephoc: 14 ;batch: 200 ;Bloss:2.083 ;Aloss:0.256 ;edits: 0.40 ;activateAProb:1.0 ;acc: 0.395 ;iou: 0.515 ;time: 0:00:51\n",
      "Edit: True ;A: True ;ephoc: 14 ;batch: 250 ;Bloss:2.034 ;Aloss:0.256 ;edits: 0.45 ;activateAProb:1.0 ;acc: 0.415 ;iou: 0.525 ;time: 0:01:05\n",
      "\n",
      "*Training B: False ;B Train loss: 2.130 ;A Train loss: 0.212 ;Edit num: 0.329 ;Train accuracy: 0.466 ;IOU accuracy: 0.572 ;Time: 0:01:17 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 15 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 15 ;batch: 0 ;Bloss:1.852 ;Aloss:0.267 ;edits: 0.42 ;activateAProb:1.0 ;acc: 0.535 ;iou: 0.630 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 15 ;batch: 50 ;Bloss:1.592 ;Aloss:0.222 ;edits: 0.18 ;activateAProb:1.0 ;acc: 0.560 ;iou: 0.665 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 15 ;batch: 100 ;Bloss:1.613 ;Aloss:0.176 ;edits: 0.24 ;activateAProb:1.0 ;acc: 0.610 ;iou: 0.670 ;time: 0:00:26\n",
      "Edit: True ;A: True ;ephoc: 15 ;batch: 150 ;Bloss:1.846 ;Aloss:0.215 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.515 ;iou: 0.630 ;time: 0:00:38\n",
      "Edit: True ;A: True ;ephoc: 15 ;batch: 200 ;Bloss:3.277 ;Aloss:0.198 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.125 ;iou: 0.320 ;time: 0:00:50\n",
      "Edit: True ;A: True ;ephoc: 15 ;batch: 250 ;Bloss:1.668 ;Aloss:0.174 ;edits: 0.20 ;activateAProb:1.0 ;acc: 0.660 ;iou: 0.725 ;time: 0:01:04\n",
      "\n",
      "*Training B: True ;B Train loss: 2.135 ;A Train loss: 0.214 ;Edit num: 0.334 ;Train accuracy: 0.463 ;IOU accuracy: 0.570 ;Time: 0:01:17 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 16 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 16 ;batch: 0 ;gs: 1486 ;lr: 0.0500 ;B loss:2.636 ;A loss:0.211 ;edits:0.38 ;activateAProb:1.0 ;acc: 0.330 ;iou: 0.510 ;time: 0:00:01\n",
      "Edit: True ;A: True ;ephoc: 16 ;batch: 50 ;gs: 1536 ;lr: 0.0500 ;B loss:1.758 ;A loss:0.255 ;edits:0.28 ;activateAProb:1.0 ;acc: 0.555 ;iou: 0.645 ;time: 0:00:17\n",
      "Edit: True ;A: True ;ephoc: 16 ;batch: 100 ;gs: 1586 ;lr: 0.0500 ;B loss:2.103 ;A loss:0.257 ;edits:0.41 ;activateAProb:1.0 ;acc: 0.515 ;iou: 0.615 ;time: 0:00:33\n",
      "Edit: True ;A: True ;ephoc: 16 ;batch: 150 ;gs: 1636 ;lr: 0.0500 ;B loss:2.936 ;A loss:0.170 ;edits:0.34 ;activateAProb:1.0 ;acc: 0.245 ;iou: 0.405 ;time: 0:00:49\n",
      "Edit: True ;A: True ;ephoc: 16 ;batch: 200 ;gs: 1686 ;lr: 0.0500 ;B loss:2.220 ;A loss:0.215 ;edits:0.40 ;activateAProb:1.0 ;acc: 0.505 ;iou: 0.575 ;time: 0:01:06\n",
      "Edit: False ;A: True ;batch: 250 ;gs: 1736 ;lr: 0.0500 ;B loss: 1.330 ;acc: 0.800 ;iou: 0.840 ;time: 0:01:22\n",
      "\n",
      "*Training B: False ;B Train loss: 2.087 ;A Train loss: 0.207 ;Edit num: 0.310 ;Train accuracy: 0.516 ;IOU accuracy: 0.620 ;Time: 0:01:38 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 1782 ;lr: 0.0500 ;B loss: 2.213 ;acc: 0.495 ;iou: 0.665 ;time: 0:01:38\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 1782 ;lr: 0.0500 ;B loss: 1.364 ;acc: 0.770 ;iou: 0.840 ;time: 0:01:44\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 1782 ;lr: 0.0500 ;B loss: 1.711 ;acc: 0.670 ;iou: 0.750 ;time: 0:01:51\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 1782 ;lr: 0.0500 ;B loss: 2.533 ;acc: 0.400 ;iou: 0.550 ;time: 0:01:57\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 1782 ;lr: 0.0500 ;B loss: 1.732 ;acc: 0.695 ;iou: 0.785 ;time: 0:02:05\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 1782 ;lr: 0.0500 ;B loss: 1.325 ;acc: 0.785 ;iou: 0.845 ;time: 0:02:11\n",
      "\n",
      "*Training B: False ;B Train loss: 1.773 ;Train accuracy: 0.661 ;IOU accuracy: 0.764 ;Time: 0:02:17 \n",
      "\n",
      "Testing, ephoc: 16\n",
      "batch: 0 ;B loss: 1.799 ;acc: 0.855 ;iou_acc: 0.920 ;time: 0:02:18\n",
      "batch: 50 ;B loss: 1.257 ;acc: 0.790 ;iou_acc: 0.840 ;time: 0:02:22\n",
      "batch: 100 ;B loss: 1.546 ;acc: 0.730 ;iou_acc: 0.785 ;time: 0:02:26\n",
      "batch: 150 ;B loss: 1.562 ;acc: 0.695 ;iou_acc: 0.770 ;time: 0:02:31\n",
      "batch: 200 ;B loss: 1.906 ;acc: 0.555 ;iou_acc: 0.685 ;time: 0:02:38\n",
      "batch: 250 ;B loss: 2.321 ;acc: 0.495 ;iou_acc: 0.620 ;time: 0:02:45\n",
      "\n",
      "*BTrain: False ;Test loss: 1.793 ;Test accuracy 0.650 ;IOU accuracy: 0.754 ;Time: 0:02:55\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 17 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 17 ;batch: 0 ;Bloss:1.754 ;Aloss:0.209 ;edits: 0.31 ;activateAProb:1.0 ;acc: 0.535 ;iou: 0.595 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 17 ;batch: 50 ;Bloss:3.367 ;Aloss:0.218 ;edits: 0.45 ;activateAProb:1.0 ;acc: 0.140 ;iou: 0.275 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 17 ;batch: 100 ;Bloss:1.615 ;Aloss:0.228 ;edits: 0.20 ;activateAProb:1.0 ;acc: 0.630 ;iou: 0.685 ;time: 0:00:27\n",
      "Edit: True ;A: True ;ephoc: 17 ;batch: 150 ;Bloss:3.244 ;Aloss:0.177 ;edits: 0.38 ;activateAProb:1.0 ;acc: 0.195 ;iou: 0.330 ;time: 0:00:39\n",
      "Edit: True ;A: True ;ephoc: 17 ;batch: 200 ;Bloss:1.530 ;Aloss:0.229 ;edits: 0.16 ;activateAProb:1.0 ;acc: 0.760 ;iou: 0.800 ;time: 0:00:52\n",
      "Edit: True ;A: True ;ephoc: 17 ;batch: 250 ;Bloss:1.683 ;Aloss:0.232 ;edits: 0.30 ;activateAProb:1.0 ;acc: 0.630 ;iou: 0.690 ;time: 0:01:06\n",
      "\n",
      "*Training B: False ;B Train loss: 2.116 ;A Train loss: 0.208 ;Edit num: 0.307 ;Train accuracy: 0.480 ;IOU accuracy: 0.582 ;Time: 0:01:17 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 18 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 18 ;batch: 0 ;Bloss:3.344 ;Aloss:0.204 ;edits: 0.41 ;activateAProb:1.0 ;acc: 0.160 ;iou: 0.330 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 18 ;batch: 50 ;Bloss:2.930 ;Aloss:0.178 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.215 ;iou: 0.360 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 18 ;batch: 100 ;Bloss:2.740 ;Aloss:0.183 ;edits: 0.25 ;activateAProb:1.0 ;acc: 0.295 ;iou: 0.455 ;time: 0:00:27\n",
      "Edit: True ;A: True ;ephoc: 18 ;batch: 150 ;Bloss:2.735 ;Aloss:0.182 ;edits: 0.32 ;activateAProb:1.0 ;acc: 0.285 ;iou: 0.425 ;time: 0:00:41\n",
      "Edit: True ;A: True ;ephoc: 18 ;batch: 200 ;Bloss:1.907 ;Aloss:0.386 ;edits: 0.39 ;activateAProb:1.0 ;acc: 0.865 ;iou: 0.935 ;time: 0:00:54\n",
      "Edit: True ;A: True ;ephoc: 18 ;batch: 250 ;Bloss:1.781 ;Aloss:0.255 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.520 ;iou: 0.640 ;time: 0:01:07\n",
      "\n",
      "*Training B: True ;B Train loss: 2.107 ;A Train loss: 0.201 ;Edit num: 0.294 ;Train accuracy: 0.487 ;IOU accuracy: 0.587 ;Time: 0:01:19 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 19 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 19 ;batch: 0 ;gs: 1783 ;lr: 0.0500 ;B loss:1.819 ;A loss:0.282 ;edits:0.22 ;activateAProb:1.0 ;acc: 0.865 ;iou: 0.935 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 19 ;batch: 50 ;gs: 1833 ;lr: 0.0500 ;B loss:1.651 ;A loss:0.163 ;edits:0.27 ;activateAProb:1.0 ;acc: 0.685 ;iou: 0.735 ;time: 0:00:15\n",
      "Edit: True ;A: True ;ephoc: 19 ;batch: 100 ;gs: 1883 ;lr: 0.0500 ;B loss:2.235 ;A loss:0.246 ;edits:0.44 ;activateAProb:1.0 ;acc: 0.520 ;iou: 0.615 ;time: 0:00:32\n",
      "Edit: True ;A: True ;ephoc: 19 ;batch: 150 ;gs: 1933 ;lr: 0.0500 ;B loss:1.645 ;A loss:0.245 ;edits:0.24 ;activateAProb:1.0 ;acc: 0.640 ;iou: 0.710 ;time: 0:00:45\n",
      "Edit: True ;A: True ;ephoc: 19 ;batch: 200 ;gs: 1983 ;lr: 0.0500 ;B loss:2.463 ;A loss:0.181 ;edits:0.28 ;activateAProb:1.0 ;acc: 0.400 ;iou: 0.565 ;time: 0:01:01\n",
      "Edit: True ;A: True ;ephoc: 19 ;batch: 250 ;gs: 2033 ;lr: 0.0500 ;B loss:2.010 ;A loss:0.150 ;edits:0.12 ;activateAProb:1.0 ;acc: 0.510 ;iou: 0.620 ;time: 0:01:19\n",
      "\n",
      "*Training B: False ;B Train loss: 2.050 ;A Train loss: 0.195 ;Edit num: 0.287 ;Train accuracy: 0.537 ;IOU accuracy: 0.640 ;Time: 0:01:33 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 2079 ;lr: 0.0500 ;B loss: 1.795 ;acc: 0.885 ;iou: 0.940 ;time: 0:01:33\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 2079 ;lr: 0.0500 ;B loss: 1.289 ;acc: 0.835 ;iou: 0.855 ;time: 0:01:39\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 2079 ;lr: 0.0500 ;B loss: 1.665 ;acc: 0.670 ;iou: 0.790 ;time: 0:01:45\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 2079 ;lr: 0.0500 ;B loss: 1.260 ;acc: 0.790 ;iou: 0.835 ;time: 0:01:51\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 2079 ;lr: 0.0500 ;B loss: 2.169 ;acc: 0.545 ;iou: 0.720 ;time: 0:01:58\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 2079 ;lr: 0.0500 ;B loss: 1.854 ;acc: 0.635 ;iou: 0.705 ;time: 0:02:05\n",
      "\n",
      "*Training B: False ;B Train loss: 1.748 ;Train accuracy: 0.672 ;IOU accuracy: 0.774 ;Time: 0:02:10 \n",
      "\n",
      "Testing, ephoc: 19\n",
      "batch: 0 ;B loss: 1.811 ;acc: 0.855 ;iou_acc: 0.920 ;time: 0:02:11\n",
      "batch: 50 ;B loss: 1.262 ;acc: 0.790 ;iou_acc: 0.840 ;time: 0:02:15\n",
      "batch: 100 ;B loss: 1.520 ;acc: 0.740 ;iou_acc: 0.795 ;time: 0:02:20\n",
      "batch: 150 ;B loss: 1.545 ;acc: 0.705 ;iou_acc: 0.780 ;time: 0:02:25\n",
      "batch: 200 ;B loss: 1.867 ;acc: 0.560 ;iou_acc: 0.685 ;time: 0:02:32\n",
      "batch: 250 ;B loss: 2.295 ;acc: 0.480 ;iou_acc: 0.635 ;time: 0:02:40\n",
      "\n",
      "*BTrain: False ;Test loss: 1.772 ;Test accuracy 0.657 ;IOU accuracy: 0.760 ;Time: 0:02:50\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 20 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 20 ;batch: 0 ;Bloss:1.634 ;Aloss:0.299 ;edits: 0.24 ;activateAProb:1.0 ;acc: 0.695 ;iou: 0.735 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 20 ;batch: 50 ;Bloss:2.703 ;Aloss:0.186 ;edits: 0.30 ;activateAProb:1.0 ;acc: 0.220 ;iou: 0.395 ;time: 0:00:12\n",
      "Edit: True ;A: True ;ephoc: 20 ;batch: 100 ;Bloss:2.898 ;Aloss:0.173 ;edits: 0.29 ;activateAProb:1.0 ;acc: 0.220 ;iou: 0.420 ;time: 0:00:25\n",
      "Edit: True ;A: True ;ephoc: 20 ;batch: 150 ;Bloss:2.845 ;Aloss:0.189 ;edits: 0.33 ;activateAProb:1.0 ;acc: 0.255 ;iou: 0.375 ;time: 0:00:38\n",
      "Edit: True ;A: True ;ephoc: 20 ;batch: 200 ;Bloss:2.863 ;Aloss:0.220 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.295 ;iou: 0.455 ;time: 0:00:51\n",
      "Edit: True ;A: True ;ephoc: 20 ;batch: 250 ;Bloss:2.075 ;Aloss:0.277 ;edits: 0.41 ;activateAProb:1.0 ;acc: 0.465 ;iou: 0.530 ;time: 0:01:04\n",
      "\n",
      "*Training B: False ;B Train loss: 2.079 ;A Train loss: 0.201 ;Edit num: 0.303 ;Train accuracy: 0.498 ;IOU accuracy: 0.601 ;Time: 0:01:17 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 21 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 21 ;batch: 0 ;Bloss:2.000 ;Aloss:0.158 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.460 ;iou: 0.535 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 21 ;batch: 50 ;Bloss:2.282 ;Aloss:0.272 ;edits: 0.44 ;activateAProb:1.0 ;acc: 0.360 ;iou: 0.475 ;time: 0:00:12\n",
      "Edit: True ;A: True ;ephoc: 21 ;batch: 100 ;Bloss:1.607 ;Aloss:0.161 ;edits: 0.25 ;activateAProb:1.0 ;acc: 0.640 ;iou: 0.680 ;time: 0:00:26\n",
      "Edit: True ;A: True ;ephoc: 21 ;batch: 150 ;Bloss:1.853 ;Aloss:0.325 ;edits: 0.36 ;activateAProb:1.0 ;acc: 0.865 ;iou: 0.925 ;time: 0:00:39\n",
      "Edit: True ;A: True ;ephoc: 21 ;batch: 200 ;Bloss:2.012 ;Aloss:0.107 ;edits: 0.18 ;activateAProb:1.0 ;acc: 0.485 ;iou: 0.615 ;time: 0:00:53\n",
      "Edit: True ;A: True ;ephoc: 21 ;batch: 250 ;Bloss:3.142 ;Aloss:0.176 ;edits: 0.31 ;activateAProb:1.0 ;acc: 0.185 ;iou: 0.340 ;time: 0:01:07\n",
      "\n",
      "*Training B: True ;B Train loss: 2.073 ;A Train loss: 0.197 ;Edit num: 0.296 ;Train accuracy: 0.501 ;IOU accuracy: 0.604 ;Time: 0:01:20 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 22 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 22 ;batch: 0 ;gs: 2080 ;lr: 0.0500 ;B loss:2.049 ;A loss:0.172 ;edits:0.28 ;activateAProb:1.0 ;acc: 0.445 ;iou: 0.535 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 22 ;batch: 50 ;gs: 2130 ;lr: 0.0500 ;B loss:2.487 ;A loss:0.231 ;edits:0.40 ;activateAProb:1.0 ;acc: 0.375 ;iou: 0.500 ;time: 0:00:18\n",
      "Edit: True ;A: True ;ephoc: 22 ;batch: 100 ;gs: 2180 ;lr: 0.0500 ;B loss:2.695 ;A loss:0.125 ;edits:0.24 ;activateAProb:1.0 ;acc: 0.295 ;iou: 0.465 ;time: 0:00:34\n",
      "Edit: True ;A: True ;ephoc: 22 ;batch: 150 ;gs: 2230 ;lr: 0.0500 ;B loss:2.165 ;A loss:0.214 ;edits:0.31 ;activateAProb:1.0 ;acc: 0.400 ;iou: 0.490 ;time: 0:00:50\n",
      "Edit: True ;A: True ;ephoc: 22 ;batch: 200 ;gs: 2280 ;lr: 0.0500 ;B loss:2.200 ;A loss:0.167 ;edits:0.22 ;activateAProb:1.0 ;acc: 0.425 ;iou: 0.505 ;time: 0:01:05\n",
      "Edit: True ;A: True ;ephoc: 22 ;batch: 250 ;gs: 2330 ;lr: 0.0500 ;B loss:2.627 ;A loss:0.155 ;edits:0.28 ;activateAProb:1.0 ;acc: 0.325 ;iou: 0.485 ;time: 0:01:20\n",
      "\n",
      "*Training B: False ;B Train loss: 2.039 ;A Train loss: 0.201 ;Edit num: 0.296 ;Train accuracy: 0.540 ;IOU accuracy: 0.644 ;Time: 0:01:36 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 2376 ;lr: 0.0500 ;B loss: 1.659 ;acc: 0.650 ;iou: 0.740 ;time: 0:01:36\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 2376 ;lr: 0.0500 ;B loss: 2.029 ;acc: 0.550 ;iou: 0.675 ;time: 0:01:43\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 2376 ;lr: 0.0500 ;B loss: 2.446 ;acc: 0.440 ;iou: 0.620 ;time: 0:01:49\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 2376 ;lr: 0.0500 ;B loss: 1.914 ;acc: 0.585 ;iou: 0.695 ;time: 0:01:55\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 2376 ;lr: 0.0500 ;B loss: 1.895 ;acc: 0.635 ;iou: 0.750 ;time: 0:02:01\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 2376 ;lr: 0.0500 ;B loss: 2.310 ;acc: 0.500 ;iou: 0.650 ;time: 0:02:08\n",
      "\n",
      "*Training B: False ;B Train loss: 1.736 ;Train accuracy: 0.675 ;IOU accuracy: 0.777 ;Time: 0:02:14 \n",
      "\n",
      "Testing, ephoc: 22\n",
      "batch: 0 ;B loss: 1.816 ;acc: 0.850 ;iou_acc: 0.920 ;time: 0:02:14\n",
      "batch: 50 ;B loss: 1.269 ;acc: 0.795 ;iou_acc: 0.850 ;time: 0:02:18\n",
      "batch: 100 ;B loss: 1.503 ;acc: 0.730 ;iou_acc: 0.790 ;time: 0:02:22\n",
      "batch: 150 ;B loss: 1.539 ;acc: 0.670 ;iou_acc: 0.760 ;time: 0:02:27\n",
      "batch: 200 ;B loss: 1.835 ;acc: 0.550 ;iou_acc: 0.690 ;time: 0:02:34\n",
      "batch: 250 ;B loss: 2.282 ;acc: 0.490 ;iou_acc: 0.635 ;time: 0:02:42\n",
      "\n",
      "*BTrain: False ;Test loss: 1.762 ;Test accuracy 0.661 ;IOU accuracy: 0.764 ;Time: 0:02:52\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 23 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 23 ;batch: 0 ;Bloss:2.671 ;Aloss:0.153 ;edits: 0.30 ;activateAProb:1.0 ;acc: 0.340 ;iou: 0.485 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 23 ;batch: 50 ;Bloss:1.584 ;Aloss:0.195 ;edits: 0.25 ;activateAProb:1.0 ;acc: 0.635 ;iou: 0.730 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 23 ;batch: 100 ;Bloss:2.973 ;Aloss:0.118 ;edits: 0.21 ;activateAProb:1.0 ;acc: 0.295 ;iou: 0.430 ;time: 0:00:27\n",
      "Edit: True ;A: True ;ephoc: 23 ;batch: 150 ;Bloss:2.526 ;Aloss:0.279 ;edits: 0.54 ;activateAProb:1.0 ;acc: 0.305 ;iou: 0.410 ;time: 0:00:39\n",
      "Edit: True ;A: True ;ephoc: 23 ;batch: 200 ;Bloss:1.881 ;Aloss:0.181 ;edits: 0.29 ;activateAProb:1.0 ;acc: 0.500 ;iou: 0.590 ;time: 0:00:52\n",
      "Edit: True ;A: True ;ephoc: 23 ;batch: 250 ;Bloss:1.632 ;Aloss:0.158 ;edits: 0.25 ;activateAProb:1.0 ;acc: 0.580 ;iou: 0.670 ;time: 0:01:06\n",
      "\n",
      "*Training B: False ;B Train loss: 2.083 ;A Train loss: 0.203 ;Edit num: 0.313 ;Train accuracy: 0.498 ;IOU accuracy: 0.604 ;Time: 0:01:17 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 24 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 24 ;batch: 0 ;Bloss:1.796 ;Aloss:0.196 ;edits: 0.34 ;activateAProb:1.0 ;acc: 0.510 ;iou: 0.620 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 24 ;batch: 50 ;Bloss:2.755 ;Aloss:0.204 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.235 ;iou: 0.380 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 24 ;batch: 100 ;Bloss:2.218 ;Aloss:0.240 ;edits: 0.43 ;activateAProb:1.0 ;acc: 0.415 ;iou: 0.525 ;time: 0:00:26\n",
      "Edit: True ;A: True ;ephoc: 24 ;batch: 150 ;Bloss:2.167 ;Aloss:0.183 ;edits: 0.40 ;activateAProb:1.0 ;acc: 0.445 ;iou: 0.520 ;time: 0:00:41\n",
      "Edit: True ;A: True ;ephoc: 24 ;batch: 200 ;Bloss:1.842 ;Aloss:0.337 ;edits: 0.39 ;activateAProb:1.0 ;acc: 0.825 ;iou: 0.930 ;time: 0:00:55\n",
      "Edit: True ;A: True ;ephoc: 24 ;batch: 250 ;Bloss:1.671 ;Aloss:0.185 ;edits: 0.30 ;activateAProb:1.0 ;acc: 0.605 ;iou: 0.705 ;time: 0:01:09\n",
      "\n",
      "*Training B: True ;B Train loss: 2.093 ;A Train loss: 0.203 ;Edit num: 0.323 ;Train accuracy: 0.493 ;IOU accuracy: 0.599 ;Time: 0:01:20 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 25 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 25 ;batch: 0 ;gs: 2377 ;lr: 0.0500 ;B loss:2.269 ;A loss:0.230 ;edits:0.42 ;activateAProb:1.0 ;acc: 0.385 ;iou: 0.520 ;time: 0:00:01\n",
      "Edit: True ;A: True ;ephoc: 25 ;batch: 50 ;gs: 2427 ;lr: 0.0500 ;B loss:3.080 ;A loss:0.210 ;edits:0.40 ;activateAProb:1.0 ;acc: 0.270 ;iou: 0.445 ;time: 0:00:17\n",
      "Edit: False ;A: True ;batch: 100 ;gs: 2477 ;lr: 0.0500 ;B loss: 2.855 ;acc: 0.400 ;iou: 0.625 ;time: 0:00:33\n",
      "Edit: True ;A: True ;ephoc: 25 ;batch: 150 ;gs: 2527 ;lr: 0.0500 ;B loss:1.821 ;A loss:0.162 ;edits:0.21 ;activateAProb:1.0 ;acc: 0.550 ;iou: 0.655 ;time: 0:00:49\n",
      "Edit: True ;A: True ;ephoc: 25 ;batch: 200 ;gs: 2577 ;lr: 0.0500 ;B loss:1.910 ;A loss:0.175 ;edits:0.31 ;activateAProb:1.0 ;acc: 0.520 ;iou: 0.630 ;time: 0:01:05\n",
      "Edit: True ;A: True ;ephoc: 25 ;batch: 250 ;gs: 2627 ;lr: 0.0500 ;B loss:2.730 ;A loss:0.180 ;edits:0.27 ;activateAProb:1.0 ;acc: 0.285 ;iou: 0.440 ;time: 0:01:21\n",
      "\n",
      "*Training B: False ;B Train loss: 2.062 ;A Train loss: 0.211 ;Edit num: 0.324 ;Train accuracy: 0.529 ;IOU accuracy: 0.632 ;Time: 0:01:35 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 2673 ;lr: 0.0500 ;B loss: 1.832 ;acc: 0.595 ;iou: 0.730 ;time: 0:01:35\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 2673 ;lr: 0.0500 ;B loss: 2.625 ;acc: 0.400 ;iou: 0.590 ;time: 0:01:43\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 2673 ;lr: 0.0500 ;B loss: 2.875 ;acc: 0.375 ;iou: 0.570 ;time: 0:01:49\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 2673 ;lr: 0.0500 ;B loss: 1.565 ;acc: 0.670 ;iou: 0.760 ;time: 0:01:55\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 2673 ;lr: 0.0500 ;B loss: 1.583 ;acc: 0.720 ;iou: 0.790 ;time: 0:02:02\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 2673 ;lr: 0.0500 ;B loss: 2.459 ;acc: 0.415 ;iou: 0.590 ;time: 0:02:08\n",
      "\n",
      "*Training B: False ;B Train loss: 1.744 ;Train accuracy: 0.677 ;IOU accuracy: 0.776 ;Time: 0:02:14 \n",
      "\n",
      "Testing, ephoc: 25\n",
      "batch: 0 ;B loss: 1.772 ;acc: 0.855 ;iou_acc: 0.920 ;time: 0:02:14\n",
      "batch: 50 ;B loss: 1.267 ;acc: 0.835 ;iou_acc: 0.875 ;time: 0:02:18\n",
      "batch: 100 ;B loss: 1.520 ;acc: 0.730 ;iou_acc: 0.800 ;time: 0:02:23\n",
      "batch: 150 ;B loss: 1.554 ;acc: 0.685 ;iou_acc: 0.765 ;time: 0:02:28\n",
      "batch: 200 ;B loss: 1.853 ;acc: 0.565 ;iou_acc: 0.710 ;time: 0:02:35\n",
      "batch: 250 ;B loss: 2.289 ;acc: 0.490 ;iou_acc: 0.655 ;time: 0:02:43\n",
      "\n",
      "*BTrain: False ;Test loss: 1.771 ;Test accuracy 0.662 ;IOU accuracy: 0.764 ;Time: 0:02:53\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 26 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 26 ;batch: 0 ;Bloss:3.097 ;Aloss:0.157 ;edits: 0.30 ;activateAProb:1.0 ;acc: 0.180 ;iou: 0.370 ;time: 0:00:01\n",
      "Edit: True ;A: True ;ephoc: 26 ;batch: 50 ;Bloss:1.795 ;Aloss:0.264 ;edits: 0.29 ;activateAProb:1.0 ;acc: 0.870 ;iou: 0.960 ;time: 0:00:15\n",
      "Edit: True ;A: True ;ephoc: 26 ;batch: 100 ;Bloss:2.729 ;Aloss:0.292 ;edits: 0.44 ;activateAProb:1.0 ;acc: 0.275 ;iou: 0.445 ;time: 0:00:28\n",
      "Edit: True ;A: True ;ephoc: 26 ;batch: 150 ;Bloss:1.504 ;Aloss:0.191 ;edits: 0.31 ;activateAProb:1.0 ;acc: 0.710 ;iou: 0.760 ;time: 0:00:41\n",
      "Edit: True ;A: True ;ephoc: 26 ;batch: 200 ;Bloss:1.620 ;Aloss:0.194 ;edits: 0.22 ;activateAProb:1.0 ;acc: 0.645 ;iou: 0.690 ;time: 0:00:55\n",
      "Edit: True ;A: True ;ephoc: 26 ;batch: 250 ;Bloss:1.863 ;Aloss:0.202 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.565 ;iou: 0.645 ;time: 0:01:08\n",
      "\n",
      "*Training B: False ;B Train loss: 2.072 ;A Train loss: 0.204 ;Edit num: 0.308 ;Train accuracy: 0.503 ;IOU accuracy: 0.606 ;Time: 0:01:20 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 27 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 27 ;batch: 0 ;Bloss:1.577 ;Aloss:0.198 ;edits: 0.20 ;activateAProb:1.0 ;acc: 0.665 ;iou: 0.725 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 27 ;batch: 50 ;Bloss:1.726 ;Aloss:0.163 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.570 ;iou: 0.665 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 27 ;batch: 100 ;Bloss:2.105 ;Aloss:0.258 ;edits: 0.48 ;activateAProb:1.0 ;acc: 0.465 ;iou: 0.585 ;time: 0:00:27\n",
      "Edit: True ;A: True ;ephoc: 27 ;batch: 150 ;Bloss:2.769 ;Aloss:0.199 ;edits: 0.35 ;activateAProb:1.0 ;acc: 0.260 ;iou: 0.400 ;time: 0:00:39\n",
      "Edit: True ;A: True ;ephoc: 27 ;batch: 200 ;Bloss:2.165 ;Aloss:0.176 ;edits: 0.32 ;activateAProb:1.0 ;acc: 0.450 ;iou: 0.580 ;time: 0:00:53\n",
      "Edit: True ;A: True ;ephoc: 27 ;batch: 250 ;Bloss:1.943 ;Aloss:0.201 ;edits: 0.28 ;activateAProb:1.0 ;acc: 0.520 ;iou: 0.640 ;time: 0:01:06\n",
      "\n",
      "*Training B: True ;B Train loss: 2.063 ;A Train loss: 0.199 ;Edit num: 0.299 ;Train accuracy: 0.508 ;IOU accuracy: 0.611 ;Time: 0:01:19 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 28 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 28 ;batch: 0 ;gs: 2674 ;lr: 0.0500 ;B loss:2.283 ;A loss:0.195 ;edits:0.37 ;activateAProb:1.0 ;acc: 0.425 ;iou: 0.535 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 28 ;batch: 50 ;gs: 2724 ;lr: 0.0500 ;B loss:3.118 ;A loss:0.253 ;edits:0.37 ;activateAProb:1.0 ;acc: 0.250 ;iou: 0.445 ;time: 0:00:17\n",
      "Edit: True ;A: True ;ephoc: 28 ;batch: 100 ;gs: 2774 ;lr: 0.0500 ;B loss:2.437 ;A loss:0.253 ;edits:0.40 ;activateAProb:1.0 ;acc: 0.340 ;iou: 0.530 ;time: 0:00:33\n",
      "Edit: True ;A: True ;ephoc: 28 ;batch: 150 ;gs: 2824 ;lr: 0.0500 ;B loss:1.712 ;A loss:0.188 ;edits:0.29 ;activateAProb:1.0 ;acc: 0.690 ;iou: 0.765 ;time: 0:00:49\n",
      "Edit: True ;A: True ;ephoc: 28 ;batch: 200 ;gs: 2874 ;lr: 0.0500 ;B loss:1.582 ;A loss:0.170 ;edits:0.26 ;activateAProb:1.0 ;acc: 0.710 ;iou: 0.785 ;time: 0:01:03\n",
      "Edit: True ;A: True ;ephoc: 28 ;batch: 250 ;gs: 2924 ;lr: 0.0500 ;B loss:2.209 ;A loss:0.171 ;edits:0.28 ;activateAProb:1.0 ;acc: 0.545 ;iou: 0.640 ;time: 0:01:18\n",
      "\n",
      "*Training B: False ;B Train loss: 2.039 ;A Train loss: 0.205 ;Edit num: 0.317 ;Train accuracy: 0.543 ;IOU accuracy: 0.645 ;Time: 0:01:32 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 2970 ;lr: 0.0500 ;B loss: 1.808 ;acc: 0.665 ;iou: 0.775 ;time: 0:01:32\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 2970 ;lr: 0.0500 ;B loss: 2.647 ;acc: 0.430 ;iou: 0.640 ;time: 0:01:39\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 2970 ;lr: 0.0500 ;B loss: 2.004 ;acc: 0.540 ;iou: 0.700 ;time: 0:01:46\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 2970 ;lr: 0.0500 ;B loss: 1.322 ;acc: 0.830 ;iou: 0.890 ;time: 0:01:53\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 2970 ;lr: 0.0500 ;B loss: 1.210 ;acc: 0.860 ;iou: 0.925 ;time: 0:01:59\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 2970 ;lr: 0.0500 ;B loss: 1.901 ;acc: 0.620 ;iou: 0.735 ;time: 0:02:04\n",
      "\n",
      "*Training B: False ;B Train loss: 1.709 ;Train accuracy: 0.691 ;IOU accuracy: 0.789 ;Time: 0:02:11 \n",
      "\n",
      "Testing, ephoc: 28\n",
      "batch: 0 ;B loss: 1.810 ;acc: 0.860 ;iou_acc: 0.920 ;time: 0:02:11\n",
      "batch: 50 ;B loss: 1.248 ;acc: 0.810 ;iou_acc: 0.855 ;time: 0:02:15\n",
      "batch: 100 ;B loss: 1.486 ;acc: 0.740 ;iou_acc: 0.810 ;time: 0:02:20\n",
      "batch: 150 ;B loss: 1.539 ;acc: 0.710 ;iou_acc: 0.785 ;time: 0:02:26\n",
      "batch: 200 ;B loss: 1.807 ;acc: 0.575 ;iou_acc: 0.700 ;time: 0:02:33\n",
      "batch: 250 ;B loss: 2.239 ;acc: 0.515 ;iou_acc: 0.645 ;time: 0:02:41\n",
      "\n",
      "*BTrain: False ;Test loss: 1.741 ;Test accuracy 0.672 ;IOU accuracy: 0.774 ;Time: 0:02:50\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 29 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 29 ;batch: 0 ;Bloss:1.590 ;Aloss:0.187 ;edits: 0.28 ;activateAProb:1.0 ;acc: 0.720 ;iou: 0.785 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 29 ;batch: 50 ;Bloss:2.014 ;Aloss:0.124 ;edits: 0.19 ;activateAProb:1.0 ;acc: 0.535 ;iou: 0.655 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 29 ;batch: 100 ;Bloss:1.963 ;Aloss:0.212 ;edits: 0.38 ;activateAProb:1.0 ;acc: 0.510 ;iou: 0.590 ;time: 0:00:27\n",
      "Edit: True ;A: True ;ephoc: 29 ;batch: 150 ;Bloss:1.657 ;Aloss:0.250 ;edits: 0.28 ;activateAProb:1.0 ;acc: 0.655 ;iou: 0.730 ;time: 0:00:41\n",
      "Edit: True ;A: True ;ephoc: 29 ;batch: 200 ;Bloss:1.835 ;Aloss:0.177 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.585 ;iou: 0.680 ;time: 0:00:54\n",
      "Edit: True ;A: True ;ephoc: 29 ;batch: 250 ;Bloss:1.742 ;Aloss:0.184 ;edits: 0.28 ;activateAProb:1.0 ;acc: 0.535 ;iou: 0.650 ;time: 0:01:06\n",
      "\n",
      "*Training B: False ;B Train loss: 2.065 ;A Train loss: 0.202 ;Edit num: 0.314 ;Train accuracy: 0.514 ;IOU accuracy: 0.616 ;Time: 0:01:19 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 30 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 30 ;batch: 0 ;Bloss:1.628 ;Aloss:0.199 ;edits: 0.28 ;activateAProb:1.0 ;acc: 0.745 ;iou: 0.795 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 30 ;batch: 50 ;Bloss:1.666 ;Aloss:0.166 ;edits: 0.28 ;activateAProb:1.0 ;acc: 0.740 ;iou: 0.810 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 30 ;batch: 100 ;Bloss:1.595 ;Aloss:0.191 ;edits: 0.29 ;activateAProb:1.0 ;acc: 0.745 ;iou: 0.810 ;time: 0:00:27\n",
      "Edit: True ;A: True ;ephoc: 30 ;batch: 150 ;Bloss:1.830 ;Aloss:0.371 ;edits: 0.44 ;activateAProb:1.0 ;acc: 0.865 ;iou: 0.955 ;time: 0:00:40\n",
      "Edit: True ;A: True ;ephoc: 30 ;batch: 200 ;Bloss:2.231 ;Aloss:0.243 ;edits: 0.47 ;activateAProb:1.0 ;acc: 0.415 ;iou: 0.550 ;time: 0:00:54\n",
      "Edit: True ;A: True ;ephoc: 30 ;batch: 250 ;Bloss:3.177 ;Aloss:0.231 ;edits: 0.36 ;activateAProb:1.0 ;acc: 0.165 ;iou: 0.360 ;time: 0:01:08\n",
      "\n",
      "*Training B: True ;B Train loss: 2.069 ;A Train loss: 0.203 ;Edit num: 0.321 ;Train accuracy: 0.512 ;IOU accuracy: 0.614 ;Time: 0:01:20 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 31 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 31 ;batch: 0 ;gs: 2971 ;lr: 0.0500 ;B loss:1.828 ;A loss:0.199 ;edits:0.38 ;activateAProb:1.0 ;acc: 0.655 ;iou: 0.735 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 31 ;batch: 50 ;gs: 3021 ;lr: 0.0500 ;B loss:1.668 ;A loss:0.152 ;edits:0.30 ;activateAProb:1.0 ;acc: 0.640 ;iou: 0.725 ;time: 0:00:16\n",
      "Edit: True ;A: True ;ephoc: 31 ;batch: 100 ;gs: 3071 ;lr: 0.0500 ;B loss:1.952 ;A loss:0.227 ;edits:0.34 ;activateAProb:1.0 ;acc: 0.460 ;iou: 0.530 ;time: 0:00:31\n",
      "Edit: True ;A: True ;ephoc: 31 ;batch: 150 ;gs: 3121 ;lr: 0.0500 ;B loss:2.167 ;A loss:0.207 ;edits:0.37 ;activateAProb:1.0 ;acc: 0.455 ;iou: 0.540 ;time: 0:00:47\n",
      "Edit: True ;A: True ;ephoc: 31 ;batch: 200 ;gs: 3171 ;lr: 0.0500 ;B loss:2.278 ;A loss:0.209 ;edits:0.36 ;activateAProb:1.0 ;acc: 0.420 ;iou: 0.525 ;time: 0:01:03\n",
      "Edit: True ;A: True ;ephoc: 31 ;batch: 250 ;gs: 3221 ;lr: 0.0500 ;B loss:1.750 ;A loss:0.236 ;edits:0.26 ;activateAProb:1.0 ;acc: 0.540 ;iou: 0.600 ;time: 0:01:20\n",
      "\n",
      "*Training B: False ;B Train loss: 2.045 ;A Train loss: 0.208 ;Edit num: 0.322 ;Train accuracy: 0.542 ;IOU accuracy: 0.643 ;Time: 0:01:35 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 3267 ;lr: 0.0500 ;B loss: 1.402 ;acc: 0.770 ;iou: 0.840 ;time: 0:01:35\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 3267 ;lr: 0.0500 ;B loss: 1.278 ;acc: 0.815 ;iou: 0.885 ;time: 0:01:41\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 3267 ;lr: 0.0500 ;B loss: 1.573 ;acc: 0.735 ;iou: 0.800 ;time: 0:01:48\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 3267 ;lr: 0.0500 ;B loss: 1.779 ;acc: 0.630 ;iou: 0.695 ;time: 0:01:55\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 3267 ;lr: 0.0500 ;B loss: 1.891 ;acc: 0.640 ;iou: 0.740 ;time: 0:02:01\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 3267 ;lr: 0.0500 ;B loss: 1.328 ;acc: 0.750 ;iou: 0.810 ;time: 0:02:08\n",
      "\n",
      "*Training B: False ;B Train loss: 1.701 ;Train accuracy: 0.694 ;IOU accuracy: 0.791 ;Time: 0:02:14 \n",
      "\n",
      "Testing, ephoc: 31\n",
      "batch: 0 ;B loss: 1.818 ;acc: 0.860 ;iou_acc: 0.930 ;time: 0:02:14\n",
      "batch: 50 ;B loss: 1.237 ;acc: 0.845 ;iou_acc: 0.875 ;time: 0:02:18\n",
      "batch: 100 ;B loss: 1.490 ;acc: 0.740 ;iou_acc: 0.810 ;time: 0:02:23\n",
      "batch: 150 ;B loss: 1.531 ;acc: 0.685 ;iou_acc: 0.760 ;time: 0:02:29\n",
      "batch: 200 ;B loss: 1.804 ;acc: 0.600 ;iou_acc: 0.735 ;time: 0:02:36\n",
      "batch: 250 ;B loss: 2.216 ;acc: 0.520 ;iou_acc: 0.675 ;time: 0:02:44\n",
      "\n",
      "*BTrain: False ;Test loss: 1.736 ;Test accuracy 0.675 ;IOU accuracy: 0.775 ;Time: 0:02:53\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 32 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 32 ;batch: 0 ;Bloss:1.641 ;Aloss:0.270 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.690 ;iou: 0.725 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 32 ;batch: 50 ;Bloss:1.816 ;Aloss:0.183 ;edits: 0.21 ;activateAProb:1.0 ;acc: 0.515 ;iou: 0.600 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 32 ;batch: 100 ;Bloss:1.876 ;Aloss:0.222 ;edits: 0.33 ;activateAProb:1.0 ;acc: 0.530 ;iou: 0.615 ;time: 0:00:25\n",
      "Edit: True ;A: True ;ephoc: 32 ;batch: 150 ;Bloss:2.224 ;Aloss:0.229 ;edits: 0.36 ;activateAProb:1.0 ;acc: 0.350 ;iou: 0.500 ;time: 0:00:38\n",
      "Edit: True ;A: True ;ephoc: 32 ;batch: 200 ;Bloss:2.063 ;Aloss:0.284 ;edits: 0.46 ;activateAProb:1.0 ;acc: 0.450 ;iou: 0.515 ;time: 0:00:51\n",
      "Edit: True ;A: True ;ephoc: 32 ;batch: 250 ;Bloss:2.085 ;Aloss:0.269 ;edits: 0.45 ;activateAProb:1.0 ;acc: 0.450 ;iou: 0.555 ;time: 0:01:04\n",
      "\n",
      "*Training B: False ;B Train loss: 2.072 ;A Train loss: 0.207 ;Edit num: 0.327 ;Train accuracy: 0.506 ;IOU accuracy: 0.608 ;Time: 0:01:16 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 33 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 33 ;batch: 0 ;Bloss:1.559 ;Aloss:0.110 ;edits: 0.11 ;activateAProb:1.0 ;acc: 0.625 ;iou: 0.735 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 33 ;batch: 50 ;Bloss:1.891 ;Aloss:0.195 ;edits: 0.33 ;activateAProb:1.0 ;acc: 0.490 ;iou: 0.575 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 33 ;batch: 100 ;Bloss:3.267 ;Aloss:0.199 ;edits: 0.40 ;activateAProb:1.0 ;acc: 0.170 ;iou: 0.405 ;time: 0:00:26\n",
      "Edit: True ;A: True ;ephoc: 33 ;batch: 150 ;Bloss:1.826 ;Aloss:0.430 ;edits: 0.49 ;activateAProb:1.0 ;acc: 0.820 ;iou: 0.915 ;time: 0:00:40\n",
      "Edit: True ;A: True ;ephoc: 33 ;batch: 200 ;Bloss:2.641 ;Aloss:0.166 ;edits: 0.25 ;activateAProb:1.0 ;acc: 0.270 ;iou: 0.440 ;time: 0:00:53\n",
      "Edit: True ;A: True ;ephoc: 33 ;batch: 250 ;Bloss:1.580 ;Aloss:0.203 ;edits: 0.29 ;activateAProb:1.0 ;acc: 0.745 ;iou: 0.805 ;time: 0:01:08\n",
      "\n",
      "*Training B: True ;B Train loss: 2.067 ;A Train loss: 0.205 ;Edit num: 0.321 ;Train accuracy: 0.507 ;IOU accuracy: 0.610 ;Time: 0:01:20 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 34 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 34 ;batch: 0 ;gs: 3268 ;lr: 0.0500 ;B loss:2.215 ;A loss:0.244 ;edits:0.44 ;activateAProb:1.0 ;acc: 0.450 ;iou: 0.505 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 34 ;batch: 50 ;gs: 3318 ;lr: 0.0500 ;B loss:1.658 ;A loss:0.171 ;edits:0.27 ;activateAProb:1.0 ;acc: 0.660 ;iou: 0.690 ;time: 0:00:16\n",
      "Edit: True ;A: True ;ephoc: 34 ;batch: 100 ;gs: 3368 ;lr: 0.0500 ;B loss:2.071 ;A loss:0.131 ;edits:0.18 ;activateAProb:1.0 ;acc: 0.570 ;iou: 0.675 ;time: 0:00:33\n",
      "Edit: True ;A: True ;ephoc: 34 ;batch: 150 ;gs: 3418 ;lr: 0.0500 ;B loss:2.562 ;A loss:0.225 ;edits:0.29 ;activateAProb:1.0 ;acc: 0.385 ;iou: 0.525 ;time: 0:00:49\n",
      "Edit: False ;A: True ;batch: 200 ;gs: 3468 ;lr: 0.0500 ;B loss: 2.170 ;acc: 0.540 ;iou: 0.655 ;time: 0:01:04\n",
      "Edit: True ;A: True ;ephoc: 34 ;batch: 250 ;gs: 3518 ;lr: 0.0500 ;B loss:1.794 ;A loss:0.214 ;edits:0.11 ;activateAProb:1.0 ;acc: 0.885 ;iou: 0.935 ;time: 0:01:20\n",
      "\n",
      "*Training B: False ;B Train loss: 2.034 ;A Train loss: 0.204 ;Edit num: 0.316 ;Train accuracy: 0.549 ;IOU accuracy: 0.650 ;Time: 0:01:37 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 3564 ;lr: 0.0500 ;B loss: 1.575 ;acc: 0.740 ;iou: 0.810 ;time: 0:01:37\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 3564 ;lr: 0.0500 ;B loss: 1.275 ;acc: 0.860 ;iou: 0.880 ;time: 0:01:43\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 3564 ;lr: 0.0500 ;B loss: 1.779 ;acc: 0.655 ;iou: 0.750 ;time: 0:01:50\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 3564 ;lr: 0.0500 ;B loss: 2.223 ;acc: 0.515 ;iou: 0.670 ;time: 0:01:56\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 3564 ;lr: 0.0500 ;B loss: 2.148 ;acc: 0.530 ;iou: 0.645 ;time: 0:02:03\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 3564 ;lr: 0.0500 ;B loss: 1.858 ;acc: 0.875 ;iou: 0.940 ;time: 0:02:10\n",
      "\n",
      "*Training B: False ;B Train loss: 1.676 ;Train accuracy: 0.700 ;IOU accuracy: 0.797 ;Time: 0:02:16 \n",
      "\n",
      "Testing, ephoc: 34\n",
      "batch: 0 ;B loss: 1.886 ;acc: 0.855 ;iou_acc: 0.925 ;time: 0:02:16\n",
      "batch: 50 ;B loss: 1.223 ;acc: 0.835 ;iou_acc: 0.870 ;time: 0:02:20\n",
      "batch: 100 ;B loss: 1.449 ;acc: 0.740 ;iou_acc: 0.815 ;time: 0:02:25\n",
      "batch: 150 ;B loss: 1.497 ;acc: 0.725 ;iou_acc: 0.805 ;time: 0:02:31\n",
      "batch: 200 ;B loss: 1.762 ;acc: 0.590 ;iou_acc: 0.735 ;time: 0:02:38\n",
      "batch: 250 ;B loss: 2.183 ;acc: 0.560 ;iou_acc: 0.675 ;time: 0:02:45\n",
      "\n",
      "*BTrain: False ;Test loss: 1.715 ;Test accuracy 0.681 ;IOU accuracy: 0.781 ;Time: 0:02:55\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 35 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 35 ;batch: 0 ;Bloss:1.860 ;Aloss:0.202 ;edits: 0.35 ;activateAProb:1.0 ;acc: 0.500 ;iou: 0.555 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 35 ;batch: 50 ;Bloss:1.708 ;Aloss:0.216 ;edits: 0.32 ;activateAProb:1.0 ;acc: 0.720 ;iou: 0.765 ;time: 0:00:13\n",
      "Edit: True ;A: True ;ephoc: 35 ;batch: 100 ;Bloss:2.574 ;Aloss:0.214 ;edits: 0.36 ;activateAProb:1.0 ;acc: 0.240 ;iou: 0.375 ;time: 0:00:26\n",
      "Edit: True ;A: True ;ephoc: 35 ;batch: 150 ;Bloss:2.861 ;Aloss:0.195 ;edits: 0.36 ;activateAProb:1.0 ;acc: 0.245 ;iou: 0.390 ;time: 0:00:39\n",
      "Edit: True ;A: True ;ephoc: 35 ;batch: 200 ;Bloss:1.643 ;Aloss:0.192 ;edits: 0.31 ;activateAProb:1.0 ;acc: 0.730 ;iou: 0.785 ;time: 0:00:53\n",
      "Edit: True ;A: True ;ephoc: 35 ;batch: 250 ;Bloss:1.965 ;Aloss:0.265 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.405 ;iou: 0.490 ;time: 0:01:07\n",
      "\n",
      "*Training B: False ;B Train loss: 2.092 ;A Train loss: 0.212 ;Edit num: 0.343 ;Train accuracy: 0.496 ;IOU accuracy: 0.601 ;Time: 0:01:20 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 36 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 36 ;batch: 0 ;Bloss:2.606 ;Aloss:0.206 ;edits: 0.36 ;activateAProb:1.0 ;acc: 0.325 ;iou: 0.500 ;time: 0:00:01\n",
      "Edit: True ;A: True ;ephoc: 36 ;batch: 50 ;Bloss:1.740 ;Aloss:0.195 ;edits: 0.27 ;activateAProb:1.0 ;acc: 0.560 ;iou: 0.650 ;time: 0:00:14\n",
      "Edit: True ;A: True ;ephoc: 36 ;batch: 100 ;Bloss:1.945 ;Aloss:0.178 ;edits: 0.34 ;activateAProb:1.0 ;acc: 0.540 ;iou: 0.650 ;time: 0:00:27\n",
      "Edit: True ;A: True ;ephoc: 36 ;batch: 150 ;Bloss:1.849 ;Aloss:0.206 ;edits: 0.38 ;activateAProb:1.0 ;acc: 0.555 ;iou: 0.660 ;time: 0:00:40\n",
      "Edit: True ;A: True ;ephoc: 36 ;batch: 200 ;Bloss:1.750 ;Aloss:0.187 ;edits: 0.30 ;activateAProb:1.0 ;acc: 0.495 ;iou: 0.550 ;time: 0:00:54\n",
      "Edit: True ;A: True ;ephoc: 36 ;batch: 250 ;Bloss:1.656 ;Aloss:0.234 ;edits: 0.33 ;activateAProb:1.0 ;acc: 0.600 ;iou: 0.690 ;time: 0:01:08\n",
      "\n",
      "*Training B: True ;B Train loss: 2.095 ;A Train loss: 0.210 ;Edit num: 0.341 ;Train accuracy: 0.497 ;IOU accuracy: 0.601 ;Time: 0:01:21 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 37 ; Training B: True\n",
      "Edit: True ;A: True ;ephoc: 37 ;batch: 0 ;gs: 3565 ;lr: 0.0500 ;B loss:1.828 ;A loss:0.151 ;edits:0.35 ;activateAProb:1.0 ;acc: 0.530 ;iou: 0.570 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 37 ;batch: 50 ;gs: 3615 ;lr: 0.0500 ;B loss:2.118 ;A loss:0.192 ;edits:0.38 ;activateAProb:1.0 ;acc: 0.450 ;iou: 0.535 ;time: 0:00:17\n",
      "Edit: True ;A: True ;ephoc: 37 ;batch: 100 ;gs: 3665 ;lr: 0.0500 ;B loss:1.616 ;A loss:0.140 ;edits:0.27 ;activateAProb:1.0 ;acc: 0.700 ;iou: 0.775 ;time: 0:00:33\n",
      "Edit: False ;A: True ;batch: 150 ;gs: 3715 ;lr: 0.0500 ;B loss: 1.408 ;acc: 0.775 ;iou: 0.830 ;time: 0:00:49\n",
      "Edit: True ;A: True ;ephoc: 37 ;batch: 200 ;gs: 3765 ;lr: 0.0500 ;B loss:1.952 ;A loss:0.231 ;edits:0.46 ;activateAProb:1.0 ;acc: 0.520 ;iou: 0.585 ;time: 0:01:04\n",
      "Edit: True ;A: True ;ephoc: 37 ;batch: 250 ;gs: 3815 ;lr: 0.0500 ;B loss:1.708 ;A loss:0.206 ;edits:0.28 ;activateAProb:1.0 ;acc: 0.730 ;iou: 0.780 ;time: 0:01:21\n",
      "\n",
      "*Training B: False ;B Train loss: 2.043 ;A Train loss: 0.209 ;Edit num: 0.333 ;Train accuracy: 0.544 ;IOU accuracy: 0.645 ;Time: 0:01:37 \n",
      "\n",
      "No Edit\n",
      "ooooooo\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 3861 ;lr: 0.0500 ;B loss: 1.366 ;acc: 0.750 ;iou: 0.805 ;time: 0:01:37\n",
      "Edit: False ;A: False ;batch: 50 ;gs: 3861 ;lr: 0.0500 ;B loss: 1.719 ;acc: 0.685 ;iou: 0.780 ;time: 0:01:43\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 3861 ;lr: 0.0500 ;B loss: 1.269 ;acc: 0.800 ;iou: 0.875 ;time: 0:01:50\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 3861 ;lr: 0.0500 ;B loss: 1.392 ;acc: 0.760 ;iou: 0.810 ;time: 0:01:57\n",
      "Edit: False ;A: False ;batch: 200 ;gs: 3861 ;lr: 0.0500 ;B loss: 1.426 ;acc: 0.735 ;iou: 0.820 ;time: 0:02:03\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 3861 ;lr: 0.0500 ;B loss: 1.297 ;acc: 0.890 ;iou: 0.925 ;time: 0:02:09\n",
      "\n",
      "*Training B: False ;B Train loss: 1.689 ;Train accuracy: 0.700 ;IOU accuracy: 0.796 ;Time: 0:02:15 \n",
      "\n",
      "Testing, ephoc: 37\n",
      "batch: 0 ;B loss: 1.834 ;acc: 0.845 ;iou_acc: 0.920 ;time: 0:02:16\n",
      "batch: 50 ;B loss: 1.243 ;acc: 0.830 ;iou_acc: 0.870 ;time: 0:02:20\n",
      "batch: 100 ;B loss: 1.466 ;acc: 0.740 ;iou_acc: 0.815 ;time: 0:02:25\n",
      "batch: 150 ;B loss: 1.515 ;acc: 0.705 ;iou_acc: 0.780 ;time: 0:02:31\n",
      "batch: 200 ;B loss: 1.776 ;acc: 0.595 ;iou_acc: 0.745 ;time: 0:02:38\n",
      "batch: 250 ;B loss: 2.200 ;acc: 0.530 ;iou_acc: 0.685 ;time: 0:02:46\n",
      "\n",
      "*BTrain: False ;Test loss: 1.729 ;Test accuracy 0.678 ;IOU accuracy: 0.778 ;Time: 0:02:55\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 38 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 38 ;batch: 0 ;Bloss:2.785 ;Aloss:0.208 ;edits: 0.46 ;activateAProb:1.0 ;acc: 0.340 ;iou: 0.500 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 38 ;batch: 50 ;Bloss:2.307 ;Aloss:0.225 ;edits: 0.43 ;activateAProb:1.0 ;acc: 0.390 ;iou: 0.475 ;time: 0:00:12\n",
      "Edit: True ;A: True ;ephoc: 38 ;batch: 100 ;Bloss:2.036 ;Aloss:0.266 ;edits: 0.47 ;activateAProb:1.0 ;acc: 0.480 ;iou: 0.545 ;time: 0:00:24\n",
      "Edit: True ;A: True ;ephoc: 38 ;batch: 150 ;Bloss:1.698 ;Aloss:0.119 ;edits: 0.24 ;activateAProb:1.0 ;acc: 0.575 ;iou: 0.670 ;time: 0:00:36\n",
      "Edit: True ;A: True ;ephoc: 38 ;batch: 200 ;Bloss:1.814 ;Aloss:0.159 ;edits: 0.24 ;activateAProb:1.0 ;acc: 0.590 ;iou: 0.680 ;time: 0:00:49\n",
      "Edit: True ;A: True ;ephoc: 38 ;batch: 250 ;Bloss:1.833 ;Aloss:0.342 ;edits: 0.37 ;activateAProb:1.0 ;acc: 0.830 ;iou: 0.900 ;time: 0:01:04\n",
      "\n",
      "*Training B: False ;B Train loss: 2.060 ;A Train loss: 0.208 ;Edit num: 0.329 ;Train accuracy: 0.510 ;IOU accuracy: 0.614 ;Time: 0:01:15 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 39 ; Training B: False\n",
      "Edit: True ;A: True ;ephoc: 39 ;batch: 0 ;Bloss:2.773 ;Aloss:0.213 ;edits: 0.44 ;activateAProb:1.0 ;acc: 0.245 ;iou: 0.425 ;time: 0:00:00\n",
      "Edit: True ;A: True ;ephoc: 39 ;batch: 50 ;Bloss:1.901 ;Aloss:0.323 ;edits: 0.34 ;activateAProb:1.0 ;acc: 0.850 ;iou: 0.920 ;time: 0:00:13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f862881d69c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0meditProb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0medit_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         imScale=50)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-f89a65b2ec98>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trn_data, tst_data, ephocs_num, edit_reward, startA, activation_ephoc, muteB, start_ephoc, dropout_in, dropout_out, dropout_img, dropout_q, editProb, max_activateAProb, activateAProb, addNoise, imScale)\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                             A_loss, _ = sess.run(\n\u001b[0;32m--> 801\u001b[0;31m                                 [self.A_loss, self.A_optimizer], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mtoTrainB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_hidden=200\n",
    "params_dir = params_dir_tmp+'RL/all_hidden:'+str(num_hidden)\n",
    "tf.reset_default_graph()\n",
    "m = Model(\n",
    "    batch_size=200, \n",
    "    num_hidden=num_hidden,\n",
    "    embed_size=embed_vecs.shape[1],\n",
    "    img_dims=trainset[0][0][1][0].shape[1], \n",
    "    bbox_dims=trainset[0][0][1][1].shape[1], \n",
    "    lr=.05,\n",
    "    vocab=vocab, \n",
    "    decay_steps=10000, \n",
    "    decay_rate=0.9, \n",
    "    rnn_editProb=0.2,\n",
    "    coefAlr=1,\n",
    "    bnorm=False,\n",
    "    toQscale=True\n",
    ")\n",
    "\n",
    "\n",
    "# start comp\n",
    "print('params_dir:', params_dir)\n",
    "print('num_hidden:', m.num_hidden)\n",
    "print('learning rate:', m.lr)\n",
    "tst, trn = m.train(trainset, testset,\n",
    "        ephocs_num=200,\n",
    "        start_ephoc=6,\n",
    "        startA=5,\n",
    "        activation_ephoc=7,\n",
    "        muteB=2, \n",
    "        activateAProb=1.,\n",
    "        max_activateAProb=1.,\n",
    "        editProb=0.9,\n",
    "        edit_reward=-0.9,\n",
    "        imScale=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params_dir: ../data/training/models/All/RL/all_hidden:200\n",
      "num_hidden: 200\n",
      "learning rate: 0.05\n",
      "# Train set size: 297\n",
      "# Training batches: 297\n",
      "# Test set size: 297\n",
      "# Testing batches: 297\n",
      "Loading parameters from ../data/training/models/All/RL/all_hidden:200/model.ckpt-4\n",
      "INFO:tensorflow:Restoring parameters from ../data/training/models/All/RL/all_hidden:200/model.ckpt-4\n",
      "================================================== \n",
      "Train, ephoc: 5 ; Training B: False\n",
      "Edit: True ;A: False ;ephoc: 5 ;batch: 0 ;B loss: 1.86 ;A loss: 0.32 ;edits: 0.21 ;activateAProb: 1.000 ;acc: 0.520 ;iou: 0.650 ;time: 0:00:00\n",
      "Edit: True ;A: False ;ephoc: 5 ;batch: 50 ;B loss: 1.46 ;A loss: 0.24 ;edits: 0.22 ;activateAProb: 1.000 ;acc: 0.670 ;iou: 0.735 ;time: 0:00:13\n",
      "Edit: True ;A: False ;ephoc: 5 ;batch: 100 ;B loss: 1.98 ;A loss: 0.19 ;edits: 0.20 ;activateAProb: 1.000 ;acc: 0.530 ;iou: 0.600 ;time: 0:00:26\n",
      "Edit: True ;A: False ;ephoc: 5 ;batch: 150 ;B loss: 1.37 ;A loss: 0.20 ;edits: 0.21 ;activateAProb: 1.000 ;acc: 0.685 ;iou: 0.780 ;time: 0:00:40\n",
      "Edit: True ;A: False ;ephoc: 5 ;batch: 200 ;B loss: 1.52 ;A loss: 0.16 ;edits: 0.17 ;activateAProb: 1.000 ;acc: 0.570 ;iou: 0.760 ;time: 0:00:54\n",
      "Edit: True ;A: False ;ephoc: 5 ;batch: 250 ;B loss: 2.25 ;A loss: 0.17 ;edits: 0.21 ;activateAProb: 1.000 ;acc: 0.485 ;iou: 0.630 ;time: 0:01:07\n",
      "\n",
      "*Training B: False ;B Train loss: 2.047 ;A Train loss: 0.184 ;Edit num: 0.199 ;Train accuracy: 0.513 ;IOU accuracy: 0.626 ;Time: 0:01:20 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 6 ; Training B: True\n",
      "Edit: False ;A: True ;batch: 0 ;gs: 595 ;lr: 0.0500 ;B loss: 1.423 ;acc: 0.725 ;iou: 0.830 ;time: 0:00:01\n",
      "Edit: True ;A: True ;ephoc: 6 ;batch: 50 ;gs: 645 ;lr: 0.0500 ;B loss: 1.70 ;A loss: 0.29 ;edits: 0.22 ;activateAProb: 1.000 ;acc: 0.770 ;iou: 0.900 ;time: 0:00:17\n",
      "Edit: True ;A: True ;ephoc: 6 ;batch: 100 ;gs: 695 ;lr: 0.0500 ;B loss: 1.65 ;A loss: 0.37 ;edits: 0.48 ;activateAProb: 1.000 ;acc: 0.595 ;iou: 0.690 ;time: 0:00:34\n",
      "Edit: True ;A: True ;ephoc: 6 ;batch: 150 ;gs: 745 ;lr: 0.0500 ;B loss: 1.76 ;A loss: 0.24 ;edits: 0.38 ;activateAProb: 1.000 ;acc: 0.645 ;iou: 0.725 ;time: 0:00:49\n",
      "Edit: True ;A: True ;ephoc: 6 ;batch: 200 ;gs: 795 ;lr: 0.0500 ;B loss: 2.45 ;A loss: 0.26 ;edits: 0.44 ;activateAProb: 1.000 ;acc: 0.355 ;iou: 0.495 ;time: 0:01:04\n",
      "Edit: True ;A: True ;ephoc: 6 ;batch: 250 ;gs: 845 ;lr: 0.0500 ;B loss: 1.95 ;A loss: 0.29 ;edits: 0.33 ;activateAProb: 1.000 ;acc: 0.460 ;iou: 0.525 ;time: 0:01:18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-790bde7ac9d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0meditProb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0medit_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         imScale=50)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-2a9744806b26>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trn_data, tst_data, ephocs_num, edit_reward, startA, activation_ephoc, muteB, start_ephoc, dropout_in, dropout_out, dropout_img, dropout_q, editProb, max_activateAProb, activateAProb, addNoise, imScale)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misEdit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0misEdit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimScale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimScale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m                             iou_acc = self.iou_accuracy(\n\u001b[1;32m    811\u001b[0m                                 \u001b[0mtrn_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimScale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimScale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-2a9744806b26>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(self, imScale, data, start, end, sess, feed_dict, isEdit)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_img\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_q\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/asi/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_hidden=200\n",
    "params_dir = params_dir_tmp+'RL/all_hidden:'+str(num_hidden)\n",
    "tf.reset_default_graph()\n",
    "m = Model(\n",
    "    batch_size=200, \n",
    "    num_hidden=num_hidden,\n",
    "    embed_size=embed_vecs.shape[1],\n",
    "    img_dims=trainset[0][0][1][0].shape[1], \n",
    "    bbox_dims=trainset[0][0][1][1].shape[1], \n",
    "    lr=.05,\n",
    "    vocab=vocab, \n",
    "    decay_steps=10000, \n",
    "    decay_rate=0.9, \n",
    "    rnn_editProb=0.2,\n",
    "    coefAlr=1,\n",
    "    bnorm=False,\n",
    "    toQscale=True\n",
    ")\n",
    "\n",
    "\n",
    "# start comp\n",
    "print('params_dir:', params_dir)\n",
    "print('num_hidden:', m.num_hidden)\n",
    "print('learning rate:', m.lr)\n",
    "tst, trn = m.train(trainset, testset,\n",
    "        ephocs_num=200,\n",
    "        start_ephoc=5,\n",
    "        startA=0,\n",
    "        activation_ephoc=7,\n",
    "        muteB=2, \n",
    "        activateAProb=1.,\n",
    "        max_activateAProb=1.,\n",
    "        editProb=0.9,\n",
    "        edit_reward=-0.9,\n",
    "        imScale=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params_dir: ../data/training/models/All/RL/all_hidden:200\n",
      "num_hidden: 200\n",
      "learning rate: 0.05\n",
      "# Train set size: 297\n",
      "# Training batches: 297\n",
      "# Test set size: 297\n",
      "# Testing batches: 297\n",
      "Initializing variables\n",
      "================================================== \n",
      "Train, ephoc: 0 ; Training B: True\n",
      "Edit: False ;A: False ;batch: 0 ;gs: 1 ;lr: 0.0500 ;B loss: 1.537 ;acc: 0.315 ;iou: 0.445 ;time: 0:00:01\n",
      "Edit: True ;A: False ;ephoc: 0 ;batch: 50 ;gs: 51 ;lr: 0.0500 ;B loss: 3.56 ;A loss: 0.67 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.065 ;iou: 0.165 ;time: 0:00:17\n",
      "Edit: True ;A: False ;ephoc: 0 ;batch: 100 ;gs: 101 ;lr: 0.0500 ;B loss: 3.35 ;A loss: 0.84 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.170 ;iou: 0.265 ;time: 0:00:33\n",
      "Edit: False ;A: False ;batch: 150 ;gs: 151 ;lr: 0.0500 ;B loss: 2.365 ;acc: 0.355 ;iou: 0.415 ;time: 0:00:49\n",
      "Edit: True ;A: False ;ephoc: 0 ;batch: 200 ;gs: 201 ;lr: 0.0500 ;B loss: 3.25 ;A loss: 0.63 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.245 ;iou: 0.390 ;time: 0:01:04\n",
      "Edit: True ;A: False ;ephoc: 0 ;batch: 250 ;gs: 251 ;lr: 0.0500 ;B loss: 2.87 ;A loss: 0.42 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.315 ;iou: 0.475 ;time: 0:01:20\n",
      "\n",
      "*Training B: True ;B Train loss: 2.391 ;A Train loss: 0.426 ;Edit num: 0.000 ;Train accuracy: 0.356 ;IOU accuracy: 0.461 ;Time: 0:01:35 \n",
      "\n",
      "Testing, ephoc: 0\n",
      "batch: 0 ;B loss: 1.409 ;acc: 0.685 ;iou_acc: 0.815 ;time: 0:01:36\n",
      "batch: 50 ;B loss: 1.716 ;acc: 0.630 ;iou_acc: 0.725 ;time: 0:01:40\n",
      "batch: 100 ;B loss: 2.151 ;acc: 0.525 ;iou_acc: 0.670 ;time: 0:01:44\n",
      "batch: 150 ;B loss: 2.205 ;acc: 0.515 ;iou_acc: 0.605 ;time: 0:01:50\n",
      "batch: 200 ;B loss: 2.589 ;acc: 0.425 ;iou_acc: 0.540 ;time: 0:01:57\n",
      "batch: 250 ;B loss: 2.996 ;acc: 0.295 ;iou_acc: 0.455 ;time: 0:02:05\n",
      "\n",
      "*BTrain: True ;Test loss: 2.311 ;Test accuracy 0.463 ;IOU accuracy: 0.584 ;Time: 0:02:15\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 1 ; Training B: False\n",
      "Edit: True ;A: False ;ephoc: 1 ;batch: 0 ;B loss: 3.46 ;A loss: 0.40 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.195 ;iou: 0.330 ;time: 0:00:01\n",
      "Edit: True ;A: False ;ephoc: 1 ;batch: 50 ;B loss: 2.47 ;A loss: 0.43 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.415 ;iou: 0.545 ;time: 0:00:14\n",
      "Edit: True ;A: False ;ephoc: 1 ;batch: 100 ;B loss: 1.45 ;A loss: 0.29 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.710 ;iou: 0.815 ;time: 0:00:27\n",
      "Edit: True ;A: False ;ephoc: 1 ;batch: 150 ;B loss: 2.92 ;A loss: 0.35 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.290 ;iou: 0.420 ;time: 0:00:40\n",
      "Edit: True ;A: False ;ephoc: 1 ;batch: 200 ;B loss: 2.26 ;A loss: 0.31 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.475 ;iou: 0.600 ;time: 0:00:54\n",
      "Edit: True ;A: False ;ephoc: 1 ;batch: 250 ;B loss: 2.91 ;A loss: 0.44 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.335 ;iou: 0.465 ;time: 0:01:07\n",
      "\n",
      "*Training B: False ;B Train loss: 2.305 ;A Train loss: 0.279 ;Edit num: 0.000 ;Train accuracy: 0.470 ;IOU accuracy: 0.591 ;Time: 0:01:20 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 2 ; Training B: False\n",
      "Edit: True ;A: False ;ephoc: 2 ;batch: 0 ;B loss: 3.33 ;A loss: 0.29 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.265 ;iou: 0.430 ;time: 0:00:00\n",
      "Edit: True ;A: False ;ephoc: 2 ;batch: 50 ;B loss: 3.43 ;A loss: 0.25 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.170 ;iou: 0.340 ;time: 0:00:14\n",
      "Edit: True ;A: False ;ephoc: 2 ;batch: 100 ;B loss: 1.44 ;A loss: 0.20 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.715 ;iou: 0.780 ;time: 0:00:27\n",
      "Edit: True ;A: False ;ephoc: 2 ;batch: 150 ;B loss: 2.02 ;A loss: 0.11 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.560 ;iou: 0.650 ;time: 0:00:41\n",
      "Edit: True ;A: False ;ephoc: 2 ;batch: 200 ;B loss: 2.56 ;A loss: 0.18 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.350 ;iou: 0.475 ;time: 0:00:54\n",
      "Edit: True ;A: False ;ephoc: 2 ;batch: 250 ;B loss: 1.42 ;A loss: 0.26 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.820 ;iou: 0.885 ;time: 0:01:07\n",
      "\n",
      "*Training B: False ;B Train loss: 2.305 ;A Train loss: 0.223 ;Edit num: 0.000 ;Train accuracy: 0.470 ;IOU accuracy: 0.591 ;Time: 0:01:20 \n",
      "\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 3 ; Training B: True\n",
      "Edit: True ;A: False ;ephoc: 3 ;batch: 0 ;gs: 298 ;lr: 0.0500 ;B loss: 2.52 ;A loss: 0.17 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.430 ;iou: 0.580 ;time: 0:00:01\n",
      "Edit: True ;A: False ;ephoc: 3 ;batch: 50 ;gs: 348 ;lr: 0.0500 ;B loss: 1.46 ;A loss: 0.19 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.780 ;iou: 0.840 ;time: 0:00:17\n",
      "Edit: False ;A: False ;batch: 100 ;gs: 398 ;lr: 0.0500 ;B loss: 2.758 ;acc: 0.380 ;iou: 0.515 ;time: 0:00:34\n",
      "Edit: True ;A: False ;ephoc: 3 ;batch: 150 ;gs: 448 ;lr: 0.0500 ;B loss: 1.57 ;A loss: 0.24 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.640 ;iou: 0.725 ;time: 0:00:50\n",
      "Edit: True ;A: False ;ephoc: 3 ;batch: 200 ;gs: 498 ;lr: 0.0500 ;B loss: 3.05 ;A loss: 0.12 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.320 ;iou: 0.490 ;time: 0:01:06\n",
      "Edit: False ;A: False ;batch: 250 ;gs: 548 ;lr: 0.0500 ;B loss: 1.662 ;acc: 0.615 ;iou: 0.730 ;time: 0:01:23\n",
      "\n",
      "*Training B: True ;B Train loss: 2.129 ;A Train loss: 0.172 ;Edit num: 0.000 ;Train accuracy: 0.529 ;IOU accuracy: 0.646 ;Time: 0:01:39 \n",
      "\n",
      "Testing, ephoc: 3\n",
      "batch: 0 ;B loss: 1.727 ;acc: 0.795 ;iou_acc: 0.895 ;time: 0:01:39\n",
      "batch: 50 ;B loss: 1.341 ;acc: 0.730 ;iou_acc: 0.795 ;time: 0:01:44\n",
      "batch: 100 ;B loss: 1.693 ;acc: 0.650 ;iou_acc: 0.720 ;time: 0:01:49\n",
      "batch: 150 ;B loss: 1.694 ;acc: 0.635 ;iou_acc: 0.720 ;time: 0:01:55\n",
      "batch: 200 ;B loss: 2.082 ;acc: 0.525 ;iou_acc: 0.660 ;time: 0:02:02\n",
      "batch: 250 ;B loss: 2.549 ;acc: 0.400 ;iou_acc: 0.580 ;time: 0:02:11\n",
      "\n",
      "*BTrain: True ;Test loss: 1.959 ;Test accuracy 0.574 ;IOU accuracy: 0.689 ;Time: 0:02:21\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 4 ; Training B: False\n",
      "Edit: True ;A: False ;ephoc: 4 ;batch: 0 ;B loss: 1.42 ;A loss: 0.17 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.640 ;iou: 0.765 ;time: 0:00:01\n",
      "Edit: True ;A: False ;ephoc: 4 ;batch: 50 ;B loss: 1.73 ;A loss: 0.16 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.645 ;iou: 0.705 ;time: 0:00:15\n",
      "Edit: True ;A: False ;ephoc: 4 ;batch: 100 ;B loss: 1.81 ;A loss: 0.13 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.605 ;iou: 0.715 ;time: 0:00:29\n",
      "Edit: True ;A: False ;ephoc: 4 ;batch: 150 ;B loss: 2.27 ;A loss: 0.14 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.435 ;iou: 0.575 ;time: 0:00:41\n",
      "Edit: True ;A: False ;ephoc: 4 ;batch: 200 ;B loss: 2.09 ;A loss: 0.11 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.500 ;iou: 0.630 ;time: 0:00:55\n",
      "Edit: True ;A: False ;ephoc: 4 ;batch: 250 ;B loss: 2.11 ;A loss: 0.12 ;edits: 0.00 ;activateAProb: 0.000 ;acc: 0.600 ;iou: 0.705 ;time: 0:01:09\n",
      "\n",
      "*Training B: False ;B Train loss: 1.952 ;A Train loss: 0.151 ;Edit num: 0.000 ;Train accuracy: 0.579 ;IOU accuracy: 0.693 ;Time: 0:01:23 \n",
      "\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_hidden=200\n",
    "params_dir = params_dir_tmp+'RL/all_hidden:'+str(num_hidden)\n",
    "tf.reset_default_graph()\n",
    "m = Model(\n",
    "    batch_size=200, \n",
    "    num_hidden=num_hidden,\n",
    "    embed_size=embed_vecs.shape[1],\n",
    "    img_dims=trainset[0][0][1][0].shape[1], \n",
    "    bbox_dims=trainset[0][0][1][1].shape[1], \n",
    "    lr=.05,\n",
    "    vocab=vocab, \n",
    "    decay_steps=10000, \n",
    "    decay_rate=0.9, \n",
    "    rnn_editProb=0.,\n",
    "    coefAlr=1,\n",
    "    bnorm=False,\n",
    "    toQscale=True\n",
    ")\n",
    "\n",
    "\n",
    "# start comp\n",
    "print('params_dir:', params_dir)\n",
    "print('num_hidden:', m.num_hidden)\n",
    "print('learning rate:', m.lr)\n",
    "tst, trn = m.train(trainset, testset,\n",
    "        ephocs_num=5,\n",
    "        start_ephoc=0,\n",
    "        startA=0,\n",
    "        activation_ephoc=7,\n",
    "        muteB=2, \n",
    "        activateAProb=0.,\n",
    "        max_activateAProb=0.,\n",
    "        editProb=0.9,\n",
    "        edit_reward=-0.9,\n",
    "        imScale=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
