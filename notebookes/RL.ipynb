{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import retriever\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset_file = '../data/training/order_train_data.bin'\n",
    "testset_file = '../data/training/order_test_data.bin'\n",
    "vocab_file =  '../data/metadata/w2v_vocab.json'\n",
    "params_dir_tmp = '../data/training/models/All/'\n",
    "embed_path =  '../data/metadata/w2v.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Each entry in this list has the following structure:</h3>\n",
    "<ul>\n",
    "<li>entry[0]: query indexes </li>\n",
    "<li>entry[1:n]: n items where each item is [bounding box vector, bounding box spaital features]. Note that different enteries might have different number of possible  bounding boxes (i.e. different n) </li>\n",
    "<li>entry[n+1]: integer, entry[ 1 + entry[n+1]] is the ture bbox </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set batches #: 297\n",
      "Train batch size: 200\n",
      "Train set data size: 59400\n",
      "Test set batches #: 297\n",
      "Test Batch size: 200\n",
      "Test set data size: 59400\n"
     ]
    }
   ],
   "source": [
    "# A data point is taken from the dataset only if the query length is \n",
    "# bigger than zero and it has more than two possible bounding boxes\n",
    "# to choose from\n",
    "\n",
    "trainset = np.load(open(trainset_file, 'rb'))\n",
    "trainset = [item for item in trainset if len(item)>2 and len(item[0])>0]\n",
    "print('Train set batches #:', len(trainset))\n",
    "print('Train batch size:', len(trainset[0]))\n",
    "print('Train set data size:', len(trainset)*len(trainset[0]))\n",
    "\n",
    "testset = np.load(open(testset_file, 'rb'))\n",
    "testset = [item for item in testset if len(item)>2 and len(item[0])>0]\n",
    "print('Test set batches #:', len(trainset))\n",
    "print('Test Batch size:', len(trainset[0]))\n",
    "print('Test set data size:', len(trainset)*len(trainset[0]))\n",
    "\n",
    "\n",
    "with open(vocab_file, 'r') as f:\n",
    "    vocab = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab['<unk>'] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8242, 8241)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), vocab['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# w2c words vectors\n",
    "embed_vecs = np.load(open(embed_path, 'rb')).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> augment_data function </h3>\n",
    "<br>We try sevral regularization methods. One of the things I've tried is to add data points where for each data I pick a query from a random data point and a set of bbox from a different random point. We build the labels (bboxes) distribution by giving an equal probability to each label. <br><br>\n",
    "The augment_data function does just that but the label of each added poind is writen as -1*number of bboxes. When we build the batch it self (function build_data in class Model), if we see a negative label index, we know its an added point and we know the number of bboxes so we can build the correct distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_data(data, ratio=0.5, addNoise=False):\n",
    "        '''\n",
    "        The function add data points. \n",
    "        We pick a query from a random data point,\n",
    "        and a set of bbox from a different random point and we join them\n",
    "        to build a new data point. The label distribution of the new data point will \n",
    "        uniform, that is, all labels will have equal probability. \n",
    "        \n",
    "        \n",
    "        Params:\n",
    "            data: a list of data entries\n",
    "                                                \n",
    "        Returns: a list of augmented data\n",
    "            \n",
    "                        \n",
    "        '''\n",
    "                          \n",
    "        q_idx = np.random.choice(range(len(data)), int(len(data)*ratio), replace=False)\n",
    "        im_idx = np.random.choice([i for i in range(len(data)) if i not in q_idx], int(len(data)*ratio))\n",
    "        for i in range(len(q_idx)):\n",
    "            q, im = data[q_idx[i]][0], data[im_idx[i]][1:-1]\n",
    "            item = [q]\n",
    "            for im_tmp in im:\n",
    "                item.append(im_tmp)\n",
    "            item.append(-len(im))\n",
    "            data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stats(test, train, ephocs=100, title=None, params=[50, 100, 150, 200]):\n",
    "    '''\n",
    "    Plot metrics graphs and print some stats.\n",
    "    \n",
    "    Params:\n",
    "        test: list. \n",
    "              Each item is a tuple, [test accuracy, test IOU, test loss]\n",
    "        \n",
    "        train: list. \n",
    "               Each item is a tuple, [train accuracy, train IOU, train loss, 0]\n",
    "               For now we can ignore the last part in the tuple (zero)\n",
    "               \n",
    "        params: The hyper-parameters to iterate over, defult to number of rnn's hidden units.\n",
    "    '''\n",
    "    \n",
    "    ephocs = range(ephocs)\n",
    "    test_res = np.array(test)\n",
    "    train_res = np.array(train)\n",
    "    test_Glabels = ['test accuracy', 'test IOU', 'test loss']\n",
    "    train_Glabels = ['train accuracy', 'train IOU', 'train loss']\n",
    "\n",
    "    for j, param in enumerate(params):\n",
    "        print('num_hidden:', param)\n",
    "        print('='*(len('num_hidden:')+3))\n",
    "        for i in range(len(train_Glabels)):\n",
    "            plt.plot(ephocs, test_res[j][:,i])\n",
    "            plt.plot(ephocs, train_res[j][:,i])\n",
    "            plt.legend([test_Glabels[i], train_Glabels[i]], loc='upper left')\n",
    "            if title is not None:\n",
    "                plt.title('%s: %d'%(title, param))\n",
    "            plt.show()\n",
    "\n",
    "            metric = ''.join(train_Glabels[i][len('train')+1:])\n",
    "            if metric=='loss':\n",
    "                print('Train min %s:%.3f'%(metric, min(train_res[j][:,i])))\n",
    "                print('Test min %s:%.3f'%(metric, min(test_res[j][:,i])))\n",
    "            else:\n",
    "                print('Train max %s:%.3f'%(metric, max(train_res[j][:,i])))\n",
    "                print('Test max %s:%.3f'%(metric, max(test_res[j][:,i])))\n",
    "        print('-'*100,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALSTM\n",
    "\n",
    "After running B with no edited words we use this RNN cell which has two LSTM cells, Bcell (for player B) and Acell (for Player A). If is Edit is False, we just run B's cell else it works as follow:\n",
    "<ol> \n",
    "<li>We run B's cells with the true query input word, getting the un-edited state. If use_worsAttn=True we add attention on the images vectors for each word.</li>\n",
    "<li>We run B's cells with the edited input word - 'unk', getting the edited state. If use_worsAttn=True we add attention on the images vectors for each word.</li>\n",
    "<li>A uses attention mechanism over all B's outputs on the unedited words (which we got before using this cell).</li>\n",
    "<li>A's input are:\n",
    "<ul><li>B's unedited state</li><li>The reward for editing a word</li><li>B's loss having no edited words.</li></ul></li>\n",
    "<br><li>A's output is then goes throw a transformation which yields two values, one for editing a word and another for not:\n",
    "<ul>\n",
    "    <li>With probability of editRandomlyProb we ignore A and decide whether to edit or not with probability of rnn_editProb</li>\n",
    "    <li>With probability of 1-editRandomlyProb A chooses an action: edit only if the value for edit the word is higher (i.e. we pass B's edited state to the next time step), else we pass B's unedited state.</li></ul>.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ALSTM(tf.nn.rnn_cell.LSTMCell):\n",
    "    def __init__(self, \n",
    "                 batch_size, \n",
    "                 num_units, \n",
    "                 \n",
    "                 # Size of A's attention vector.\n",
    "                 words_attn_dim, \n",
    "                 # B's outputs (for each time step).\n",
    "                 words_attn_states, \n",
    "                 # Inicates whether attentionvec is a padding (0) or not (1).\n",
    "                 words_attn_idx, \n",
    "                 \n",
    "                 # Size of B's attention vector ([image vector, spital features] size) .\n",
    "                 img_attn_dim, \n",
    "                 # B's outputs (for each time step).\n",
    "                 img_attn_states, \n",
    "                 \n",
    "                 \n",
    "                 # Inicates whether attentionvec is a padding (0) or not (1).\n",
    "                 img_attn_idx, \n",
    "                 \n",
    "                 unk, #'unk' word vector\n",
    "                 \n",
    "                 # Probabilty for choosing an action (edit or not) randomly, \n",
    "                 editRandomlyProb, \n",
    "                 # Probabilty for edit a word in rnn, when decision are taken randomly.\n",
    "                 rnn_editProb, \n",
    "                 \n",
    "                 \n",
    "                 # Whehter to edit the query or not.\n",
    "                 isEdit, \n",
    "                 \n",
    "                 # Whether B uses words levlel attention or not.\n",
    "                 use_wordAttn,\n",
    "                 \n",
    "                 # If isIDX is True then use the decisions in set_actions.\n",
    "                 isIDX,\n",
    "                 set_actions=None,\n",
    "                 \n",
    "                 # If True add noise instead of using 'unk'\n",
    "                 useNoise=False,\n",
    "                 \n",
    "                 # If useNoise is true word vec = alpha*word_vector+(1-alpha)*noise\n",
    "                 alpha=.3,\n",
    "                 \n",
    "                 # Dropout ratio for rnn's inputs and outpouts\n",
    "                 dropout_in=1.,\n",
    "                 dropout_out=1.,\n",
    "                 \n",
    "                 #this holds A's rewards and B's losses to\n",
    "                 # be add to A's feature vectors.\n",
    "                 reward_loss=None,\n",
    "                 state_is_tuple=True,):\n",
    "        \n",
    "        \n",
    "        # When useing A, the cell state will contain the concatenation \n",
    "        # of both B and A states. Therefore we set the unit number to be\n",
    "        # 2*(A and B unit size).\n",
    "        super().__init__(2*num_units+1, state_is_tuple=state_is_tuple)\n",
    "    \n",
    "        self.words_attn_states = words_attn_states\n",
    "        self.words_attn_idx = words_attn_idx\n",
    "        self.words_attn_dim = words_attn_dim\n",
    "        \n",
    "        self.img_attn_states = img_attn_states\n",
    "        self.img_attn_idx = img_attn_idx\n",
    "        self.img_attn_dim = img_attn_dim\n",
    "        \n",
    "        self.num_units = num_units\n",
    "        self.batch_size = batch_size\n",
    "        self.unk = unk \n",
    "        self.isEdit = isEdit\n",
    "        self.use_wordAttn=use_wordAttn\n",
    "        \n",
    "        # To avoide using dropout twice on the same input, if isEdit is true\n",
    "        # Bcell will only use dropout for its inputs. We'll add dropout to B's outputs\n",
    "        # only after A's action was taken.\n",
    "        self.Acell = tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.nn.rnn_cell.LSTMCell(self.num_units, state_is_tuple=True), \n",
    "            input_keep_prob=dropout_in, \n",
    "            output_keep_prob=dropout_out\n",
    "        )\n",
    "      \n",
    "        self.Bcell_tmp = tf.nn.rnn_cell.LSTMCell(self.num_units, state_is_tuple=True)\n",
    "            \n",
    "        \n",
    "        self.rnn_editProb = rnn_editProb\n",
    "        self.reward_loss = reward_loss\n",
    "        self.useNoise=useNoise\n",
    "        self.alpha=alpha\n",
    "        \n",
    "        self.isIDX=isIDX\n",
    "        self.set_actions=set_actions\n",
    "        self.editRandomlyProb = editRandomlyProb\n",
    "        self.dropout_in = dropout_in\n",
    "        self.dropout_out = dropout_out\n",
    "        \n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        '''\n",
    "        Params:\n",
    "            inputs: word embadding.\n",
    "            state:  [B's state form privious state, A's state form privious state]\n",
    "        '''\n",
    "        # takse B's state from state[:self.num_units]\n",
    "        Bstate_c = tf.slice(state[0], [0, 0], [-1, self.num_units])\n",
    "        Bstate_h = tf.slice(state[1], [0, 0], [-1, self.num_units])\n",
    "        self.Bstate =  tf.nn.rnn_cell.LSTMStateTuple(c=Bstate_c, h=Bstate_h)\n",
    "\n",
    "        \n",
    "        # If isEdit==True \n",
    "        def f1(): \n",
    "            # If B's cell uses attention\n",
    "            self.Bcell = tf.contrib.rnn.DropoutWrapper(\n",
    "                    self.Bcell_tmp, input_keep_prob=self.dropout_in)\n",
    "            \n",
    "            # If B uses words level attention over BBOXes.\n",
    "            if self.use_wordAttn:\n",
    "                words_attn = self.attention(Bstate_h, self.img_attn_states, self.img_attn_dim, self.img_attn_idx)\n",
    "                new_input = tf.concat([inputs, words_attn], -1)\n",
    "                Boutputs, Bnew_state =  self.Bcell(new_input, self.Bstate, 'Bcell')\n",
    "            else:\n",
    "                Boutputs, Bnew_state =  self.Bcell(inputs, self.Bstate, 'Bcell')\n",
    "            \n",
    "            # takse A's state from state[self.num_units: 2*self.num_units]\n",
    "            Astate_c = tf.slice(state[0], [0, self.num_units], [-1, self.num_units])\n",
    "            Astate_h = tf.slice(state[1], [0, self.num_units], [-1, self.num_units])\n",
    "            self.Astate =  tf.nn.rnn_cell.LSTMStateTuple(c=Astate_c, h=Astate_h)\n",
    "            \n",
    "            #Current time step is in the state[-1]\n",
    "            self.timesteps = tf.slice(state[1], [0, 2*self.num_units], [-1, 1])\n",
    "            self.timestep = tf.cast(self.timesteps[0][0], tf.int32)\n",
    "            \n",
    "            if self.useNoise: # just add noise to the edited words\n",
    "                new_unk_vecs = self.alpha*inputs + (1-alpha)*tf.random_normal(shape=inputs.get_shape(), stddev=0.1)\n",
    "            else: # change the edited words by 'unk'\n",
    "                unk_vecs = tf.concat([self.unk for _ in range(self.batch_size)], 0) # shape: self.batch_size x 1 x embed_size\n",
    "                new_unk_vecs = tf.squeeze(unk_vecs) # shape: self.batch_size x embed_size\n",
    "            \n",
    "            # run B's cell with unk_batch\n",
    "            if self.use_wordAttn:\n",
    "                new_unk = tf.concat([new_unk_vecs, words_attn], -1)\n",
    "            else:\n",
    "                new_unk = new_unk_vecs\n",
    "            edit_output, edit_new_state = self.Bcell(new_unk, self.Bstate, 'Bcell')  \n",
    "            out1, state1 = self.runCell(Boutputs, Bnew_state, edit_new_state)\n",
    "            return out1, state1\n",
    "        \n",
    "        def f2(): #If we don't edit\n",
    "            # If B's cell uses attention\n",
    "            self.Bcell = tf.contrib.rnn.DropoutWrapper(\n",
    "                    self.Bcell_tmp, output_keep_prob=self.dropout_out, input_keep_prob=self.dropout_in)\n",
    "            \n",
    "            if self.use_wordAttn:\n",
    "                words_attn = self.attention(Bstate_h, self.img_attn_states, self.img_attn_dim, self.img_attn_idx)\n",
    "                new_input = tf.concat([inputs, words_attn], -1)\n",
    "                Boutputs, Bnew_state =  self.Bcell(new_input, self.Bstate, 'Bcell')\n",
    "            else:\n",
    "                Boutputs, Bnew_state =  self.Bcell(inputs, self.Bstate, 'Bcell')\n",
    "                \n",
    "            outs = tf.concat([Boutputs, tf.zeros((self.batch_size, self.num_units+1))], 1)\n",
    "            new_state_c = tf.concat([Bnew_state[0], tf.zeros((self.batch_size, self.num_units+1))], 1)\n",
    "            new_state_h = tf.concat([Bnew_state[1], tf.zeros((self.batch_size, self.num_units+1))], 1)\n",
    "            return outs, tf.nn.rnn_cell.LSTMStateTuple(c=new_state_c, h=new_state_h)\n",
    "        \n",
    "        new_output, new_state = tf.cond(self.isEdit, f1, f2)\n",
    "        \n",
    "        return new_output, new_state\n",
    "    \n",
    "    def runCell(self, Boutputs, Bnew_state, edit_new_state):\n",
    "        '''\n",
    "        Run B's cell after editing.\n",
    "        \n",
    "        params:\n",
    "            Boutputs: output vector without editing.\n",
    "            Bnew_state: state vector without editing.\n",
    "            edit_new_state: state vector after editing the input word to 'unk'. \n",
    "        '''\n",
    "        \n",
    "        with tf.variable_scope('runcell'):\n",
    "            # get action values according to B's hidden state\n",
    "            Aout, Anew_state, actions_vals = self.action_vals(Boutputs) \n",
    "            \n",
    "            def g1():\n",
    "                '''\n",
    "                If isIDX use the decisions in self.action_idx\n",
    "                '''\n",
    "                return tf.slice(self.set_actions, [0,self.timestep], [-1, 1])\n",
    "            \n",
    "            def g2():\n",
    "                # A choose to edit a word if actions_vals[0]<actions_vals[1].\n",
    "                # Note: if we edit the word cond=1, else cond=0.\n",
    "                a1, a2 = tf.split(value=actions_vals, num_or_size_splits=2, axis=1)\n",
    "                A_choice = tf.cast(tf.less(a1, a2), tf.float32)\n",
    "\n",
    "                # If action is chosen randomly, edit word with probability self.rnn_editProb\n",
    "                # Note: if we edit the word cond=1, else cond=0.\n",
    "                rand = tf.multinomial(tf.log([[self.rnn_editProb, 1-self.rnn_editProb]]), self.batch_size)\n",
    "                rand_choice = tf.cast(tf.less(tf.transpose(rand), 1), tf.float32) \n",
    "\n",
    "                # Choose whether to edit a word randomly with probability of editRandomlyProb\n",
    "                editRandomly = tf.multinomial(tf.log([[self.editRandomlyProb, 1-self.editRandomlyProb]]), self.batch_size)\n",
    "                editRandomly_choice = tf.cast(tf.less(tf.transpose(editRandomly), 1), tf.float32)\n",
    "\n",
    "                # A list of decisions for each word in the batch. actions_idx[i] = 1->edit word in query i (at this time step), 0->do not edit.\n",
    "                return editRandomly_choice*rand_choice + (1-editRandomly_choice)*A_choice\n",
    "\n",
    "            \n",
    "            # a list of A's decisions for each batch.  1->edit, 0-> do not edit.\n",
    "            cond = tf.cast(tf.cond(self.isIDX, g1, g2), tf.float32)\n",
    "\n",
    "            # We'd like to know the action values and decision for each word,\n",
    "            # therefore theses info are placed on the first 3 dimensions of the \n",
    "            # output vector. Note that this vector is not passed to the \n",
    "            # next tiee step so it won't affect the model. \n",
    "            outs = tf.concat([actions_vals, cond], 1)\n",
    "\n",
    "            # B's i state is replaced by the edited state if cond[i]=1 (i =1, 2, ..., batch_size)\n",
    "            new_edit_state_c = (1-cond)*Bnew_state[0] + cond*edit_new_state[0]\n",
    "            new_edit_state_h = (1-cond)*Bnew_state[1] + cond*edit_new_state[1]\n",
    "\n",
    "\n",
    "            new_outs = tf.concat([outs, tf.zeros((self.batch_size, 2*self.num_units-3)), self.timesteps+1], 1)\n",
    "            new_state_c = tf.concat([new_edit_state_c, Anew_state[0], self.timesteps+1], 1)\n",
    "            new_state_h = tf.concat([new_edit_state_h, Anew_state[1], self.timesteps+1], 1)\n",
    "            return new_outs, tf.nn.rnn_cell.LSTMStateTuple(c=new_state_c, h=new_state_h)\n",
    "    \n",
    "    \n",
    "    def action_vals(self, Boutputs):\n",
    "        '''\n",
    "        Get values for editing/not editing the input word.\n",
    "        \n",
    "        Params:\n",
    "            Boutputs: output vector without editing.\n",
    "            \n",
    "        Returns vals where:\n",
    "            Aout: A's cell output\n",
    "            Anew_state: A's cell state\n",
    "            vals: Tensor where vals[0] is the value for not editing the word \n",
    "                    and vals[1] is the value for editing the word.\n",
    "        '''\n",
    "        \n",
    "        with tf.variable_scope('action_vals') as scope:\n",
    "            Aattn = self.attention(self.Astate[1], self.words_attn_states, self.words_attn_dim, self.words_attn_idx)\n",
    "            \n",
    "            # A's input: [input, B's output, attntion state, reward, B's loss with no edits]\n",
    "            Anew_inputs = tf.concat([Boutputs, Aattn, self.reward_loss], 1)\n",
    "            \n",
    "            Aout, Anew_state = self.Acell(Anew_inputs, self.Astate, 'Acell')\n",
    "            vals = tf.nn.relu(self.linear(Aout, 2))\n",
    "\n",
    "            # Save the variables that only A uses. \n",
    "            # These variables will be trained separately from B's model.\n",
    "            self.Avars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)\n",
    "            \n",
    "            return Aout, Anew_state, vals\n",
    "            \n",
    "        \n",
    "    def linear(self, inputs, output_dim, scope='linear', bias=True, reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=False):\n",
    "            W = tf.get_variable('W', initializer=tf.random_uniform_initializer(maxval=1., minval=-1.),\n",
    "                                shape=(inputs.get_shape()[-1], output_dim))\n",
    "            if bias:\n",
    "                b = tf.get_variable('b', initializer=tf.constant_initializer(0.1),\n",
    "                               shape=[1, output_dim])\n",
    "                return tf.matmul(inputs, W) + b\n",
    "\n",
    "        return tf.matmul(inputs, W)\n",
    "    \n",
    "    \n",
    "    def attention(self, state, attn_states, attn_dim, attn_idx, relu=False):\n",
    "        '''\n",
    "        Attention mechanism (see https://arxiv.org/pdf/1409.0473.pdf)\n",
    "        \n",
    "        state: State from previous time step.\n",
    "        attn_states: Attetntion states. \n",
    "                     Tensor of shape (batch_size x max([len(attention_vectors[i]) for i in range(bach_size)]) x attn_dim)\n",
    "        attn_dim: Attention vector size.\n",
    "        attn_idx,: Tensor used for masking of shape (batch_size x max([len(attention_vectors[i]) for i in range(bach_size)]). \n",
    "                   attn_idx[i, j]=1 if the j's attention vcctior of sample i  is not padding, else its equat to 0.\n",
    "        '''\n",
    "        \n",
    "        self.attn_length = tf.shape(attn_states)[1]  \n",
    "        \n",
    "        # Computing... hidden_attn = W*v_att (use tf.nn.conv2d for efficiency)\n",
    "        attn_vecs = tf.reshape(attn_states, [self.batch_size, self.attn_length, 1, attn_dim])\n",
    "        W = tf.get_variable(\"attn_W\", [1, 1, attn_dim, self.num_units])\n",
    "        hidden_attn = tf.nn.conv2d(attn_vecs, W, [1, 1, 1, 1], \"SAME\")\n",
    "\n",
    "        # Computing... hidden_s = U*v_state\n",
    "        hidden_s = tf.reshape(\n",
    "            self.linear(\n",
    "                tf.cast(state, tf.float32), output_dim=self.num_units, scope='hidden_s_linear'), [-1, 1, 1,  self.num_units], name='hidden_s')\n",
    "\n",
    "        # Computing alpha\n",
    "        v = tf.get_variable(\"attn_v\", [self.num_units])\n",
    "        if relu:\n",
    "            logits = tf.reduce_sum(v * tf.nn.relu(hidden_attn + hidden_s), [2, 3])\n",
    "        else:\n",
    "            logits = tf.reduce_sum(v * tf.nn.tanh(hidden_attn + hidden_s), [2, 3])\n",
    "\n",
    "        # Masked softmax\n",
    "        max_logits = tf.reduce_max(logits, axis=-1)\n",
    "        masked_logits = tf.exp(logits-tf.expand_dims(max_logits, axis=1))*attn_idx\n",
    "        alpha = masked_logits/tf.reduce_sum(masked_logits, axis=-1, keep_dims=True)\n",
    "\n",
    "        a = tf.reduce_sum(tf.reshape(alpha, [-1, self.attn_length, 1, 1]) * attn_vecs, [1, 2])\n",
    "        b = tf.contrib.layers.fully_connected(a, num_outputs=self.num_units)\n",
    "                                               \n",
    "\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "When A joins the game, each iteration is completed via three steps:\n",
    "<ul>\n",
    "<li>We feed the query as it is to B and run it alone (no optimization nor training is done at this step)</li>\n",
    "<li>We calculate B's loss per query and use it to calculate the reward for query i as follow: <br> <br><b>edit_reard*Bloss_i/length_of_query_i</b><br>\n",
    "<li>We run A and B together (no optimization nor training is done at this step). At this step we get A's decition and B's loss on the edited query. We use B's loss to calculate the Bellman's value for each time step. </li>\n",
    "<li>We again run A and B together, forcing A to make the same actions it did as in the previous run. Since now we know the real value, action and reward for each time step (these will be the same as in the previous step since we did not train optimize the parameters yet), we finaly train the model</li>\n",
    "</ul>\n",
    "<br>\n",
    "But first we check the model's performance with out A. Class Model has a set of conditioning variables that set a different ragularization methods in the model. We start by checking them with out A's interference. We can also make the model RNN become bidirectional (by setting useBidirectionalRnn to True) and use words level attention (by setting use_wordAttn to True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self,\n",
    "                 batch_size, \n",
    "                 num_hidden, \n",
    "                 \n",
    "                 #Image's vector size.\n",
    "                 img_dims, \n",
    "                 \n",
    "                 #Spaital features length.\n",
    "                 bbox_dims, \n",
    "                 vocab, \n",
    "                 lr, #  B's learning rate.\n",
    "                 decay_steps, \n",
    "                 decay_rate, \n",
    "                 \n",
    "                 # A's leanring rate = B's learning rate x coefAlr.\n",
    "                 coefAlr,\n",
    "                 \n",
    "                 # whether to use bach normaliztion for the last attention layer\n",
    "                 bnorm,\n",
    "                 embed_size=embed_vecs.shape[1],\n",
    "                 \n",
    "                 # Whether B uses words levlel attention or not.\n",
    "                 use_wordAttn=False,\n",
    "                 \n",
    "                 # Whther to use bidirectional rnn\n",
    "                 useBidirectionalRnn=False,\n",
    "                 \n",
    "                 # Urnn_norm: Whether to use batch normalization for the queries.\n",
    "                 # Uatt_norm: Whether to use batch normalization for the VGG outputs.\n",
    "                 Urnn_norm=True, \n",
    "                 Uatt_norm=True,\n",
    "                 \n",
    "                 # If useNoise, A add Normal noise to a word \n",
    "                 # instead of editing it: word_vec = alpha*word_vec+(1-alpha)*noise\n",
    "                 useNoise=False,\n",
    "                 alpha=.5,\n",
    "                \n",
    "                 # Whether to scale and normelize queries \n",
    "                 # embedding to have zero mean and QSTD std\n",
    "                 toQscale = False,\n",
    "                 Qstd = 0.1):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.img_dims = img_dims\n",
    "        self.bbox_dims = bbox_dims \n",
    "        self.num_hidden = num_hidden\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab = vocab\n",
    "        self.lr = lr\n",
    "        self.toQscale=toQscale\n",
    "        self.Qstd=Qstd\n",
    "\n",
    "        # Probabilty for choosing an action (edit or not) randomly, \n",
    "        self.editRandomlyProb = tf.placeholder(tf.float32, name='editRandomlyProb_holder')\n",
    "        # If the action is chosen randomly. edit with probability of rnn_editProb\n",
    "        self.rnn_editProb = tf.placeholder(tf.float32, name='rnn_editProb_holder')\n",
    "\n",
    "        self.queries = tf.placeholder(tf.int32, [None, None], name='queries')\n",
    "        self.img  = tf.placeholder(tf.float32, [None, None, self.img_dims], name='img')# VGG output vectors\n",
    "        self.bboxes = tf.placeholder(tf.float32, [None, None, self.bbox_dims], name='bboxes')# spatial bbox's features.\n",
    "\n",
    "        # attn_idx: inicates whether attention box is a dummy (0) or not (1).\n",
    "        self.attn_idx = tf.placeholder(tf.float32, [None, None], name='attn_idx')\n",
    "        \n",
    "        # If isIDX is True then use the decisions in set_actions instead of A's decisions.\n",
    "        # We use it to force A to take the decitions it made while we calculated the bellman values.\n",
    "        self.isIDX = tf.placeholder(tf.bool, name='isIDX_holder')\n",
    "        self.set_actions = tf.placeholder(tf.float32, [None, None], name='actions_idx_holder')\n",
    "\n",
    "        self.labels = tf.placeholder(tf.float32, [None, None], name='labels')\n",
    "        self.isEdit = tf.placeholder(tf.bool, name='isEdit') # whehter to edit the query or not.\n",
    "\n",
    "\n",
    "        # this holds A's rewards and B losses per query, to be add to A's feature vectors.\n",
    "        self.reward_loss = tf.placeholder(tf.float32, [None,2], name='rewards_loss')\n",
    "\n",
    "        # Dropout ratio for rnn's inputs and outpouts\n",
    "        self.dropout_in = tf.placeholder(tf.float32, name='dropoutIn_holder')\n",
    "        self.dropout_out = tf.placeholder(tf.float32, name='dropoutOut_holder')\n",
    "\n",
    "        # Dropout ratio for attention vector (for the final attention layer before the loss function)\n",
    "        self.dropout_img = tf.placeholder(tf.float32, name='dropoutImg_holder')\n",
    "        # Dropout ratio for query vector (for the final attention layer before the loss function)\n",
    "        self.dropout_q = tf.placeholder(tf.float32, name='dropoutImg_holder')\n",
    "\n",
    "        # B outputs vectors (with no words edits), These are A's attention vectors\n",
    "        # which it uses to decide whter to edit a word.\n",
    "        self.Aattn_vecs = tf.placeholder(tf.float32, [None, None, None], name='Aattn_vecs_holder')    \n",
    "        self.unk = tf.constant([[vocab['<unk>']]], tf.int32)\n",
    "\n",
    "        self.isTrain = tf.placeholder(tf.bool, name='isTrain_holder') \n",
    "        self.queries_lens = self.length(self.queries) # list of all the lengths of the batch's queriey \n",
    "\n",
    "        # Concatinate images vectors and their spaital features. \n",
    "        # These vectors wlll be used for attenionn when \n",
    "        # we calculate the loss function.\n",
    "        attn_vecs = tf.concat([self.img, self.bboxes], 2) \n",
    "        voc_size = len(self.vocab)\n",
    "\n",
    "        # Load pre-trained word imaddings.\n",
    "        # w2v_embed is not trainable.\n",
    "        with tf.variable_scope('w2v'):\n",
    "            w2v_embed = tf.get_variable('w2v_embed', initializer=embed_vecs, trainable=False)\n",
    "            w2v_queries = tf.nn.embedding_lookup(w2v_embed, self.queries, name='w2v_queries')\n",
    "\n",
    "        with tf.variable_scope('embed'):\n",
    "            embed = tf.get_variable('embed', shape=[voc_size, self.embed_size], \n",
    "                                    initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1))\n",
    "            embed_queries_tmp = tf.nn.embedding_lookup(embed, self.queries, name='embed_queries')\n",
    "\n",
    "        embed_queries = embed_queries_tmp+w2v_queries\n",
    "\n",
    "        with tf.variable_scope('rnn'):\n",
    "            Aattn_idx = tf.cast(tf.abs(tf.sign(self.queries)), tf.float32)\n",
    "\n",
    "\n",
    "            cell = ALSTM(num_units=self.num_hidden, \n",
    "                            words_attn_dim=self.num_hidden, \n",
    "                            words_attn_states=self.Aattn_vecs, \n",
    "                            words_attn_idx=Aattn_idx,\n",
    "                            img_attn_dim=self.img_dims+self.bbox_dims,\n",
    "                            img_attn_states=attn_vecs,\n",
    "                            img_attn_idx=self.attn_idx,\n",
    "                            batch_size=self.batch_size, \n",
    "                            unk=tf.nn.embedding_lookup(embed, self.unk), rnn_editProb=self.rnn_editProb,\n",
    "                            isEdit=self.isEdit, dropout_in=self.dropout_in, dropout_out=self.dropout_out,\n",
    "                            reward_loss=self.reward_loss, use_wordAttn=use_wordAttn,\n",
    "                            isIDX=self.isIDX, set_actions=self.set_actions, editRandomlyProb=self.editRandomlyProb,\n",
    "                            useNoise=useNoise, alpha=alpha)\n",
    "\n",
    "            if useBidirectionalRnn:\n",
    "                self.outputs, self.last_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    cell_fw=cell,\n",
    "                    cell_bw=cell,\n",
    "                    dtype=tf.float32,\n",
    "                    sequence_length=self.queries_lens,\n",
    "                    inputs=embed_queries)\n",
    "\n",
    "                # self.last_states contain both forward state\n",
    "                # and backward state.\n",
    "                # We don't use A with bidirectional rnn\n",
    "                # so no need for self.values (action values as calculated by A)\n",
    "                Bstate = tf.concat(\n",
    "                    [tf.slice(self.last_states[0][1], [0,0], [-1, self.num_hidden]), \n",
    "                     tf.slice(self.last_states[1][1], [0,0], [-1, self.num_hidden])], -1)\n",
    "            else:\n",
    "                self.outputs, self.last_states = tf.nn.dynamic_rnn(\n",
    "                    cell=cell,\n",
    "                    dtype=tf.float32,\n",
    "                    sequence_length=self.queries_lens,\n",
    "                    inputs=embed_queries)\n",
    "\n",
    "                # self.values[0]=value for not editing, self.values[1]=value for editing\n",
    "                self.values = tf.slice(self.outputs, [0,0,0], [-1,-1,2])\n",
    "                Bstate = tf.slice(self.last_states[1], [0,0], [-1, self.num_hidden])  \n",
    "\n",
    "\n",
    "        # The variable A uses\n",
    "        Avars = cell.Avars\n",
    "\n",
    "        if bnorm: # If using batch normalization \n",
    "            self.scores = self.bnorm_attention(Bstate, Urnn_norm=Urnn_norm, Uatt_norm=Uatt_norm) \n",
    "        else:\n",
    "            self.scores = self.attention(Bstate) \n",
    "\n",
    "\n",
    "        # Cross entophy loss for each of the queries in the batch.\n",
    "        self.B_ce = -tf.reduce_sum(\n",
    "                        self.labels*tf.log(self.scores+0.00000001)+\n",
    "                            (1-self.labels)*tf.log((1-self.scores)+0.00000001), \n",
    "                            axis=-1)\n",
    "\n",
    "\n",
    "        # We don't use A with bidirectional rnn\n",
    "        if not useBidirectionalRnn:\n",
    "            # A's decision for each word.\n",
    "            self.idx = tf.squeeze(tf.slice(self.outputs, [0,0,2], [-1,-1,1]))\n",
    "\n",
    "            self.edit_num = tf.reduce_mean(tf.reduce_sum(\n",
    "                tf.cast(self.idx, tf.float32)*tf.expand_dims(\n",
    "                        1/tf.cast(self.queries_lens, tf.float32), axis=1), axis=1))\n",
    "\n",
    "            # After running A for the first time, we get A's decisions and their values.\n",
    "            # we then calulate the following tensors:\n",
    "            # actions_idx[j,i] = 1 if the word i in query j was edited or 0 otherwise.     \n",
    "            # bell_vall holds the values for each decision by the bellman function. \n",
    "\n",
    "            # self.actions_idx will be used to get the values of the action that were taken by A.                 \n",
    "            self.actions_idx = tf.placeholder(shape=[None, None], dtype=tf.int32, name=\"actions_idx\")\n",
    "            self.bell_val = tf.placeholder(shape=[None, None], dtype=tf.float32, name=\"bell_val\")\n",
    "\n",
    "            # pyrite_val: holds the values for each of A's decisions, claculated by A. \n",
    "            # see https://en.wikipedia.org/wiki/Pyrite.\n",
    "            self.pyrite_val = tf.reshape(tf.gather_nd(tf.reshape(self.values, (-1,2)), self.actions_idx), (self.batch_size, -1))\n",
    "\n",
    "            # RMSE loss (adding smoothing factor so the gradient won't divide by zero)\n",
    "            self.A_loss = tf.reduce_mean(tf.reduce_sum(\n",
    "                tf.sqrt(tf.square((self.bell_val-self.pyrite_val+0.000001)*tf.cast(\n",
    "                    tf.sign(tf.abs(self.queries)), tf.float32)))/tf.expand_dims(\n",
    "                                                    tf.cast(self.queries_lens, tf.float32), axis=1), axis=-1))\n",
    "\n",
    "        self.B_loss = tf.reduce_mean(self.B_ce)\n",
    "\n",
    "        ##############\n",
    "        # Optimizers #\n",
    "        ##############\n",
    "\n",
    "        starter_learning_rate = self.lr\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                       decay_steps=decay_steps, decay_rate=decay_rate, staircase=True)\n",
    "\n",
    "        if not useBidirectionalRnn:\n",
    "            # Train only A variables \n",
    "            self.A_optimizer =  tf.train.GradientDescentOptimizer(\n",
    "                    learning_rate=coefAlr*self.learning_rate).minimize(self.A_loss, var_list=Avars)  \n",
    "\n",
    "        # Train only B variables \n",
    "        Bvars = [var for var in tf.trainable_variables() if var not in Avars]\n",
    "        self.B_optimizer =  tf.train.GradientDescentOptimizer(\n",
    "                    learning_rate=self.learning_rate).minimize(self.B_loss, global_step=self.global_step, var_list=Bvars)  \n",
    "\n",
    "        if not os.path.exists(params_dir):\n",
    "                os.makedirs(params_dir)\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "    def length(self, seq):\n",
    "        '''\n",
    "        Retruns real lengths (before addings) of all queries in seq  .\n",
    "        '''\n",
    "        return tf.cast(tf.reduce_sum(tf.sign(tf.abs(seq)), reduction_indices=1), tf.int32)\n",
    "       \n",
    "\n",
    "    def linear(self, inputs, output_dim, scope='linear', bias=True, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            W = tf.get_variable('W', initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                shape=(inputs.get_shape()[-1], output_dim))\n",
    "            if bias:\n",
    "                b = tf.get_variable('b', initializer=tf.constant_initializer(0.1),\n",
    "                               shape=[1, output_dim])\n",
    "                return tf.matmul(inputs, W) + b\n",
    "            \n",
    "            return tf.matmul(inputs, W)\n",
    "\n",
    "            \n",
    "    def Qscale(self, qVecs):\n",
    "        '''\n",
    "        Scale queries embedding vectors to have zero mean and std STD.\n",
    "        \n",
    "        Params:\n",
    "            qVecs: Tensor (shape: batch_size x number_of_hidden_units) \n",
    "                   holding the queries vectors, \n",
    "        '''\n",
    "        qVecs_min = tf.reduce_min(qVecs, axis=-1, keep_dims=True)\n",
    "        qVecs_max = tf.reduce_max(qVecs, axis=-1, keep_dims=True)\n",
    "        qVecs_scale = (qVecs-qVecs_min)/(qVecs_max-qVecs_min) # Scale to 0-1\n",
    "        s = tf.contrib.keras.backend.std(qVecs_scale, axis=-1, keepdims=True)\n",
    "        tmp = self.Qstd * qVecs_scale/s #Scale to have self.Qstd std\n",
    "        new_qVecs = tmp - tf.reduce_mean(tmp, axis=-1, keep_dims=True) # zero mean\n",
    "        \n",
    "        return new_qVecs\n",
    "    \n",
    "    def attention(self, q_embed):\n",
    "        '''\n",
    "        Given B's output vector, calculate the attention over \n",
    "        all the query's bounding boxes vectors, That is, calculate:\n",
    "        \n",
    "        probs = softmax(relu(context(Sq+Satt+b)))\n",
    "        \n",
    "        Where:\n",
    "        Sq = <Wq, queries_states>\n",
    "        Sattn = <Wattn, attention_bboxes_vectors>\n",
    "        \n",
    "        The  bounding box with the highest attention score will be chosen as the correct bounding box.\n",
    "        \n",
    "        Params:\n",
    "            q_embed: Tensor of shape (batch size x num_hidden)B's outputs. \n",
    "            \n",
    "        Returns:\n",
    "            probs: Tensor of shape (batch_size x max bbox number for query).\n",
    "                   Attention score for each bbox.\n",
    "        '''\n",
    "        # concatenate img vectors with spaical features\n",
    "        attn_vecs = tf.concat([self.img, self.bboxes], 2)\n",
    "        \n",
    "        # B's outputs, shape: (batch size x num_hidden)\n",
    "        if self.toQscale:\n",
    "            Urnn = self.Qscale(q_embed)\n",
    "        else:\n",
    "            Urnn = q_embed\n",
    "        \n",
    "        # Attention vectors, \n",
    "        # shape: (batch size x max bbox number for query x attention vector size)\n",
    "        Uatt = attn_vecs\n",
    "           \n",
    "        with tf.variable_scope('l1'):\n",
    "            b = tf.get_variable(\n",
    "                    'b', \n",
    "                    initializer=tf.constant_initializer(0.1), \n",
    "                    shape=[1, self.num_hidden])\n",
    "\n",
    "            context = tf.get_variable(\n",
    "                    'context', \n",
    "                    initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1), \n",
    "                    shape=[self.num_hidden, 1])\n",
    "\n",
    "\n",
    "            Sq = tf.nn.dropout(\n",
    "                self.linear(Urnn, self.num_hidden, bias=False, scope='Sq'), \n",
    "                self.dropout_q)\n",
    "            \n",
    "            Sattn = tf.nn.dropout(\n",
    "                tf.reshape(\n",
    "                    self.linear(\n",
    "                        tf.reshape(Uatt, (-1, self.img_dims+self.bbox_dims)), \n",
    "                        self.num_hidden, \n",
    "                        bias=False, scope='Sattn'), \n",
    "                    [self.batch_size, -1, self.num_hidden]),\n",
    "                self.dropout_img)\n",
    "\n",
    "        out = tf.nn.relu(tf.expand_dims(Sq, 1) + Sattn + b)\n",
    "        logits = tf.reshape(tf.matmul(tf.reshape(out, (-1, tf.shape(out)[-1])),  context), (tf.shape(out)[0], -1))\n",
    "\n",
    "        # Calculate logits's masked softmax (we use self.attn_idx to mas\n",
    "        max_logits = tf.reduce_max(logits, axis=-1)\n",
    "        masked_logits = tf.exp(logits-tf.expand_dims(max_logits, axis=1))*self.attn_idx\n",
    "        probs = self.attn_idx*masked_logits/tf.reduce_sum(masked_logits, axis=-1, keep_dims=True)\n",
    "\n",
    "        return probs\n",
    "\n",
    "    \n",
    "    def bnorm_attention(self, q_embed, Urnn_norm=True, Uatt_norm=True):\n",
    "        '''\n",
    "        Given B's output vector, calculate the attention over \n",
    "        all the query's bounding boxes vectors using batch normalization. , That is, calculate:\n",
    "        \n",
    "        probs = softmax(relu(context(Sq+Satt+b)))\n",
    "        \n",
    "        Where:\n",
    "        Sq = <Wq, queries_states>\n",
    "        Sattn = <Wattn, attention_bboxes_vectors>\n",
    "        \n",
    "        The  bounding box with the highest attention score will be chosen as the correct bounding box.\n",
    "        This function uses batch normalization. \n",
    "        \n",
    "        Params:\n",
    "            q_embed: Tensor of shape (batch size x num_hidden)B's outputs. \n",
    "            Urnn_norm: Whether to use batch normalization for the queries.\n",
    "            Uatt_norm: Whether to use batch normalization for the VGG outputs.\n",
    "            \n",
    "        Returns:\n",
    "            probs: Tensor of shape (batch_size x max bbox number for query).\n",
    "                   Attention score for each bbox.\n",
    "        '''\n",
    "        # concatenate img vectors with with spaical features\n",
    "        attn_vecs = tf.concat([self.img, self.bboxes], 2)\n",
    "        Urnn = q_embed\n",
    "        Uatt = attn_vecs\n",
    "        \n",
    "        if Urnn_norm:\n",
    "            # B's outputs with bath normalization. \n",
    "            # shape: (batch size x num_hidden)\n",
    "            Urnn = tf.contrib.layers.batch_norm(\n",
    "                q_embed, center=True, scale=True, is_training=self.isTrain) \n",
    "        \n",
    "        if Uatt_norm:\n",
    "            # Attention vectors with bath normalization. \n",
    "            # shape: (batch size x max bbox number for query x attention vector size)\n",
    "            Uatt = tf.contrib.layers.batch_norm(\n",
    "                attn_vecs, center=True, scale=True, is_training=self.isTrain)\n",
    "           \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            with tf.variable_scope('bnorm_l1'):\n",
    "                b = tf.get_variable(\n",
    "                        'b', \n",
    "                        initializer=tf.constant_initializer(0.1), \n",
    "                        shape=[1, self.num_hidden])\n",
    "\n",
    "                context = tf.get_variable(\n",
    "                        'context', \n",
    "                        initializer=tf.random_uniform_initializer(minval=-0.1, maxval=0.1), \n",
    "                        shape=[self.num_hidden, 1])\n",
    "\n",
    "\n",
    "                Sq = tf.nn.dropout(\n",
    "                    self.linear(Urnn, self.num_hidden, bias=False, scope='Sq'), \n",
    "                    self.dropout_q)\n",
    "                \n",
    "                Sattn = tf.nn.dropout(\n",
    "                            tf.reshape(\n",
    "                                self.linear(\n",
    "                                    tf.reshape(Uatt, (-1, self.img_dims+self.bbox_dims)), \n",
    "                                    self.num_hidden, \n",
    "                                    bias=False, scope='Sattn'), \n",
    "                                 [self.batch_size, -1, self.num_hidden]),\n",
    "                            self.dropout_img)\n",
    "                    \n",
    "                   \n",
    "            out = tf.nn.relu(tf.expand_dims(Sq, 1) + Sattn + b)\n",
    "            logits = tf.reshape(tf.matmul(tf.reshape(out, (-1, tf.shape(out)[-1])),  context), (tf.shape(out)[0], -1))\n",
    "\n",
    "            # Calculate logits's masked softmax (we use self.attn_idx to mas\n",
    "            max_logits = tf.reduce_max(logits, axis=-1)\n",
    "            masked_logits = tf.exp(logits-tf.expand_dims(max_logits, axis=1))*self.attn_idx\n",
    "            probs = self.attn_idx*masked_logits/tf.reduce_sum(masked_logits, axis=-1, keep_dims=True)\n",
    "\n",
    "            return probs\n",
    "  \n",
    "        \n",
    "    def q_padding(self, seq, max_length):\n",
    "        '''\n",
    "        Pad  seq with vocab['<pad>'] (0) to max_length length.\n",
    "        '''                  \n",
    "        return seq + [self.vocab['<pad>']]*(max_length-len(seq))\n",
    "\n",
    "    \n",
    "    def build_data(self, data, start, end, imScale, addNoise=False):\n",
    "        '''\n",
    "        Build batch.\n",
    "        ------------\n",
    "        \n",
    "        Params:\n",
    "            data: each entry in this list has the following structure:\n",
    "                  [query indexes, [bounding box vector (VGG), bounding box spaital features], ..., \n",
    "                  [bounding box vector (VGG), bounding box spaital features], index of the true label]\n",
    "                  \n",
    "            start/end: batch data is built from data[start:end]\n",
    "            \n",
    "        Returns:\n",
    "            attn_idx: attn_idx[i, j]=1 if the j'th bbox in the i'th query is not padding, else equals to 0. \n",
    "            \n",
    "            padded_queries: list of queries, padded to the length of the longest query in the batch.\n",
    "                            Note: vocab['pad']=0\n",
    "                            \n",
    "            padded_im: list of bounding boxes vectors, padded to the maximum number of bbox per query.\n",
    "                       Note: padded vector is vector of zeros. \n",
    "                            \n",
    "            padded_bbox: list of bounding boxes spatial features, padded to the maximum number of bbox per query.\n",
    "                         Note: padded vector is vector of zeros.  \n",
    "        \n",
    "            onehot_labels: onehot_labels[i][j]=1 if j is the true bbox for query i, else  onehot_labels[i][j]=0\n",
    "            \n",
    "            addNoise: Boolean. Whether to add normal noise to the images.\n",
    "            \n",
    "            imScale: If not None, scale the image vectors (VGG16 outputs) to have 0 mean and 1imScale std\n",
    "                        \n",
    "        '''\n",
    "                          \n",
    "        qlen = max([len(data[i][0]) for i in range(start, end)]) # Length fo the longest query\n",
    "        imlen = max([len(data[i]) for i in range(start, end)])-2 # Maximum number of bbox per query.\n",
    "        padded_queries, padded_im, padded_bbox, attn_idx = [], [], [], []\n",
    "        \n",
    "        # Build one hot labels from the labels index, given in the data.                  \n",
    "        labels = [item[-1] for item in data[start:end]] #data[i][-1]=index of the true bbox of query i\n",
    "        dist_labels = np.zeros((end-start, imlen)) #label distribution\n",
    "        \n",
    "        # Real data points\n",
    "        dist_labels[[i for i in np.arange(end-start) if labels[i]>=0], [l for l in labels if l>=0]]=1\n",
    "        \n",
    "        # augmented data points\n",
    "        # the label of each added poind is writen as -1*number of bboxes. \n",
    "        # When we build the batch it self (function build_data in class Model), \n",
    "        # if we see a negative label index, we know its an added point and we know the \n",
    "        # number of bboxes so we can build the correct distribution.\n",
    "        for i in np.arange(end-start):\n",
    "            if labels[i]<0:\n",
    "                dist_labels[i] = [-1/labels[i] for _ in range(-labels[i])]+[0. for _ in range(imlen+labels[i])]\n",
    "                          \n",
    "        im_dim, bbox_dim = data[0][1][0].shape[1], data[0][1][1].shape[1]\n",
    "        for i in range(start, end):\n",
    "            padded_queries.append(self.q_padding(data[i][0], qlen))\n",
    "            \n",
    "            attn_idx.append([1 for _ in range(len(data[i])-2)]+[0 for _ in range(imlen-(len(data[i])-2))])\n",
    "            \n",
    "            padded_im.append(np.concatenate([data[i][j][0] for j in range(1, len(data[i])-1)] + \n",
    "                                       [np.full((imlen-(len(data[i])-2), im_dim), vocab['<pad>'], dtype=np.float32)], axis=0))\n",
    "            \n",
    "            padded_bbox.append(np.concatenate([data[i][j][1] for j in range(1, len(data[i])-1)] + \n",
    "                                       [np.full((imlen-(len(data[i])-2),bbox_dim), vocab['<pad>'], dtype=np.float32)], axis=0))\n",
    "           \n",
    "        \n",
    "        \n",
    "        if addNoise:\n",
    "            padded_im+=(padded_im+np.random.normal(0, .1, np.array(padded_im).shape))*np.expand_dims(attn_idx, 2)\n",
    "        else:\n",
    "            padded_im=np.array(padded_im)\n",
    "            \n",
    "        if imScale is not None:\n",
    "            # Smoothing factor\n",
    "            halper = (1-np.expand_dims(np.array(attn_idx).astype(np.float32), 2))\n",
    "            img_min = np.min(padded_im, axis=-1, keepdims=True)\n",
    "            img_max = np.max(padded_im, axis=-1, keepdims=True)\n",
    "            img_scale = (padded_im-img_min)/((img_max-img_min)+halper) # Scale to 0-1\n",
    "            s = np.std(img_scale, axis=-1, keepdims=True)\n",
    "            padded_im = img_scale/(imScale*s+halper)  #Scale to have 1/imScale std\n",
    "            padded_im = padded_im - np.mean(padded_im, axis=-1, keepdims=True) # scale to zero mean\n",
    "            \n",
    "        return np.array(attn_idx), np.array(padded_queries, dtype=np.int32), padded_im, np.array(padded_bbox), np.array(dist_labels)\n",
    "            \n",
    "   \n",
    "    def ground(self, data=None, start=None, end=None, sess=None, feed_dict = None, isEdit=True, imScale=False):\n",
    "        '''\n",
    "        Given a query and a list of bboxes, the function returns the index of the chosen bbox and the ground truth bbox.\n",
    "        \n",
    "        Params:\n",
    "            data: A numpy array with datasat's data points\n",
    "            start/end: The function only take data points from data[start:end]\n",
    "            imScale: whether to scale the images vectors\n",
    "        '''\n",
    "        isSess = (sess==None)\n",
    "        if isSess:\n",
    "            sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            if isSess:\n",
    "                tf.global_variables_initializer().run()\n",
    "                ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "                else:\n",
    "                    print('Initializing variables')\n",
    "            if feed_dict is None:\n",
    "                attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(data, start, end, imScale=imScale)\n",
    "                feed_dict = {\n",
    "                        self.queries:padded_queries,\n",
    "                        self.img:padded_im,\n",
    "                        self.bboxes:padded_bbox,\n",
    "                        self.labels:labels,\n",
    "                        self.attn_idx:attn_idx\n",
    "                    }\n",
    "            feed_dict[self.isTrain]=False\n",
    "            feed_dict[self.isEdit] = isEdit\n",
    "            feed_dict[self.dropout_in]=1.\n",
    "            feed_dict[self.dropout_out]=1.\n",
    "            feed_dict[self.dropout_img]=1.\n",
    "            feed_dict[self.dropout_q]=1.\n",
    "            scores = sess.run(self.scores, feed_dict=feed_dict) # get score for each bbox\n",
    "\n",
    "        return np.argmax(scores, axis=1), np.argmax(feed_dict[self.labels], axis=1)\n",
    "        \n",
    "        \n",
    "    def iou_accuracy(self, data, start, end, imScale, sess=None, feed_dict = None, threshold=0.5, test=False, isEdit=True):\n",
    "        '''\n",
    "        Calculate the IOU score between the Model bbox and the true bbox.\n",
    "        \n",
    "         Params:\n",
    "            data: A numpy array with datasat's data points\n",
    "            start/end: The function only take data points from data[start:end]\n",
    "            imScale: whether to scale the images vectors\n",
    "            threshold: If IOU>0.5 this is a true positive\n",
    "        ''' \n",
    "                          \n",
    "        # Get score for each bbox (labels) and th true bbox index (gt_idx)                  \n",
    "        if feed_dict is None:\n",
    "            labels, gt_idx = self.ground(data, start, end, sess=sess, feed_dict=feed_dict, \n",
    "                                         isEdit=isEdit, imScale=imScale)\n",
    "        else: labels, gt_idx = self.ground(sess=sess, feed_dict=feed_dict, \n",
    "                                           isEdit=isEdit, imScale=imScale)\n",
    "        acc = 0\n",
    "        \n",
    "        for i in range(start, end):\n",
    "            gt = data[i][gt_idx[i-start]+1][1][0] # ground truth bbox. Note that len(data)!=len(gt_idx)=batch_size\n",
    "            crops = np.expand_dims(data[i][labels[i-start]+1][1][0], axis=0) #Model chosen bbox. Note that len(data)!=len(labels)=batch_size\n",
    "            acc += (retriever.compute_iou(crops, gt)[0]>threshold) #IOU for the i sample.\n",
    "            \n",
    "        return acc/(end-start)\n",
    "        \n",
    "    def accuracy(self, imScale, data=None, start=None, end=None, sess=None, feed_dict = None, isEdit=True):\n",
    "        isSess = (sess==None)\n",
    "        if isSess:\n",
    "            print('Building sess')\n",
    "            sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            if isSess:\n",
    "                print('Building sess used')\n",
    "                tf.global_variables_initializer().run()\n",
    "                ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    print('3')\n",
    "                    self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "                else:\n",
    "                    print('Initializing variables')\n",
    "            if feed_dict is None:\n",
    "                print('Building feed_dict')\n",
    "                attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(data, start, end, imScale=imScale)\n",
    "                feed_dict = {\n",
    "                        self.queries:padded_queries,\n",
    "                        self.img:padded_im,\n",
    "                        self.bboxes:padded_bbox,\n",
    "                        self.attn_idx:attn_idx,\n",
    "                        self.labels:labels,\n",
    "                    }\n",
    "                \n",
    "            feed_dict[self.isTrain]=False\n",
    "            feed_dict[self.isEdit] = isEdit\n",
    "            feed_dict[self.dropout_in]=1.\n",
    "            feed_dict[self.dropout_out]=1.\n",
    "            feed_dict[self.dropout_img]=1.\n",
    "            feed_dict[self.dropout_q]=1.\n",
    "            scores = sess.run(self.scores, feed_dict=feed_dict)\n",
    "            acc = sum(np.equal(np.argmax(scores, axis=1), np.argmax(feed_dict[self.labels], axis=1))/len(feed_dict[self.labels]))\n",
    "\n",
    "                    \n",
    "        return acc\n",
    "    \n",
    "    def discount_rewards(self, r, last_r, gamma=1.0):\n",
    "        \"\"\" \n",
    "        take 1D float array of rewards and compute discounted reward \n",
    "        using bellman function.\n",
    "        \n",
    "        params:\n",
    "            r: r[i,j]=1 reward of action A did on word j at query i.\n",
    "            last_r: B's loss for query i with no edits. This is the final reward.\n",
    "            gamma: discount factor.\n",
    "        \"\"\"\n",
    "                          \n",
    "        discounted_r = np.zeros(r.shape)\n",
    "        running_add = last_r # B loss\n",
    "        discounted_r = [i for i in range(r.shape[-1])]\n",
    "        for t in reversed(range(0, r.shape[-1])):\n",
    "            running_add = running_add * gamma + r[:,t]\n",
    "            discounted_r[t] = running_add\n",
    "        return np.array(discounted_r).T\n",
    "        \n",
    "    def train(self, trn_data, tst_data, ephocs_num, edit_reward, rnn_editProb, startA=3, \n",
    "              activation_ephoc=10, muteB=3, start_ephoc=0, dropout_in=1., onlyB=False,\n",
    "              dropout_out=1., dropout_img=1., dropout_q=1., editProb=0.5,\n",
    "              addNoise=False, imScale=None):\n",
    "                          \n",
    "        '''\n",
    "        Params:\n",
    "             trn_data: list, train set. \n",
    "             \n",
    "             tst_data: list, test set. \n",
    "             \n",
    "             ephocs_num: number of ephocs\n",
    "             \n",
    "             start_ephoc: number of first ephoc.\n",
    "             \n",
    "             edit_reward: int, coefficient to multiply the reward by when editing a word.\n",
    "             \n",
    "             startA: int, Start competition only at ephoc # startA.\n",
    "             \n",
    "             activation_ephoc: at ephoc numer \"activation_ephoc\", A will be activate.\n",
    "                               That is, for (activation_ephoc-startA) number of ephocs, \n",
    "                               A will chooce an action randomly.\n",
    "             \n",
    "            muteB: After A starts, for each ephoc which A & B trains, \n",
    "                   only A will be trained for this amount of ephocs.\n",
    "                   \n",
    "            editProb: robabilty for editing a query.\n",
    "            \n",
    "            rnn_editProb: Probabilty for edit a word in rnn, when decision are taken randomly.  \n",
    "            \n",
    "            dropout_in: dropout ratio of B's rnn inputs.\n",
    "            \n",
    "            dropout_output: dropout ratio of B's rnn output.\n",
    "            \n",
    "            dropout_img: dropout ratio of images vectors before the last attention layer .\n",
    "            \n",
    "            addNoise: Boolean. Whether to add normal noise to the images (see build_data).\n",
    "            \n",
    "            imScale: If not None, scale the image vectors (VGG16 outputs) to have 0 mean and 1imScale std\n",
    "            \n",
    "            onlyB: boolean, Wheter to train only B. By setting startA and acctivation_ephoc larger than ephocs_num\n",
    "                   and setting onlyB to True, we get A out of the game \n",
    "                               \n",
    "        '''                  \n",
    "        \n",
    "        trn_nbatch = len(trn_data)\n",
    "        tst_nbatch = len(tst_data)\n",
    "        print('# Train set size:', len([len(batch) for batch in trn_data]))\n",
    "        print('# Training batches:', trn_nbatch)\n",
    "        print('# Test set size:', len([len(batch) for batch in tst_data]))\n",
    "        print('# Testing batches:', tst_nbatch)\n",
    "        self.test_res, self.train_res = [], [] #list to hold accuracy of test and train sets\n",
    "        \n",
    "        # When activating A, the probability to choose an action randomly is 1/delta \n",
    "        # We increase delta during training.\n",
    "        delta = 1\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            tf.global_variables_initializer().run()\n",
    "            ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                print('Loading parameters from', ckpt.model_checkpoint_path)\n",
    "                self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "            else:\n",
    "                print('Initializing variables')\n",
    "                \n",
    "            for ephoc in range(start_ephoc, ephocs_num):\n",
    "                startTime = datetime.now().replace(microsecond=0)\n",
    "                          \n",
    "                # Train B only after competition start (ephoc<startA). \n",
    "                # Each time we train B, we train only B for muteB ephocs.\n",
    "                toTrainB = ephoc<startA or (ephoc-(startA-1))%(muteB+1)==0 # Whether to train only B \n",
    "                \n",
    "                # Every 2 times we train B and A (+ only A for muteB ephocs), we increase\n",
    "                # delta by one\n",
    "                if (ephoc-activation_ephoc)%((muteB+1)*2)==0 and ephoc>=activation_ephoc:\n",
    "                    delta+=1\n",
    "                \n",
    "                # * If ephoc<startA, train A with no word edits\n",
    "                #   by setting editRandomlyProb to one and rnn_editProb to 0 (always \n",
    "                #   choose an action randomly with zero probability for edit). \n",
    "                # * If we train B in this ephoc, set editRandomlyProb to zero.\n",
    "                if ephoc<startA:\n",
    "                    rnn_editProb_holder = 0.\n",
    "                    editRandomlyProb=1.\n",
    "                elif toTrainB==False:\n",
    "                    rnn_editProb_holder = rnn_editProb\n",
    "                    editRandomlyProb=1/delta\n",
    "                else:\n",
    "                    rnn_editProb_holder= 0.\n",
    "                    editRandomlyProb=0.\n",
    "                        \n",
    "                # When we train B with edited words (itrInEphoc=2), we show B results without editing (on train set).\n",
    "                # Note the we train B with edited words only if delta>2.\n",
    "                itrInEphoc=1\n",
    "                if onlyB==False and delta>2 and toTrainB:\n",
    "                    itrInEphoc=2\n",
    "                    \n",
    "                print('='*50,'\\nTrain, ephoc:',ephoc, '; Training B:', toTrainB)\n",
    "                np.random.shuffle(trn_data)\n",
    "                for ep in range(itrInEphoc):\n",
    "                    trn_ce_loss, A_trn_loss, B_trn_loss = 0, 0, 0\n",
    "                    trn_acc, trn_iou, edits_count = 0, 0, 0\n",
    "                    tst_ce_loss, tst_loss, tst_acc, tst_iou = 0, 0, 0, 0\n",
    "                    if ep==1:\n",
    "                        print('No Edit')\n",
    "                        print('ooooooo')\n",
    "                    edit_num = 0 # average number of edited words per query\n",
    "                    editRount_count = 0 # number of iterations with competition.\n",
    "                    \n",
    "                    for b in range(trn_nbatch):\n",
    "                        if ep==1:\n",
    "                            '''\n",
    "                            If ep=1, we're in round 2 in which we only show B's \n",
    "                            results with out editing (on the train set).\n",
    "                            '''\n",
    "                            isEdit=False\n",
    "                        else:\n",
    "                            # If ephoc<startA we train B and A with no edits\n",
    "                            # then, if B is trained, only if delta>2 (which mean A was trained with un-random choices)\n",
    "                            # we can train B with A edits.\n",
    "                            isEdit = onlyB==False and ephoc<startA or toTrainB==False or (delta>2 and np.random.rand(1)[0]<editProb)\n",
    "                            \n",
    "                        attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(trn_data[b], \n",
    "                                                                                            0, \n",
    "                                                                                            self.batch_size, \n",
    "                                                                                            addNoise=addNoise,\n",
    "                                                                                            imScale=imScale)\n",
    "                \n",
    "                        reward_loss = np.array([[0., 0.] for _ in range(self.batch_size)]) # dummy holder \n",
    "                    \n",
    "\n",
    "                        feed_dict = {\n",
    "                            self.queries:padded_queries,\n",
    "                            self.img:padded_im,\n",
    "                            self.bboxes:padded_bbox,\n",
    "                            self.attn_idx:attn_idx,\n",
    "                            self.labels: labels,\n",
    "                            self.unk:np.array([[vocab['<unk>']]]),\n",
    "                            self.isEdit:isEdit,\n",
    "                            self.reward_loss:reward_loss,\n",
    "                            self.Aattn_vecs:[[[]]],# dummy holder \n",
    "                            self.dropout_in:dropout_in,\n",
    "                            self.dropout_out:dropout_out,\n",
    "                            self.dropout_img:dropout_img,\n",
    "                            self.dropout_q:dropout_q,\n",
    "                            self.isTrain:True,\n",
    "                            self.isIDX:False,\n",
    "                            self.set_actions:[[]],# dummy holder \n",
    "                            self.rnn_editProb:rnn_editProb_holder,\n",
    "                            self.editRandomlyProb:editRandomlyProb\n",
    "                        }\n",
    "                        \n",
    "                        \n",
    "                        if isEdit==True:\n",
    "                            editRount_count+=1\n",
    "                          \n",
    "                            '''We first run B with no edit in order to get it's loss and outputs.'''\n",
    "                            feed_dict[self.isEdit] = False\n",
    "                            queries_lens, B_ce, outputs = sess.run([self.queries_lens, self.B_ce, self.outputs], feed_dict=feed_dict)\n",
    "                            feed_dict[self.isEdit] = True\n",
    "                            # B_ce contain the loss for each query.\n",
    "                            # We scale all the losses to have mean 1 and std 0.3.\n",
    "                            Bce_min = min(B_ce)\n",
    "                            Bce_max = max(B_ce)\n",
    "                            Bce_scale = (B_ce-Bce_min)/(Bce_max-Bce_min)\n",
    "                            s = np.std(Bce_scale)\n",
    "                            BCE = 0.3*Bce_scale/s\n",
    "                            BCE = 1 + BCE - np.mean(BCE)\n",
    "                          \n",
    "                            # reward for edit a word = edit_reward*BCE/queries_lens.\n",
    "                            # A's get the reward and loss per query  as 2 of its features.\n",
    "                            rewards_loss = np.concatenate(\n",
    "                                [np.expand_dims(edit_reward*BCE/queries_lens, 1), np.expand_dims(BCE, 1)], 1)\n",
    "                            \n",
    "                            feed_dict[self.reward_loss]=rewards_loss\n",
    "                            \n",
    "                            # A's attention vectors are B's outputs with no edited words.\n",
    "                            Aattn_vecs = outputs[:,:,:self.num_hidden]\n",
    "                            feed_dict[self.Aattn_vecs]=Aattn_vecs\n",
    "                           \n",
    "                            '''\n",
    "                            We then run B with A's edits. \n",
    "                            We get idx (A's decision [0,1] for each words).\n",
    "                            edit_num: the ratio of edited words per query \n",
    "                            B_ce, B_loss: B's loss per query and B overall mean.\n",
    "                            '''\n",
    "                           \n",
    "                            idx, B_ce1, B_loss, edit_num = sess.run(\n",
    "                                [self.idx, self.B_ce, self.B_loss, self.edit_num], \n",
    "                                 feed_dict=feed_dict)\n",
    "\n",
    "                            # Get reward per word.\n",
    "                            # If word i in query j was edited idx[j,i]=1 --> rewards[i,j] = 1*reward\n",
    "                            # If word i in query j was not edited idx[j,i]=0 --> rewards[i,j] = 0*reward\n",
    "                            rewards = idx*np.expand_dims(\n",
    "                                edit_reward*BCE/queries_lens, axis=1)\n",
    "                            \n",
    "                            # Scale new losses\n",
    "                            Bce_min = min(B_ce1)\n",
    "                            Bce_max = max(B_ce1)\n",
    "                            Bce_scale = (B_ce1-Bce_min)/(Bce_max-Bce_min)\n",
    "                            s = np.std(Bce_scale)\n",
    "                            BCE1 = .3*Bce_scale/s\n",
    "                            BCE1 = 1 + BCE1 - np.mean(BCE1)\n",
    "                            \n",
    "                            feed_dict[self.actions_idx] = np.abs(list(zip(range(\n",
    "                                        self.batch_size*idx.shape[-1]), idx.reshape(-1))))\n",
    "                            feed_dict[self.bell_val] = self.discount_rewards(rewards, BCE1)\n",
    "                            feed_dict[self.set_actions] = idx\n",
    "                            feed_dict[self.isIDX]=True\n",
    "                                                         \n",
    "                            A_loss, _ = sess.run(\n",
    "                                [self.A_loss, self.A_optimizer], feed_dict=feed_dict)\n",
    "                                                        \n",
    "                            if toTrainB:\n",
    "                                feed_dict[self.isIDX]=False\n",
    "                                lr, gs, B_loss, edit_num, _ = sess.run(\n",
    "                                    [self.learning_rate, self.global_step, self.B_loss, self.edit_num, self.B_optimizer], \n",
    "                                    feed_dict=feed_dict)\n",
    "                \n",
    "                    \n",
    "                            acc = self.accuracy(sess=sess, feed_dict=feed_dict, isEdit=isEdit, imScale=imScale)  \n",
    "                            iou_acc = self.iou_accuracy(\n",
    "                                trn_data[b], 0, self.batch_size, imScale=imScale,\n",
    "                                sess=sess, feed_dict=feed_dict, isEdit=isEdit)\n",
    "\n",
    "                            trn_acc += acc/trn_nbatch\n",
    "                            A_trn_loss += A_loss\n",
    "                            B_trn_loss += B_loss/trn_nbatch\n",
    "                            trn_iou += iou_acc/trn_nbatch\n",
    "                            edits_count += edit_num\n",
    "                            \n",
    "                            if b%50==0:\n",
    "                                if toTrainB: \n",
    "                                    print('Edit:%s'%(isEdit), \n",
    "                                          ';batch:%d'%(b), \n",
    "                                          ';gs:%d'%(gs), ';lr:%.3f'%(lr), ';Bloss:%.3f'%(B_loss), \n",
    "                                          ';Aloss:%.3f'%(A_loss), ';edits:%.2f'%(edit_num),  ';editRandomlyProb:%.2f'%(editRandomlyProb),\n",
    "                                          ';acc:%.3f'%(acc), ';iou:%.3f'%(iou_acc), ';time:', datetime.now().replace(microsecond=0)-startTime)    \n",
    "                                else:\n",
    "                                    print('Edit:%s'%(isEdit),\n",
    "                                          ';batch:%d'%(b), \n",
    "                                          ';Bloss:%.3f'%(B_loss), ';Aloss:%.3f'%(A_loss), \n",
    "                                          ';edits:%.2f'%(edit_num), ';editRandomlyProb:%.2f'%(editRandomlyProb), ';acc:%.3f'%(acc),\n",
    "                                          ';iou:%.3f'%(iou_acc), ';time:', datetime.now().replace(microsecond=0)-startTime)  \n",
    "                                \n",
    "\n",
    "                               \n",
    "                        else: # isEdit=False\n",
    "                            if ep==0:\n",
    "                                B_loss, lr, gs, _ = sess.run([self.B_loss, self.learning_rate, \n",
    "                                                                self.global_step, self.B_optimizer], feed_dict=feed_dict)\n",
    "                            else: \n",
    "                                # just show results of B with no edits\n",
    "                                B_loss, lr, gs = sess.run([self.B_loss, self.learning_rate, self.global_step], feed_dict=feed_dict)\n",
    "\n",
    "                            acc = self.accuracy(sess=sess, feed_dict=feed_dict, isEdit=isEdit, imScale=imScale)  \n",
    "                            iou_acc = self.iou_accuracy(trn_data[b], 0, self.batch_size, imScale=imScale,\n",
    "                                                        sess=sess, feed_dict=feed_dict, isEdit=isEdit)\n",
    "\n",
    "                            trn_acc += acc/trn_nbatch\n",
    "                            B_trn_loss += B_loss/trn_nbatch\n",
    "                            trn_iou += iou_acc/trn_nbatch\n",
    "\n",
    "                            if b%50==0:\n",
    "                                print('Edit:%s'%(isEdit), \n",
    "                                      ';batch:%d'%(b),  \n",
    "                                      ';gs:%d'%(gs), ';lr:%.3f'%(lr),\n",
    "                                      ';Bloss:%.3f'%(B_loss),  ';acc:%.3f'%(acc), \n",
    "                                      ';iou:%.3f'%(iou_acc),\n",
    "                                      ';time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "\n",
    "                    if editRount_count>0:\n",
    "                        print('\\n*Training B:', ephoc%3==0, ';B Train loss: %.3f'%(B_trn_loss), ';A Train loss: %.3f'%(A_trn_loss/editRount_count),                                                                                             \n",
    "                          ';Edit num: %.3f'%(edits_count/editRount_count), \n",
    "                          ';Train accuracy: %.3f'%(trn_acc),  ';IOU accuracy: %.3f'%(trn_iou), \n",
    "                          ';Time:', datetime.now().replace(microsecond=0)-startTime, '\\n')\n",
    "                    else:\n",
    "                        print('\\n*Training B:', ephoc%3==0, ';B Train loss: %.3f'%(B_trn_loss),';Train accuracy: %.3f'%(trn_acc), \n",
    "                          ';IOU accuracy: %.3f'%(trn_iou),  \n",
    "                          ';Time:', datetime.now().replace(microsecond=0)-startTime, '\\n')\n",
    "                          \n",
    "                if editRount_count>0:\n",
    "                    self.train_res.append([trn_acc, trn_iou, B_trn_loss, A_trn_loss/editRount_count])\n",
    "                else: \n",
    "                    self.train_res.append([trn_acc, trn_iou, B_trn_loss, 0])\n",
    "                self.saver.save(sess, params_dir + \"/model.ckpt\", global_step=ephoc)    \n",
    "                if ephoc<startA or toTrainB:\n",
    "                    print('Testing, ephoc:',ephoc)\n",
    "                    tstTime = datetime.now().replace(microsecond=0)\n",
    "                    tst_loss, tst_acc, tst_iou = 0, 0, 0\n",
    "                    for b in range(tst_nbatch):\n",
    "                        attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(tst_data[b],\n",
    "                                                                                    0, self.batch_size, imScale=imScale)\n",
    "                        rewards_loss = np.array([[0., 0.] for _ in range(self.batch_size)])\n",
    "                        feed_dict = {\n",
    "                            self.queries:padded_queries,\n",
    "                            self.img:padded_im,\n",
    "                            self.bboxes:padded_bbox,\n",
    "                            self.attn_idx:attn_idx,\n",
    "                            self.labels: labels,\n",
    "                            self.unk:np.array([[vocab['<unk>']]]),\n",
    "                            self.reward_loss:reward_loss,\n",
    "                            self.isEdit:False,\n",
    "                            self.Aattn_vecs:[[[]]],\n",
    "                            self.dropout_in:1.,\n",
    "                            self.dropout_out:1.,\n",
    "                            self.dropout_img:1.,\n",
    "                            self.dropout_q:1.,\n",
    "                            self.isTrain:False,\n",
    "                            self.isIDX:False,\n",
    "                            self.set_actions:[[]],# dummy holder \n",
    "                            self.rnn_editProb:0.,\n",
    "                            self.editRandomlyProb:0.\n",
    "                        }\n",
    "                        B_loss = sess.run(self.B_loss, feed_dict=feed_dict)\n",
    "\n",
    "                        acc = self.accuracy(sess=sess, feed_dict=feed_dict, isEdit=False, imScale=imScale)\n",
    "                        iou_acc = self.iou_accuracy(\n",
    "                            tst_data[b], 0, self.batch_size, sess=sess, \n",
    "                            feed_dict=feed_dict, isEdit=False, imScale=imScale)\n",
    "                        \n",
    "                        tst_acc += acc/tst_nbatch\n",
    "                        tst_loss += B_loss/tst_nbatch\n",
    "                        tst_iou += iou_acc/tst_nbatch\n",
    "                        if b%50==0:\n",
    "                            print('batch:', b, ';B loss: %.3f'%(B_loss), ';acc: %.3f'%(acc), \n",
    "                                   ';iou_acc: %.3f'%(iou_acc), ';time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "                    print('\\n*BTrain:', ephoc%3==0, ';Test loss: %.3f'%(tst_loss), ';Test accuracy %.3f'%(tst_acc), \n",
    "                          ';IOU accuracy: %.3f'%(tst_iou), ';Time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "                    self.test_res.append([tst_acc, tst_iou, tst_loss])\n",
    "                print('='*50,'\\n')\n",
    "            return self.test_res, self.train_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edit_reward: -1.0\n",
      "params_dir: ../data/training/models/All/RL/EXP/hidden:200\n",
      "num_hidden: 200\n",
      "learning rate: 0.05\n",
      "# Train set size: 297\n",
      "# Training batches: 297\n",
      "# Test set size: 297\n",
      "# Testing batches: 297\n",
      "Initializing variables\n",
      "================================================== \n",
      "Train, ephoc: 0 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:1 ;lr:0.050 ;Bloss:2.506 ;acc:0.150 ;iou:0.265 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:51 ;lr:0.050 ;Bloss:2.691 ;acc:0.245 ;iou:0.370 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:101 ;lr:0.050 ;Bloss:2.469 ;acc:0.335 ;iou:0.435 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:151 ;lr:0.050 ;Bloss:1.806 ;acc:0.610 ;iou:0.705 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:201 ;lr:0.050 ;Bloss:2.039 ;acc:0.620 ;iou:0.695 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:251 ;lr:0.050 ;Bloss:2.757 ;acc:0.385 ;iou:0.530 ;time: 0:00:39\n",
      "\n",
      "*Training B: True ;B Train loss: 2.585 ;Train accuracy: 0.383 ;IOU accuracy: 0.492 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 0\n",
      "batch: 0 ;B loss: 1.106 ;acc: 0.710 ;iou_acc: 0.815 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 1.693 ;acc: 0.650 ;iou_acc: 0.725 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 2.056 ;acc: 0.530 ;iou_acc: 0.635 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 2.255 ;acc: 0.480 ;iou_acc: 0.600 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 2.609 ;acc: 0.415 ;iou_acc: 0.555 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 3.058 ;acc: 0.315 ;iou_acc: 0.435 ;time: 0:01:19\n",
      "\n",
      "*BTrain: True ;Test loss: 2.316 ;Test accuracy 0.468 ;IOU accuracy: 0.588 ;Time: 0:01:29\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 1 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:298 ;lr:0.050 ;Bloss:2.018 ;acc:0.580 ;iou:0.675 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:348 ;lr:0.050 ;Bloss:3.461 ;acc:0.225 ;iou:0.420 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:398 ;lr:0.050 ;Bloss:3.642 ;acc:0.225 ;iou:0.455 ;time: 0:00:18\n",
      "Edit:False ;batch:150 ;gs:448 ;lr:0.050 ;Bloss:1.268 ;acc:0.770 ;iou:0.795 ;time: 0:00:27\n",
      "Edit:False ;batch:200 ;gs:498 ;lr:0.050 ;Bloss:1.905 ;acc:0.565 ;iou:0.675 ;time: 0:00:35\n",
      "Edit:False ;batch:250 ;gs:548 ;lr:0.050 ;Bloss:1.026 ;acc:0.800 ;iou:0.860 ;time: 0:00:42\n",
      "\n",
      "*Training B: False ;B Train loss: 1.984 ;Train accuracy: 0.566 ;IOU accuracy: 0.677 ;Time: 0:00:49 \n",
      "\n",
      "Testing, ephoc: 1\n",
      "batch: 0 ;B loss: 0.842 ;acc: 0.805 ;iou_acc: 0.895 ;time: 0:00:50\n",
      "batch: 50 ;B loss: 1.110 ;acc: 0.760 ;iou_acc: 0.835 ;time: 0:00:54\n",
      "batch: 100 ;B loss: 1.541 ;acc: 0.640 ;iou_acc: 0.715 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 1.505 ;acc: 0.710 ;iou_acc: 0.770 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.978 ;acc: 0.520 ;iou_acc: 0.665 ;time: 0:01:13\n",
      "batch: 250 ;B loss: 2.484 ;acc: 0.430 ;iou_acc: 0.570 ;time: 0:01:22\n",
      "\n",
      "*BTrain: False ;Test loss: 1.755 ;Test accuracy 0.594 ;IOU accuracy: 0.707 ;Time: 0:01:33\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 2 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:595 ;lr:0.050 ;Bloss:1.126 ;acc:0.765 ;iou:0.845 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:645 ;lr:0.050 ;Bloss:2.141 ;acc:0.495 ;iou:0.580 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:695 ;lr:0.050 ;Bloss:0.658 ;acc:0.835 ;iou:0.925 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:745 ;lr:0.050 ;Bloss:1.163 ;acc:0.750 ;iou:0.845 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:795 ;lr:0.050 ;Bloss:2.480 ;acc:0.410 ;iou:0.565 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:845 ;lr:0.050 ;Bloss:1.925 ;acc:0.530 ;iou:0.680 ;time: 0:00:39\n",
      "\n",
      "*Training B: False ;B Train loss: 1.661 ;Train accuracy: 0.630 ;IOU accuracy: 0.736 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 2\n",
      "batch: 0 ;B loss: 0.767 ;acc: 0.815 ;iou_acc: 0.895 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.966 ;acc: 0.805 ;iou_acc: 0.860 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 1.392 ;acc: 0.690 ;iou_acc: 0.755 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 1.354 ;acc: 0.720 ;iou_acc: 0.785 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.840 ;acc: 0.570 ;iou_acc: 0.710 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 2.368 ;acc: 0.430 ;iou_acc: 0.615 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.619 ;Test accuracy 0.626 ;IOU accuracy: 0.732 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 3 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:892 ;lr:0.050 ;Bloss:1.051 ;acc:0.790 ;iou:0.860 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:942 ;lr:0.050 ;Bloss:1.620 ;acc:0.640 ;iou:0.760 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:992 ;lr:0.050 ;Bloss:1.132 ;acc:0.785 ;iou:0.855 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:1042 ;lr:0.050 ;Bloss:1.715 ;acc:0.580 ;iou:0.690 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:1092 ;lr:0.050 ;Bloss:2.428 ;acc:0.380 ;iou:0.565 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:1142 ;lr:0.050 ;Bloss:1.617 ;acc:0.625 ;iou:0.700 ;time: 0:00:42\n",
      "\n",
      "*Training B: True ;B Train loss: 1.564 ;Train accuracy: 0.655 ;IOU accuracy: 0.755 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 3\n",
      "batch: 0 ;B loss: 0.722 ;acc: 0.845 ;iou_acc: 0.915 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.952 ;acc: 0.770 ;iou_acc: 0.830 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 1.362 ;acc: 0.690 ;iou_acc: 0.760 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 1.441 ;acc: 0.675 ;iou_acc: 0.755 ;time: 0:01:04\n",
      "batch: 200 ;B loss: 1.905 ;acc: 0.560 ;iou_acc: 0.670 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 2.436 ;acc: 0.390 ;iou_acc: 0.550 ;time: 0:01:20\n",
      "\n",
      "*BTrain: True ;Test loss: 1.632 ;Test accuracy 0.620 ;IOU accuracy: 0.722 ;Time: 0:01:31\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 4 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:1189 ;lr:0.050 ;Bloss:0.518 ;acc:0.915 ;iou:0.945 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:1239 ;lr:0.050 ;Bloss:2.458 ;acc:0.410 ;iou:0.570 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:1289 ;lr:0.050 ;Bloss:3.102 ;acc:0.260 ;iou:0.445 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:1339 ;lr:0.050 ;Bloss:1.305 ;acc:0.725 ;iou:0.805 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:1389 ;lr:0.050 ;Bloss:0.944 ;acc:0.805 ;iou:0.885 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:1439 ;lr:0.050 ;Bloss:0.778 ;acc:0.860 ;iou:0.930 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 1.502 ;Train accuracy: 0.671 ;IOU accuracy: 0.770 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 4\n",
      "batch: 0 ;B loss: 0.661 ;acc: 0.855 ;iou_acc: 0.915 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.828 ;acc: 0.820 ;iou_acc: 0.865 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 1.267 ;acc: 0.745 ;iou_acc: 0.805 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 1.258 ;acc: 0.755 ;iou_acc: 0.825 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.755 ;acc: 0.580 ;iou_acc: 0.720 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 2.214 ;acc: 0.470 ;iou_acc: 0.630 ;time: 0:01:20\n",
      "\n",
      "*BTrain: False ;Test loss: 1.490 ;Test accuracy 0.659 ;IOU accuracy: 0.761 ;Time: 0:01:30\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 5 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:1486 ;lr:0.050 ;Bloss:1.008 ;acc:0.795 ;iou:0.855 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:1536 ;lr:0.050 ;Bloss:1.649 ;acc:0.660 ;iou:0.785 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:1586 ;lr:0.050 ;Bloss:1.802 ;acc:0.600 ;iou:0.690 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:1636 ;lr:0.050 ;Bloss:0.758 ;acc:0.875 ;iou:0.900 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:1686 ;lr:0.050 ;Bloss:1.586 ;acc:0.645 ;iou:0.715 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:1736 ;lr:0.050 ;Bloss:1.363 ;acc:0.700 ;iou:0.785 ;time: 0:00:43\n",
      "\n",
      "*Training B: False ;B Train loss: 1.451 ;Train accuracy: 0.685 ;IOU accuracy: 0.782 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 5\n",
      "batch: 0 ;B loss: 0.602 ;acc: 0.870 ;iou_acc: 0.935 ;time: 0:00:50\n",
      "batch: 50 ;B loss: 0.797 ;acc: 0.840 ;iou_acc: 0.875 ;time: 0:00:54\n",
      "batch: 100 ;B loss: 1.220 ;acc: 0.740 ;iou_acc: 0.795 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 1.251 ;acc: 0.750 ;iou_acc: 0.825 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.716 ;acc: 0.615 ;iou_acc: 0.750 ;time: 0:01:12\n",
      "batch: 250 ;B loss: 2.168 ;acc: 0.475 ;iou_acc: 0.650 ;time: 0:01:21\n",
      "\n",
      "*BTrain: False ;Test loss: 1.453 ;Test accuracy 0.668 ;IOU accuracy: 0.769 ;Time: 0:01:31\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 6 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:1783 ;lr:0.050 ;Bloss:0.811 ;acc:0.850 ;iou:0.905 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:1833 ;lr:0.050 ;Bloss:0.863 ;acc:0.835 ;iou:0.895 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:1883 ;lr:0.050 ;Bloss:1.106 ;acc:0.790 ;iou:0.860 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:1933 ;lr:0.050 ;Bloss:0.945 ;acc:0.830 ;iou:0.890 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:1983 ;lr:0.050 ;Bloss:1.630 ;acc:0.650 ;iou:0.750 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:2033 ;lr:0.050 ;Bloss:2.068 ;acc:0.540 ;iou:0.665 ;time: 0:00:41\n",
      "\n",
      "*Training B: True ;B Train loss: 1.404 ;Train accuracy: 0.696 ;IOU accuracy: 0.790 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 6\n",
      "batch: 0 ;B loss: 0.576 ;acc: 0.880 ;iou_acc: 0.940 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.779 ;acc: 0.845 ;iou_acc: 0.890 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 1.199 ;acc: 0.730 ;iou_acc: 0.785 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 1.224 ;acc: 0.730 ;iou_acc: 0.810 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.696 ;acc: 0.615 ;iou_acc: 0.755 ;time: 0:01:13\n",
      "batch: 250 ;B loss: 2.142 ;acc: 0.505 ;iou_acc: 0.670 ;time: 0:01:22\n",
      "\n",
      "*BTrain: True ;Test loss: 1.432 ;Test accuracy 0.673 ;IOU accuracy: 0.772 ;Time: 0:01:32\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 7 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:2080 ;lr:0.050 ;Bloss:2.379 ;acc:0.450 ;iou:0.645 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:2130 ;lr:0.050 ;Bloss:1.420 ;acc:0.665 ;iou:0.730 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:2180 ;lr:0.050 ;Bloss:1.258 ;acc:0.740 ;iou:0.830 ;time: 0:00:18\n",
      "Edit:False ;batch:150 ;gs:2230 ;lr:0.050 ;Bloss:1.638 ;acc:0.640 ;iou:0.725 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:2280 ;lr:0.050 ;Bloss:0.950 ;acc:0.795 ;iou:0.885 ;time: 0:00:35\n",
      "Edit:False ;batch:250 ;gs:2330 ;lr:0.050 ;Bloss:1.324 ;acc:0.735 ;iou:0.835 ;time: 0:00:43\n",
      "\n",
      "*Training B: False ;B Train loss: 1.370 ;Train accuracy: 0.706 ;IOU accuracy: 0.798 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 7\n",
      "batch: 0 ;B loss: 0.544 ;acc: 0.880 ;iou_acc: 0.935 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.754 ;acc: 0.840 ;iou_acc: 0.875 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 1.179 ;acc: 0.765 ;iou_acc: 0.820 ;time: 0:01:00\n",
      "batch: 150 ;B loss: 1.197 ;acc: 0.740 ;iou_acc: 0.815 ;time: 0:01:06\n",
      "batch: 200 ;B loss: 1.640 ;acc: 0.640 ;iou_acc: 0.770 ;time: 0:01:13\n",
      "batch: 250 ;B loss: 2.065 ;acc: 0.510 ;iou_acc: 0.670 ;time: 0:01:21\n",
      "\n",
      "*BTrain: False ;Test loss: 1.394 ;Test accuracy 0.683 ;IOU accuracy: 0.781 ;Time: 0:01:30\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 8 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:2377 ;lr:0.050 ;Bloss:1.979 ;acc:0.555 ;iou:0.660 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:2427 ;lr:0.050 ;Bloss:1.636 ;acc:0.650 ;iou:0.730 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:2477 ;lr:0.050 ;Bloss:1.079 ;acc:0.825 ;iou:0.865 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:2527 ;lr:0.050 ;Bloss:0.490 ;acc:0.910 ;iou:0.955 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:2577 ;lr:0.050 ;Bloss:1.509 ;acc:0.690 ;iou:0.765 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:2627 ;lr:0.050 ;Bloss:1.176 ;acc:0.800 ;iou:0.825 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 1.343 ;Train accuracy: 0.715 ;IOU accuracy: 0.805 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 8\n",
      "batch: 0 ;B loss: 0.549 ;acc: 0.875 ;iou_acc: 0.935 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.740 ;acc: 0.840 ;iou_acc: 0.880 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 1.186 ;acc: 0.750 ;iou_acc: 0.805 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 1.201 ;acc: 0.710 ;iou_acc: 0.795 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.669 ;acc: 0.600 ;iou_acc: 0.750 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 2.103 ;acc: 0.510 ;iou_acc: 0.650 ;time: 0:01:19\n",
      "\n",
      "*BTrain: False ;Test loss: 1.402 ;Test accuracy 0.680 ;IOU accuracy: 0.777 ;Time: 0:01:29\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 9 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:2674 ;lr:0.050 ;Bloss:2.105 ;acc:0.520 ;iou:0.635 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:2724 ;lr:0.050 ;Bloss:0.503 ;acc:0.915 ;iou:0.960 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:2774 ;lr:0.050 ;Bloss:1.565 ;acc:0.660 ;iou:0.785 ;time: 0:00:18\n",
      "Edit:False ;batch:150 ;gs:2824 ;lr:0.050 ;Bloss:0.830 ;acc:0.850 ;iou:0.880 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:2874 ;lr:0.050 ;Bloss:1.752 ;acc:0.610 ;iou:0.730 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:2924 ;lr:0.050 ;Bloss:1.097 ;acc:0.785 ;iou:0.855 ;time: 0:00:41\n",
      "\n",
      "*Training B: True ;B Train loss: 1.314 ;Train accuracy: 0.723 ;IOU accuracy: 0.812 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 9\n",
      "batch: 0 ;B loss: 0.535 ;acc: 0.890 ;iou_acc: 0.945 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.732 ;acc: 0.850 ;iou_acc: 0.885 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 1.140 ;acc: 0.760 ;iou_acc: 0.820 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 1.167 ;acc: 0.720 ;iou_acc: 0.800 ;time: 0:01:04\n",
      "batch: 200 ;B loss: 1.570 ;acc: 0.650 ;iou_acc: 0.790 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 2.023 ;acc: 0.540 ;iou_acc: 0.685 ;time: 0:01:19\n",
      "\n",
      "*BTrain: True ;Test loss: 1.354 ;Test accuracy 0.693 ;IOU accuracy: 0.790 ;Time: 0:01:29\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 10 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:2971 ;lr:0.050 ;Bloss:0.897 ;acc:0.815 ;iou:0.870 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:3021 ;lr:0.050 ;Bloss:1.610 ;acc:0.685 ;iou:0.755 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:3071 ;lr:0.050 ;Bloss:0.409 ;acc:0.940 ;iou:0.970 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:3121 ;lr:0.050 ;Bloss:1.254 ;acc:0.750 ;iou:0.835 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:3171 ;lr:0.050 ;Bloss:1.356 ;acc:0.785 ;iou:0.845 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:3221 ;lr:0.050 ;Bloss:1.421 ;acc:0.705 ;iou:0.780 ;time: 0:00:38\n",
      "\n",
      "*Training B: False ;B Train loss: 1.283 ;Train accuracy: 0.731 ;IOU accuracy: 0.817 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 10\n",
      "batch: 0 ;B loss: 0.566 ;acc: 0.870 ;iou_acc: 0.935 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.718 ;acc: 0.865 ;iou_acc: 0.895 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 1.138 ;acc: 0.775 ;iou_acc: 0.835 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 1.168 ;acc: 0.725 ;iou_acc: 0.795 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.550 ;acc: 0.655 ;iou_acc: 0.770 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 2.013 ;acc: 0.530 ;iou_acc: 0.690 ;time: 0:01:17\n",
      "\n",
      "*BTrain: False ;Test loss: 1.357 ;Test accuracy 0.694 ;IOU accuracy: 0.787 ;Time: 0:01:27\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 11 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:3268 ;lr:0.050 ;Bloss:1.470 ;acc:0.695 ;iou:0.770 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:3318 ;lr:0.050 ;Bloss:0.916 ;acc:0.805 ;iou:0.870 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:3368 ;lr:0.050 ;Bloss:0.679 ;acc:0.890 ;iou:0.920 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:3418 ;lr:0.050 ;Bloss:1.167 ;acc:0.775 ;iou:0.850 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:3468 ;lr:0.050 ;Bloss:0.493 ;acc:0.910 ;iou:0.950 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:3518 ;lr:0.050 ;Bloss:2.398 ;acc:0.455 ;iou:0.635 ;time: 0:00:39\n",
      "\n",
      "*Training B: False ;B Train loss: 1.264 ;Train accuracy: 0.737 ;IOU accuracy: 0.822 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 11\n",
      "batch: 0 ;B loss: 0.522 ;acc: 0.885 ;iou_acc: 0.945 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.679 ;acc: 0.870 ;iou_acc: 0.900 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 1.105 ;acc: 0.780 ;iou_acc: 0.840 ;time: 0:00:55\n",
      "batch: 150 ;B loss: 1.117 ;acc: 0.725 ;iou_acc: 0.790 ;time: 0:01:01\n",
      "batch: 200 ;B loss: 1.522 ;acc: 0.665 ;iou_acc: 0.800 ;time: 0:01:08\n",
      "batch: 250 ;B loss: 1.964 ;acc: 0.570 ;iou_acc: 0.715 ;time: 0:01:16\n",
      "\n",
      "*BTrain: False ;Test loss: 1.318 ;Test accuracy 0.702 ;IOU accuracy: 0.794 ;Time: 0:01:26\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 12 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:3565 ;lr:0.050 ;Bloss:1.312 ;acc:0.725 ;iou:0.775 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:3615 ;lr:0.050 ;Bloss:1.452 ;acc:0.690 ;iou:0.805 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:3665 ;lr:0.050 ;Bloss:1.609 ;acc:0.610 ;iou:0.740 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:3715 ;lr:0.050 ;Bloss:1.071 ;acc:0.805 ;iou:0.865 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:3765 ;lr:0.050 ;Bloss:2.121 ;acc:0.500 ;iou:0.660 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:3815 ;lr:0.050 ;Bloss:0.499 ;acc:0.925 ;iou:0.940 ;time: 0:00:38\n",
      "\n",
      "*Training B: True ;B Train loss: 1.239 ;Train accuracy: 0.743 ;IOU accuracy: 0.827 ;Time: 0:00:45 \n",
      "\n",
      "Testing, ephoc: 12\n",
      "batch: 0 ;B loss: 0.509 ;acc: 0.885 ;iou_acc: 0.940 ;time: 0:00:46\n",
      "batch: 50 ;B loss: 0.690 ;acc: 0.865 ;iou_acc: 0.895 ;time: 0:00:50\n",
      "batch: 100 ;B loss: 1.108 ;acc: 0.795 ;iou_acc: 0.845 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 1.117 ;acc: 0.725 ;iou_acc: 0.800 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.579 ;acc: 0.645 ;iou_acc: 0.780 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 2.004 ;acc: 0.560 ;iou_acc: 0.680 ;time: 0:01:18\n",
      "\n",
      "*BTrain: True ;Test loss: 1.324 ;Test accuracy 0.702 ;IOU accuracy: 0.795 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 13 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:3862 ;lr:0.050 ;Bloss:1.691 ;acc:0.630 ;iou:0.735 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:3912 ;lr:0.050 ;Bloss:2.002 ;acc:0.610 ;iou:0.775 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:3962 ;lr:0.050 ;Bloss:1.483 ;acc:0.665 ;iou:0.795 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:4012 ;lr:0.050 ;Bloss:0.609 ;acc:0.910 ;iou:0.935 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:4062 ;lr:0.050 ;Bloss:1.783 ;acc:0.645 ;iou:0.780 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:4112 ;lr:0.050 ;Bloss:1.065 ;acc:0.800 ;iou:0.880 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 1.220 ;Train accuracy: 0.749 ;IOU accuracy: 0.831 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 13\n",
      "batch: 0 ;B loss: 0.484 ;acc: 0.905 ;iou_acc: 0.950 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.703 ;acc: 0.850 ;iou_acc: 0.885 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 1.117 ;acc: 0.800 ;iou_acc: 0.845 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 1.144 ;acc: 0.745 ;iou_acc: 0.820 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.488 ;acc: 0.655 ;iou_acc: 0.795 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.975 ;acc: 0.530 ;iou_acc: 0.665 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.309 ;Test accuracy 0.705 ;IOU accuracy: 0.795 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 14 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:4159 ;lr:0.050 ;Bloss:0.919 ;acc:0.820 ;iou:0.875 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:4209 ;lr:0.050 ;Bloss:0.717 ;acc:0.870 ;iou:0.915 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:4259 ;lr:0.050 ;Bloss:1.380 ;acc:0.740 ;iou:0.845 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:4309 ;lr:0.050 ;Bloss:0.952 ;acc:0.800 ;iou:0.875 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:4359 ;lr:0.050 ;Bloss:2.341 ;acc:0.505 ;iou:0.685 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:4409 ;lr:0.050 ;Bloss:2.415 ;acc:0.495 ;iou:0.670 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 1.200 ;Train accuracy: 0.753 ;IOU accuracy: 0.835 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 14\n",
      "batch: 0 ;B loss: 0.473 ;acc: 0.910 ;iou_acc: 0.960 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.711 ;acc: 0.835 ;iou_acc: 0.870 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 1.111 ;acc: 0.785 ;iou_acc: 0.835 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 1.133 ;acc: 0.755 ;iou_acc: 0.820 ;time: 0:01:04\n",
      "batch: 200 ;B loss: 1.491 ;acc: 0.645 ;iou_acc: 0.785 ;time: 0:01:12\n",
      "batch: 250 ;B loss: 1.976 ;acc: 0.525 ;iou_acc: 0.675 ;time: 0:01:21\n",
      "\n",
      "*BTrain: False ;Test loss: 1.307 ;Test accuracy 0.704 ;IOU accuracy: 0.796 ;Time: 0:01:31\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 15 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:4456 ;lr:0.050 ;Bloss:1.253 ;acc:0.780 ;iou:0.845 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:4506 ;lr:0.050 ;Bloss:1.596 ;acc:0.655 ;iou:0.755 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:4556 ;lr:0.050 ;Bloss:1.765 ;acc:0.580 ;iou:0.715 ;time: 0:00:18\n",
      "Edit:False ;batch:150 ;gs:4606 ;lr:0.050 ;Bloss:1.421 ;acc:0.720 ;iou:0.780 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:4656 ;lr:0.050 ;Bloss:0.585 ;acc:0.900 ;iou:0.925 ;time: 0:00:35\n",
      "Edit:False ;batch:250 ;gs:4706 ;lr:0.050 ;Bloss:1.008 ;acc:0.810 ;iou:0.865 ;time: 0:00:42\n",
      "\n",
      "*Training B: True ;B Train loss: 1.183 ;Train accuracy: 0.757 ;IOU accuracy: 0.838 ;Time: 0:00:51 \n",
      "\n",
      "Testing, ephoc: 15\n",
      "batch: 0 ;B loss: 0.593 ;acc: 0.870 ;iou_acc: 0.915 ;time: 0:00:52\n",
      "batch: 50 ;B loss: 0.774 ;acc: 0.835 ;iou_acc: 0.870 ;time: 0:00:56\n",
      "batch: 100 ;B loss: 1.158 ;acc: 0.750 ;iou_acc: 0.835 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 1.241 ;acc: 0.715 ;iou_acc: 0.795 ;time: 0:01:08\n",
      "batch: 200 ;B loss: 1.571 ;acc: 0.650 ;iou_acc: 0.775 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 2.202 ;acc: 0.510 ;iou_acc: 0.625 ;time: 0:01:24\n",
      "\n",
      "*BTrain: True ;Test loss: 1.458 ;Test accuracy 0.672 ;IOU accuracy: 0.766 ;Time: 0:01:35\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 16 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:4753 ;lr:0.050 ;Bloss:1.458 ;acc:0.705 ;iou:0.795 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:4803 ;lr:0.050 ;Bloss:0.885 ;acc:0.825 ;iou:0.890 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:4853 ;lr:0.050 ;Bloss:1.635 ;acc:0.675 ;iou:0.785 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:4903 ;lr:0.050 ;Bloss:0.899 ;acc:0.820 ;iou:0.890 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:4953 ;lr:0.050 ;Bloss:1.887 ;acc:0.565 ;iou:0.705 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:5003 ;lr:0.050 ;Bloss:0.423 ;acc:0.930 ;iou:0.970 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 1.162 ;Train accuracy: 0.764 ;IOU accuracy: 0.844 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 16\n",
      "batch: 0 ;B loss: 0.507 ;acc: 0.890 ;iou_acc: 0.945 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.679 ;acc: 0.870 ;iou_acc: 0.900 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 1.092 ;acc: 0.780 ;iou_acc: 0.835 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 1.073 ;acc: 0.755 ;iou_acc: 0.800 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.504 ;acc: 0.635 ;iou_acc: 0.765 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.985 ;acc: 0.520 ;iou_acc: 0.670 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.307 ;Test accuracy 0.706 ;IOU accuracy: 0.795 ;Time: 0:01:27\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 17 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:5050 ;lr:0.050 ;Bloss:2.284 ;acc:0.490 ;iou:0.675 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:5100 ;lr:0.050 ;Bloss:1.337 ;acc:0.760 ;iou:0.840 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:5150 ;lr:0.050 ;Bloss:0.840 ;acc:0.840 ;iou:0.915 ;time: 0:00:15\n",
      "Edit:False ;batch:150 ;gs:5200 ;lr:0.050 ;Bloss:1.010 ;acc:0.800 ;iou:0.880 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:5250 ;lr:0.050 ;Bloss:0.991 ;acc:0.805 ;iou:0.865 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:5300 ;lr:0.050 ;Bloss:1.896 ;acc:0.580 ;iou:0.720 ;time: 0:00:38\n",
      "\n",
      "*Training B: False ;B Train loss: 1.150 ;Train accuracy: 0.768 ;IOU accuracy: 0.846 ;Time: 0:00:46 \n",
      "\n",
      "Testing, ephoc: 17\n",
      "batch: 0 ;B loss: 0.472 ;acc: 0.895 ;iou_acc: 0.950 ;time: 0:00:46\n",
      "batch: 50 ;B loss: 0.651 ;acc: 0.855 ;iou_acc: 0.885 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 1.051 ;acc: 0.795 ;iou_acc: 0.845 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 1.072 ;acc: 0.735 ;iou_acc: 0.805 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.460 ;acc: 0.655 ;iou_acc: 0.805 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.910 ;acc: 0.550 ;iou_acc: 0.695 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.257 ;Test accuracy 0.718 ;IOU accuracy: 0.807 ;Time: 0:01:29\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 18 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:5347 ;lr:0.050 ;Bloss:0.533 ;acc:0.905 ;iou:0.950 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:5397 ;lr:0.050 ;Bloss:2.283 ;acc:0.550 ;iou:0.715 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:5447 ;lr:0.050 ;Bloss:1.386 ;acc:0.720 ;iou:0.815 ;time: 0:00:18\n",
      "Edit:False ;batch:150 ;gs:5497 ;lr:0.050 ;Bloss:1.759 ;acc:0.620 ;iou:0.750 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:5547 ;lr:0.050 ;Bloss:1.305 ;acc:0.725 ;iou:0.815 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:5597 ;lr:0.050 ;Bloss:0.385 ;acc:0.940 ;iou:0.970 ;time: 0:00:43\n",
      "\n",
      "*Training B: True ;B Train loss: 1.131 ;Train accuracy: 0.773 ;IOU accuracy: 0.850 ;Time: 0:00:51 \n",
      "\n",
      "Testing, ephoc: 18\n",
      "batch: 0 ;B loss: 0.491 ;acc: 0.895 ;iou_acc: 0.945 ;time: 0:00:52\n",
      "batch: 50 ;B loss: 0.659 ;acc: 0.850 ;iou_acc: 0.880 ;time: 0:00:56\n",
      "batch: 100 ;B loss: 1.036 ;acc: 0.805 ;iou_acc: 0.860 ;time: 0:01:02\n",
      "batch: 150 ;B loss: 1.061 ;acc: 0.750 ;iou_acc: 0.820 ;time: 0:01:08\n",
      "batch: 200 ;B loss: 1.368 ;acc: 0.690 ;iou_acc: 0.820 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 1.927 ;acc: 0.560 ;iou_acc: 0.725 ;time: 0:01:24\n",
      "\n",
      "*BTrain: True ;Test loss: 1.260 ;Test accuracy 0.717 ;IOU accuracy: 0.806 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 19 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:5644 ;lr:0.050 ;Bloss:0.473 ;acc:0.905 ;iou:0.925 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:5694 ;lr:0.050 ;Bloss:2.301 ;acc:0.530 ;iou:0.695 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:5744 ;lr:0.050 ;Bloss:1.350 ;acc:0.735 ;iou:0.805 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:5794 ;lr:0.050 ;Bloss:1.879 ;acc:0.610 ;iou:0.730 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:5844 ;lr:0.050 ;Bloss:1.800 ;acc:0.605 ;iou:0.745 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:5894 ;lr:0.050 ;Bloss:2.153 ;acc:0.570 ;iou:0.690 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 1.116 ;Train accuracy: 0.776 ;IOU accuracy: 0.853 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 19\n",
      "batch: 0 ;B loss: 0.453 ;acc: 0.900 ;iou_acc: 0.955 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.644 ;acc: 0.860 ;iou_acc: 0.890 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 1.020 ;acc: 0.820 ;iou_acc: 0.865 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 1.057 ;acc: 0.735 ;iou_acc: 0.805 ;time: 0:01:04\n",
      "batch: 200 ;B loss: 1.405 ;acc: 0.675 ;iou_acc: 0.825 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 1.893 ;acc: 0.545 ;iou_acc: 0.720 ;time: 0:01:20\n",
      "\n",
      "*BTrain: False ;Test loss: 1.238 ;Test accuracy 0.724 ;IOU accuracy: 0.811 ;Time: 0:01:31\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 20 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:5941 ;lr:0.050 ;Bloss:1.440 ;acc:0.715 ;iou:0.825 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:5991 ;lr:0.050 ;Bloss:1.901 ;acc:0.610 ;iou:0.730 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:6041 ;lr:0.050 ;Bloss:1.563 ;acc:0.690 ;iou:0.825 ;time: 0:00:18\n",
      "Edit:False ;batch:150 ;gs:6091 ;lr:0.050 ;Bloss:1.196 ;acc:0.780 ;iou:0.840 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:6141 ;lr:0.050 ;Bloss:0.849 ;acc:0.850 ;iou:0.915 ;time: 0:00:35\n",
      "Edit:False ;batch:250 ;gs:6191 ;lr:0.050 ;Bloss:1.392 ;acc:0.735 ;iou:0.830 ;time: 0:00:43\n",
      "\n",
      "*Training B: False ;B Train loss: 1.099 ;Train accuracy: 0.783 ;IOU accuracy: 0.857 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 20\n",
      "batch: 0 ;B loss: 0.480 ;acc: 0.895 ;iou_acc: 0.945 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.626 ;acc: 0.870 ;iou_acc: 0.895 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 1.002 ;acc: 0.825 ;iou_acc: 0.870 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 1.019 ;acc: 0.760 ;iou_acc: 0.830 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.416 ;acc: 0.680 ;iou_acc: 0.800 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.900 ;acc: 0.545 ;iou_acc: 0.695 ;time: 0:01:23\n",
      "\n",
      "*BTrain: False ;Test loss: 1.235 ;Test accuracy 0.725 ;IOU accuracy: 0.812 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 21 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:6238 ;lr:0.050 ;Bloss:0.365 ;acc:0.925 ;iou:0.950 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:6288 ;lr:0.050 ;Bloss:1.211 ;acc:0.740 ;iou:0.830 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:6338 ;lr:0.050 ;Bloss:3.171 ;acc:0.290 ;iou:0.515 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:6388 ;lr:0.050 ;Bloss:1.747 ;acc:0.640 ;iou:0.770 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:6438 ;lr:0.050 ;Bloss:0.994 ;acc:0.850 ;iou:0.900 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:6488 ;lr:0.050 ;Bloss:0.854 ;acc:0.840 ;iou:0.905 ;time: 0:00:42\n",
      "\n",
      "*Training B: True ;B Train loss: 1.086 ;Train accuracy: 0.785 ;IOU accuracy: 0.860 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 21\n",
      "batch: 0 ;B loss: 0.450 ;acc: 0.900 ;iou_acc: 0.950 ;time: 0:00:50\n",
      "batch: 50 ;B loss: 0.641 ;acc: 0.860 ;iou_acc: 0.885 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 1.010 ;acc: 0.805 ;iou_acc: 0.860 ;time: 0:01:00\n",
      "batch: 150 ;B loss: 1.029 ;acc: 0.740 ;iou_acc: 0.815 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.368 ;acc: 0.725 ;iou_acc: 0.835 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.890 ;acc: 0.540 ;iou_acc: 0.705 ;time: 0:01:23\n",
      "\n",
      "*BTrain: True ;Test loss: 1.221 ;Test accuracy 0.728 ;IOU accuracy: 0.814 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 22 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:6535 ;lr:0.050 ;Bloss:0.562 ;acc:0.905 ;iou:0.945 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:6585 ;lr:0.050 ;Bloss:1.768 ;acc:0.680 ;iou:0.840 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:6635 ;lr:0.050 ;Bloss:1.994 ;acc:0.550 ;iou:0.725 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:6685 ;lr:0.050 ;Bloss:1.385 ;acc:0.690 ;iou:0.790 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:6735 ;lr:0.050 ;Bloss:0.855 ;acc:0.860 ;iou:0.910 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:6785 ;lr:0.050 ;Bloss:1.747 ;acc:0.625 ;iou:0.750 ;time: 0:00:42\n",
      "\n",
      "*Training B: False ;B Train loss: 1.075 ;Train accuracy: 0.788 ;IOU accuracy: 0.862 ;Time: 0:00:49 \n",
      "\n",
      "Testing, ephoc: 22\n",
      "batch: 0 ;B loss: 0.436 ;acc: 0.900 ;iou_acc: 0.955 ;time: 0:00:50\n",
      "batch: 50 ;B loss: 0.649 ;acc: 0.850 ;iou_acc: 0.885 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 1.024 ;acc: 0.805 ;iou_acc: 0.850 ;time: 0:01:00\n",
      "batch: 150 ;B loss: 1.048 ;acc: 0.755 ;iou_acc: 0.825 ;time: 0:01:06\n",
      "batch: 200 ;B loss: 1.405 ;acc: 0.670 ;iou_acc: 0.820 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.908 ;acc: 0.530 ;iou_acc: 0.680 ;time: 0:01:23\n",
      "\n",
      "*BTrain: False ;Test loss: 1.234 ;Test accuracy 0.724 ;IOU accuracy: 0.811 ;Time: 0:01:33\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 23 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:6832 ;lr:0.050 ;Bloss:1.267 ;acc:0.765 ;iou:0.860 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:6882 ;lr:0.050 ;Bloss:1.208 ;acc:0.765 ;iou:0.850 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:6932 ;lr:0.050 ;Bloss:1.462 ;acc:0.735 ;iou:0.805 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:6982 ;lr:0.050 ;Bloss:0.852 ;acc:0.880 ;iou:0.940 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:7032 ;lr:0.050 ;Bloss:1.310 ;acc:0.740 ;iou:0.825 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:7082 ;lr:0.050 ;Bloss:1.288 ;acc:0.725 ;iou:0.800 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 1.059 ;Train accuracy: 0.794 ;IOU accuracy: 0.865 ;Time: 0:00:49 \n",
      "\n",
      "Testing, ephoc: 23\n",
      "batch: 0 ;B loss: 0.482 ;acc: 0.885 ;iou_acc: 0.950 ;time: 0:00:50\n",
      "batch: 50 ;B loss: 0.639 ;acc: 0.865 ;iou_acc: 0.890 ;time: 0:00:54\n",
      "batch: 100 ;B loss: 1.021 ;acc: 0.795 ;iou_acc: 0.865 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 1.020 ;acc: 0.775 ;iou_acc: 0.835 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.422 ;acc: 0.665 ;iou_acc: 0.810 ;time: 0:01:13\n",
      "batch: 250 ;B loss: 1.884 ;acc: 0.550 ;iou_acc: 0.705 ;time: 0:01:22\n",
      "\n",
      "*BTrain: False ;Test loss: 1.226 ;Test accuracy 0.728 ;IOU accuracy: 0.813 ;Time: 0:01:33\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 24 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:7129 ;lr:0.050 ;Bloss:1.669 ;acc:0.630 ;iou:0.755 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:7179 ;lr:0.050 ;Bloss:0.842 ;acc:0.850 ;iou:0.895 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:7229 ;lr:0.050 ;Bloss:1.384 ;acc:0.725 ;iou:0.805 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:7279 ;lr:0.050 ;Bloss:1.100 ;acc:0.800 ;iou:0.880 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:7329 ;lr:0.050 ;Bloss:1.072 ;acc:0.800 ;iou:0.850 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:7379 ;lr:0.050 ;Bloss:0.337 ;acc:0.940 ;iou:0.975 ;time: 0:00:42\n",
      "\n",
      "*Training B: True ;B Train loss: 1.049 ;Train accuracy: 0.796 ;IOU accuracy: 0.867 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 24\n",
      "batch: 0 ;B loss: 0.459 ;acc: 0.895 ;iou_acc: 0.945 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.633 ;acc: 0.860 ;iou_acc: 0.885 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.997 ;acc: 0.840 ;iou_acc: 0.890 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 1.036 ;acc: 0.760 ;iou_acc: 0.820 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.324 ;acc: 0.710 ;iou_acc: 0.840 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 1.883 ;acc: 0.550 ;iou_acc: 0.710 ;time: 0:01:24\n",
      "\n",
      "*BTrain: True ;Test loss: 1.209 ;Test accuracy 0.731 ;IOU accuracy: 0.816 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 25 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:7426 ;lr:0.050 ;Bloss:1.092 ;acc:0.790 ;iou:0.865 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:7476 ;lr:0.050 ;Bloss:1.377 ;acc:0.725 ;iou:0.865 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:7526 ;lr:0.050 ;Bloss:1.456 ;acc:0.685 ;iou:0.800 ;time: 0:00:18\n",
      "Edit:False ;batch:150 ;gs:7576 ;lr:0.050 ;Bloss:1.228 ;acc:0.775 ;iou:0.835 ;time: 0:00:27\n",
      "Edit:False ;batch:200 ;gs:7626 ;lr:0.050 ;Bloss:0.986 ;acc:0.840 ;iou:0.865 ;time: 0:00:35\n",
      "Edit:False ;batch:250 ;gs:7676 ;lr:0.050 ;Bloss:1.298 ;acc:0.740 ;iou:0.795 ;time: 0:00:43\n",
      "\n",
      "*Training B: False ;B Train loss: 1.028 ;Train accuracy: 0.800 ;IOU accuracy: 0.870 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 25\n",
      "batch: 0 ;B loss: 0.442 ;acc: 0.900 ;iou_acc: 0.945 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.635 ;acc: 0.870 ;iou_acc: 0.895 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.990 ;acc: 0.820 ;iou_acc: 0.870 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 1.036 ;acc: 0.745 ;iou_acc: 0.815 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.351 ;acc: 0.710 ;iou_acc: 0.840 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 1.872 ;acc: 0.555 ;iou_acc: 0.730 ;time: 0:01:24\n",
      "\n",
      "*BTrain: False ;Test loss: 1.206 ;Test accuracy 0.732 ;IOU accuracy: 0.818 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 26 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:7723 ;lr:0.050 ;Bloss:1.072 ;acc:0.760 ;iou:0.830 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:7773 ;lr:0.050 ;Bloss:0.357 ;acc:0.945 ;iou:0.985 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:7823 ;lr:0.050 ;Bloss:1.713 ;acc:0.665 ;iou:0.755 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:7873 ;lr:0.050 ;Bloss:0.991 ;acc:0.835 ;iou:0.895 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:7923 ;lr:0.050 ;Bloss:1.503 ;acc:0.690 ;iou:0.795 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:7973 ;lr:0.050 ;Bloss:0.334 ;acc:0.935 ;iou:0.960 ;time: 0:00:42\n",
      "\n",
      "*Training B: False ;B Train loss: 1.012 ;Train accuracy: 0.805 ;IOU accuracy: 0.875 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 26\n",
      "batch: 0 ;B loss: 0.448 ;acc: 0.890 ;iou_acc: 0.940 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.617 ;acc: 0.865 ;iou_acc: 0.885 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.973 ;acc: 0.800 ;iou_acc: 0.855 ;time: 0:01:00\n",
      "batch: 150 ;B loss: 0.997 ;acc: 0.780 ;iou_acc: 0.845 ;time: 0:01:06\n",
      "batch: 200 ;B loss: 1.341 ;acc: 0.665 ;iou_acc: 0.835 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.859 ;acc: 0.540 ;iou_acc: 0.700 ;time: 0:01:23\n",
      "\n",
      "*BTrain: False ;Test loss: 1.193 ;Test accuracy 0.734 ;IOU accuracy: 0.819 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 27 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:8020 ;lr:0.050 ;Bloss:0.865 ;acc:0.865 ;iou:0.910 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:8070 ;lr:0.050 ;Bloss:1.151 ;acc:0.740 ;iou:0.825 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:8120 ;lr:0.050 ;Bloss:1.436 ;acc:0.750 ;iou:0.870 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:8170 ;lr:0.050 ;Bloss:1.322 ;acc:0.745 ;iou:0.875 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:8220 ;lr:0.050 ;Bloss:0.311 ;acc:0.945 ;iou:0.970 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:8270 ;lr:0.050 ;Bloss:1.045 ;acc:0.815 ;iou:0.895 ;time: 0:00:42\n",
      "\n",
      "*Training B: True ;B Train loss: 1.005 ;Train accuracy: 0.807 ;IOU accuracy: 0.874 ;Time: 0:00:49 \n",
      "\n",
      "Testing, ephoc: 27\n",
      "batch: 0 ;B loss: 0.444 ;acc: 0.895 ;iou_acc: 0.940 ;time: 0:00:50\n",
      "batch: 50 ;B loss: 0.624 ;acc: 0.860 ;iou_acc: 0.890 ;time: 0:00:54\n",
      "batch: 100 ;B loss: 1.006 ;acc: 0.825 ;iou_acc: 0.875 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 1.043 ;acc: 0.765 ;iou_acc: 0.820 ;time: 0:01:06\n",
      "batch: 200 ;B loss: 1.337 ;acc: 0.690 ;iou_acc: 0.825 ;time: 0:01:13\n",
      "batch: 250 ;B loss: 1.872 ;acc: 0.550 ;iou_acc: 0.710 ;time: 0:01:23\n",
      "\n",
      "*BTrain: True ;Test loss: 1.189 ;Test accuracy 0.736 ;IOU accuracy: 0.821 ;Time: 0:01:33\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 28 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:8317 ;lr:0.050 ;Bloss:1.515 ;acc:0.710 ;iou:0.795 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:8367 ;lr:0.050 ;Bloss:0.527 ;acc:0.885 ;iou:0.945 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:8417 ;lr:0.050 ;Bloss:0.996 ;acc:0.815 ;iou:0.855 ;time: 0:00:18\n",
      "Edit:False ;batch:150 ;gs:8467 ;lr:0.050 ;Bloss:2.079 ;acc:0.570 ;iou:0.755 ;time: 0:00:27\n",
      "Edit:False ;batch:200 ;gs:8517 ;lr:0.050 ;Bloss:0.743 ;acc:0.855 ;iou:0.920 ;time: 0:00:35\n",
      "Edit:False ;batch:250 ;gs:8567 ;lr:0.050 ;Bloss:0.453 ;acc:0.945 ;iou:0.955 ;time: 0:00:44\n",
      "\n",
      "*Training B: False ;B Train loss: 0.987 ;Train accuracy: 0.811 ;IOU accuracy: 0.878 ;Time: 0:00:51 \n",
      "\n",
      "Testing, ephoc: 28\n",
      "batch: 0 ;B loss: 0.469 ;acc: 0.900 ;iou_acc: 0.945 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.601 ;acc: 0.890 ;iou_acc: 0.910 ;time: 0:00:56\n",
      "batch: 100 ;B loss: 0.979 ;acc: 0.800 ;iou_acc: 0.860 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 0.967 ;acc: 0.815 ;iou_acc: 0.840 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.365 ;acc: 0.685 ;iou_acc: 0.820 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 1.847 ;acc: 0.565 ;iou_acc: 0.725 ;time: 0:01:24\n",
      "\n",
      "*BTrain: False ;Test loss: 1.186 ;Test accuracy 0.739 ;IOU accuracy: 0.822 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 29 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:8614 ;lr:0.050 ;Bloss:0.372 ;acc:0.940 ;iou:0.960 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:8664 ;lr:0.050 ;Bloss:2.513 ;acc:0.505 ;iou:0.710 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:8714 ;lr:0.050 ;Bloss:1.719 ;acc:0.635 ;iou:0.730 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:8764 ;lr:0.050 ;Bloss:1.349 ;acc:0.720 ;iou:0.835 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:8814 ;lr:0.050 ;Bloss:1.324 ;acc:0.715 ;iou:0.785 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:8864 ;lr:0.050 ;Bloss:0.483 ;acc:0.950 ;iou:0.965 ;time: 0:00:42\n",
      "\n",
      "*Training B: False ;B Train loss: 0.977 ;Train accuracy: 0.814 ;IOU accuracy: 0.880 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 29\n",
      "batch: 0 ;B loss: 0.426 ;acc: 0.905 ;iou_acc: 0.945 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.622 ;acc: 0.855 ;iou_acc: 0.880 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.983 ;acc: 0.810 ;iou_acc: 0.865 ;time: 0:01:00\n",
      "batch: 150 ;B loss: 0.997 ;acc: 0.785 ;iou_acc: 0.850 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.303 ;acc: 0.710 ;iou_acc: 0.835 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.885 ;acc: 0.565 ;iou_acc: 0.720 ;time: 0:01:23\n",
      "\n",
      "*BTrain: False ;Test loss: 1.179 ;Test accuracy 0.740 ;IOU accuracy: 0.824 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 30 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:8911 ;lr:0.050 ;Bloss:0.366 ;acc:0.940 ;iou:0.975 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:8961 ;lr:0.050 ;Bloss:0.557 ;acc:0.910 ;iou:0.955 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:9011 ;lr:0.050 ;Bloss:0.513 ;acc:0.905 ;iou:0.965 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:9061 ;lr:0.050 ;Bloss:1.055 ;acc:0.775 ;iou:0.850 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:9111 ;lr:0.050 ;Bloss:0.696 ;acc:0.875 ;iou:0.900 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:9161 ;lr:0.050 ;Bloss:0.744 ;acc:0.880 ;iou:0.960 ;time: 0:00:43\n",
      "\n",
      "*Training B: True ;B Train loss: 0.957 ;Train accuracy: 0.820 ;IOU accuracy: 0.885 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 30\n",
      "batch: 0 ;B loss: 0.424 ;acc: 0.885 ;iou_acc: 0.945 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.628 ;acc: 0.870 ;iou_acc: 0.895 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 1.011 ;acc: 0.810 ;iou_acc: 0.870 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 1.011 ;acc: 0.780 ;iou_acc: 0.835 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.346 ;acc: 0.675 ;iou_acc: 0.825 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.888 ;acc: 0.565 ;iou_acc: 0.720 ;time: 0:01:23\n",
      "\n",
      "*BTrain: True ;Test loss: 1.195 ;Test accuracy 0.735 ;IOU accuracy: 0.820 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 31 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:9208 ;lr:0.050 ;Bloss:0.656 ;acc:0.880 ;iou:0.925 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:9258 ;lr:0.050 ;Bloss:0.702 ;acc:0.895 ;iou:0.920 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:9308 ;lr:0.050 ;Bloss:0.422 ;acc:0.940 ;iou:0.960 ;time: 0:00:18\n",
      "Edit:False ;batch:150 ;gs:9358 ;lr:0.050 ;Bloss:0.476 ;acc:0.930 ;iou:0.940 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:9408 ;lr:0.050 ;Bloss:0.442 ;acc:0.930 ;iou:0.955 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:9458 ;lr:0.050 ;Bloss:1.048 ;acc:0.785 ;iou:0.870 ;time: 0:00:43\n",
      "\n",
      "*Training B: False ;B Train loss: 0.945 ;Train accuracy: 0.823 ;IOU accuracy: 0.887 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 31\n",
      "batch: 0 ;B loss: 0.456 ;acc: 0.900 ;iou_acc: 0.945 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.605 ;acc: 0.875 ;iou_acc: 0.900 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.979 ;acc: 0.810 ;iou_acc: 0.870 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 0.992 ;acc: 0.775 ;iou_acc: 0.820 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.293 ;acc: 0.685 ;iou_acc: 0.830 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.849 ;acc: 0.555 ;iou_acc: 0.710 ;time: 0:01:23\n",
      "\n",
      "*BTrain: False ;Test loss: 1.169 ;Test accuracy 0.743 ;IOU accuracy: 0.826 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 32 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:9505 ;lr:0.050 ;Bloss:1.212 ;acc:0.775 ;iou:0.825 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:9555 ;lr:0.050 ;Bloss:0.478 ;acc:0.940 ;iou:0.970 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:9605 ;lr:0.050 ;Bloss:1.922 ;acc:0.580 ;iou:0.735 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:9655 ;lr:0.050 ;Bloss:0.446 ;acc:0.940 ;iou:0.965 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:9705 ;lr:0.050 ;Bloss:1.210 ;acc:0.750 ;iou:0.860 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:9755 ;lr:0.050 ;Bloss:1.534 ;acc:0.720 ;iou:0.810 ;time: 0:00:43\n",
      "\n",
      "*Training B: False ;B Train loss: 0.930 ;Train accuracy: 0.827 ;IOU accuracy: 0.889 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 32\n",
      "batch: 0 ;B loss: 0.445 ;acc: 0.910 ;iou_acc: 0.945 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.580 ;acc: 0.875 ;iou_acc: 0.900 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.925 ;acc: 0.815 ;iou_acc: 0.875 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 1.019 ;acc: 0.775 ;iou_acc: 0.840 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.329 ;acc: 0.695 ;iou_acc: 0.830 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.851 ;acc: 0.550 ;iou_acc: 0.700 ;time: 0:01:23\n",
      "\n",
      "*BTrain: False ;Test loss: 1.164 ;Test accuracy 0.744 ;IOU accuracy: 0.826 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 33 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:9802 ;lr:0.050 ;Bloss:0.666 ;acc:0.895 ;iou:0.915 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:9852 ;lr:0.050 ;Bloss:0.660 ;acc:0.910 ;iou:0.930 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:9902 ;lr:0.050 ;Bloss:0.808 ;acc:0.860 ;iou:0.920 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:9952 ;lr:0.050 ;Bloss:0.734 ;acc:0.860 ;iou:0.920 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:10002 ;lr:0.045 ;Bloss:1.617 ;acc:0.675 ;iou:0.775 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:10052 ;lr:0.045 ;Bloss:1.202 ;acc:0.800 ;iou:0.880 ;time: 0:00:42\n",
      "\n",
      "*Training B: True ;B Train loss: 0.919 ;Train accuracy: 0.829 ;IOU accuracy: 0.891 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 33\n",
      "batch: 0 ;B loss: 0.448 ;acc: 0.895 ;iou_acc: 0.950 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.624 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 1.030 ;acc: 0.790 ;iou_acc: 0.855 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 0.996 ;acc: 0.770 ;iou_acc: 0.805 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.368 ;acc: 0.665 ;iou_acc: 0.800 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.860 ;acc: 0.550 ;iou_acc: 0.705 ;time: 0:01:23\n",
      "\n",
      "*BTrain: True ;Test loss: 1.196 ;Test accuracy 0.739 ;IOU accuracy: 0.820 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 34 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:10099 ;lr:0.045 ;Bloss:1.351 ;acc:0.730 ;iou:0.820 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:10149 ;lr:0.045 ;Bloss:0.644 ;acc:0.880 ;iou:0.940 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:10199 ;lr:0.045 ;Bloss:1.101 ;acc:0.795 ;iou:0.885 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:10249 ;lr:0.045 ;Bloss:0.449 ;acc:0.930 ;iou:0.955 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:10299 ;lr:0.045 ;Bloss:1.030 ;acc:0.785 ;iou:0.870 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:10349 ;lr:0.045 ;Bloss:0.840 ;acc:0.830 ;iou:0.900 ;time: 0:00:42\n",
      "\n",
      "*Training B: False ;B Train loss: 0.896 ;Train accuracy: 0.835 ;IOU accuracy: 0.895 ;Time: 0:00:51 \n",
      "\n",
      "Testing, ephoc: 34\n",
      "batch: 0 ;B loss: 0.450 ;acc: 0.895 ;iou_acc: 0.940 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.609 ;acc: 0.870 ;iou_acc: 0.900 ;time: 0:00:56\n",
      "batch: 100 ;B loss: 0.936 ;acc: 0.830 ;iou_acc: 0.890 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 0.973 ;acc: 0.770 ;iou_acc: 0.840 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.282 ;acc: 0.725 ;iou_acc: 0.840 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 1.832 ;acc: 0.570 ;iou_acc: 0.730 ;time: 0:01:23\n",
      "\n",
      "*BTrain: False ;Test loss: 1.147 ;Test accuracy 0.750 ;IOU accuracy: 0.831 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 35 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:10396 ;lr:0.045 ;Bloss:1.474 ;acc:0.710 ;iou:0.815 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:10446 ;lr:0.045 ;Bloss:0.886 ;acc:0.865 ;iou:0.910 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:10496 ;lr:0.045 ;Bloss:0.649 ;acc:0.880 ;iou:0.925 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:10546 ;lr:0.045 ;Bloss:1.880 ;acc:0.580 ;iou:0.740 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:10596 ;lr:0.045 ;Bloss:1.052 ;acc:0.785 ;iou:0.860 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:10646 ;lr:0.045 ;Bloss:0.622 ;acc:0.915 ;iou:0.945 ;time: 0:00:42\n",
      "\n",
      "*Training B: False ;B Train loss: 0.887 ;Train accuracy: 0.837 ;IOU accuracy: 0.897 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 35\n",
      "batch: 0 ;B loss: 0.466 ;acc: 0.905 ;iou_acc: 0.945 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.602 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.938 ;acc: 0.830 ;iou_acc: 0.890 ;time: 0:01:00\n",
      "batch: 150 ;B loss: 0.952 ;acc: 0.800 ;iou_acc: 0.855 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.294 ;acc: 0.690 ;iou_acc: 0.835 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.847 ;acc: 0.555 ;iou_acc: 0.710 ;time: 0:01:23\n",
      "\n",
      "*BTrain: False ;Test loss: 1.143 ;Test accuracy 0.751 ;IOU accuracy: 0.831 ;Time: 0:01:33\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 36 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:10693 ;lr:0.045 ;Bloss:1.065 ;acc:0.795 ;iou:0.845 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:10743 ;lr:0.045 ;Bloss:0.532 ;acc:0.915 ;iou:0.940 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:10793 ;lr:0.045 ;Bloss:0.647 ;acc:0.910 ;iou:0.965 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:10843 ;lr:0.045 ;Bloss:0.389 ;acc:0.935 ;iou:0.965 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:10893 ;lr:0.045 ;Bloss:0.566 ;acc:0.905 ;iou:0.935 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:10943 ;lr:0.045 ;Bloss:1.017 ;acc:0.830 ;iou:0.890 ;time: 0:00:42\n",
      "\n",
      "*Training B: True ;B Train loss: 0.877 ;Train accuracy: 0.839 ;IOU accuracy: 0.898 ;Time: 0:00:51 \n",
      "\n",
      "Testing, ephoc: 36\n",
      "batch: 0 ;B loss: 0.419 ;acc: 0.910 ;iou_acc: 0.950 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.622 ;acc: 0.880 ;iou_acc: 0.910 ;time: 0:00:56\n",
      "batch: 100 ;B loss: 0.956 ;acc: 0.810 ;iou_acc: 0.860 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 0.997 ;acc: 0.760 ;iou_acc: 0.830 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.331 ;acc: 0.685 ;iou_acc: 0.830 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 1.870 ;acc: 0.560 ;iou_acc: 0.715 ;time: 0:01:23\n",
      "\n",
      "*BTrain: True ;Test loss: 1.162 ;Test accuracy 0.746 ;IOU accuracy: 0.828 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 37 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:10990 ;lr:0.045 ;Bloss:0.616 ;acc:0.915 ;iou:0.940 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:11040 ;lr:0.045 ;Bloss:1.415 ;acc:0.730 ;iou:0.815 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:11090 ;lr:0.045 ;Bloss:0.359 ;acc:0.945 ;iou:0.955 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:11140 ;lr:0.045 ;Bloss:0.822 ;acc:0.885 ;iou:0.930 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:11190 ;lr:0.045 ;Bloss:0.846 ;acc:0.860 ;iou:0.905 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:11240 ;lr:0.045 ;Bloss:1.160 ;acc:0.745 ;iou:0.835 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 0.863 ;Train accuracy: 0.842 ;IOU accuracy: 0.901 ;Time: 0:00:49 \n",
      "\n",
      "Testing, ephoc: 37\n",
      "batch: 0 ;B loss: 0.439 ;acc: 0.910 ;iou_acc: 0.950 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.594 ;acc: 0.880 ;iou_acc: 0.905 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.940 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 0.961 ;acc: 0.795 ;iou_acc: 0.855 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.303 ;acc: 0.715 ;iou_acc: 0.835 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.849 ;acc: 0.570 ;iou_acc: 0.725 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.137 ;Test accuracy 0.753 ;IOU accuracy: 0.833 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 38 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:11287 ;lr:0.045 ;Bloss:0.861 ;acc:0.875 ;iou:0.920 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:11337 ;lr:0.045 ;Bloss:0.816 ;acc:0.865 ;iou:0.920 ;time: 0:00:07\n",
      "Edit:False ;batch:100 ;gs:11387 ;lr:0.045 ;Bloss:0.670 ;acc:0.880 ;iou:0.925 ;time: 0:00:14\n",
      "Edit:False ;batch:150 ;gs:11437 ;lr:0.045 ;Bloss:1.136 ;acc:0.795 ;iou:0.860 ;time: 0:00:21\n",
      "Edit:False ;batch:200 ;gs:11487 ;lr:0.045 ;Bloss:1.135 ;acc:0.800 ;iou:0.865 ;time: 0:00:28\n",
      "Edit:False ;batch:250 ;gs:11537 ;lr:0.045 ;Bloss:0.563 ;acc:0.900 ;iou:0.950 ;time: 0:00:37\n",
      "\n",
      "*Training B: False ;B Train loss: 0.855 ;Train accuracy: 0.844 ;IOU accuracy: 0.901 ;Time: 0:00:44 \n",
      "\n",
      "Testing, ephoc: 38\n",
      "batch: 0 ;B loss: 0.468 ;acc: 0.890 ;iou_acc: 0.925 ;time: 0:00:45\n",
      "batch: 50 ;B loss: 0.694 ;acc: 0.865 ;iou_acc: 0.895 ;time: 0:00:49\n",
      "batch: 100 ;B loss: 1.031 ;acc: 0.785 ;iou_acc: 0.840 ;time: 0:00:54\n",
      "batch: 150 ;B loss: 1.074 ;acc: 0.775 ;iou_acc: 0.830 ;time: 0:01:00\n",
      "batch: 200 ;B loss: 1.317 ;acc: 0.735 ;iou_acc: 0.830 ;time: 0:01:07\n",
      "batch: 250 ;B loss: 1.993 ;acc: 0.575 ;iou_acc: 0.690 ;time: 0:01:15\n",
      "\n",
      "*BTrain: False ;Test loss: 1.246 ;Test accuracy 0.732 ;IOU accuracy: 0.812 ;Time: 0:01:25\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 39 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:11584 ;lr:0.045 ;Bloss:0.701 ;acc:0.875 ;iou:0.920 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:11634 ;lr:0.045 ;Bloss:0.892 ;acc:0.865 ;iou:0.930 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:11684 ;lr:0.045 ;Bloss:1.461 ;acc:0.720 ;iou:0.860 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:11734 ;lr:0.045 ;Bloss:1.476 ;acc:0.715 ;iou:0.795 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:11784 ;lr:0.045 ;Bloss:0.345 ;acc:0.955 ;iou:0.965 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:11834 ;lr:0.045 ;Bloss:0.248 ;acc:0.960 ;iou:0.975 ;time: 0:00:41\n",
      "\n",
      "*Training B: True ;B Train loss: 0.842 ;Train accuracy: 0.848 ;IOU accuracy: 0.904 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 39\n",
      "batch: 0 ;B loss: 0.473 ;acc: 0.900 ;iou_acc: 0.940 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.628 ;acc: 0.885 ;iou_acc: 0.910 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.982 ;acc: 0.805 ;iou_acc: 0.865 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.970 ;acc: 0.775 ;iou_acc: 0.815 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.339 ;acc: 0.700 ;iou_acc: 0.815 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.920 ;acc: 0.555 ;iou_acc: 0.720 ;time: 0:01:17\n",
      "\n",
      "*BTrain: True ;Test loss: 1.194 ;Test accuracy 0.743 ;IOU accuracy: 0.822 ;Time: 0:01:27\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 40 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:11881 ;lr:0.045 ;Bloss:0.714 ;acc:0.890 ;iou:0.945 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:11931 ;lr:0.045 ;Bloss:1.127 ;acc:0.795 ;iou:0.895 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:11981 ;lr:0.045 ;Bloss:0.733 ;acc:0.880 ;iou:0.920 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:12031 ;lr:0.045 ;Bloss:0.868 ;acc:0.825 ;iou:0.870 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:12081 ;lr:0.045 ;Bloss:1.097 ;acc:0.820 ;iou:0.875 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:12131 ;lr:0.045 ;Bloss:0.234 ;acc:0.965 ;iou:0.985 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 0.833 ;Train accuracy: 0.849 ;IOU accuracy: 0.905 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 40\n",
      "batch: 0 ;B loss: 0.441 ;acc: 0.910 ;iou_acc: 0.955 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.611 ;acc: 0.880 ;iou_acc: 0.905 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.922 ;acc: 0.805 ;iou_acc: 0.860 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.933 ;acc: 0.805 ;iou_acc: 0.850 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.320 ;acc: 0.685 ;iou_acc: 0.820 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.832 ;acc: 0.555 ;iou_acc: 0.700 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.135 ;Test accuracy 0.753 ;IOU accuracy: 0.833 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 41 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:12178 ;lr:0.045 ;Bloss:0.989 ;acc:0.835 ;iou:0.880 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:12228 ;lr:0.045 ;Bloss:0.217 ;acc:0.975 ;iou:0.980 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:12278 ;lr:0.045 ;Bloss:0.312 ;acc:0.960 ;iou:0.975 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:12328 ;lr:0.045 ;Bloss:0.363 ;acc:0.955 ;iou:0.965 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:12378 ;lr:0.045 ;Bloss:0.286 ;acc:0.955 ;iou:0.970 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:12428 ;lr:0.045 ;Bloss:0.739 ;acc:0.905 ;iou:0.940 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 0.825 ;Train accuracy: 0.851 ;IOU accuracy: 0.907 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 41\n",
      "batch: 0 ;B loss: 0.419 ;acc: 0.910 ;iou_acc: 0.950 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.626 ;acc: 0.875 ;iou_acc: 0.905 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.925 ;acc: 0.820 ;iou_acc: 0.880 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.938 ;acc: 0.805 ;iou_acc: 0.850 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.293 ;acc: 0.705 ;iou_acc: 0.840 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 1.813 ;acc: 0.575 ;iou_acc: 0.710 ;time: 0:01:19\n",
      "\n",
      "*BTrain: False ;Test loss: 1.127 ;Test accuracy 0.757 ;IOU accuracy: 0.836 ;Time: 0:01:30\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 42 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:12475 ;lr:0.045 ;Bloss:0.240 ;acc:0.975 ;iou:0.990 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:12525 ;lr:0.045 ;Bloss:0.565 ;acc:0.905 ;iou:0.935 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:12575 ;lr:0.045 ;Bloss:0.390 ;acc:0.945 ;iou:0.960 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:12625 ;lr:0.045 ;Bloss:0.413 ;acc:0.930 ;iou:0.950 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:12675 ;lr:0.045 ;Bloss:2.026 ;acc:0.545 ;iou:0.745 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:12725 ;lr:0.045 ;Bloss:0.440 ;acc:0.935 ;iou:0.960 ;time: 0:00:41\n",
      "\n",
      "*Training B: True ;B Train loss: 0.816 ;Train accuracy: 0.854 ;IOU accuracy: 0.909 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 42\n",
      "batch: 0 ;B loss: 0.426 ;acc: 0.910 ;iou_acc: 0.950 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.613 ;acc: 0.875 ;iou_acc: 0.905 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 0.913 ;acc: 0.820 ;iou_acc: 0.880 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 0.946 ;acc: 0.810 ;iou_acc: 0.875 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.278 ;acc: 0.735 ;iou_acc: 0.845 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.838 ;acc: 0.585 ;iou_acc: 0.730 ;time: 0:01:18\n",
      "\n",
      "*BTrain: True ;Test loss: 1.122 ;Test accuracy 0.758 ;IOU accuracy: 0.837 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 43 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:12772 ;lr:0.045 ;Bloss:1.042 ;acc:0.800 ;iou:0.870 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:12822 ;lr:0.045 ;Bloss:0.316 ;acc:0.970 ;iou:0.985 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:12872 ;lr:0.045 ;Bloss:1.260 ;acc:0.770 ;iou:0.850 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:12922 ;lr:0.045 ;Bloss:1.025 ;acc:0.825 ;iou:0.895 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:12972 ;lr:0.045 ;Bloss:1.695 ;acc:0.705 ;iou:0.805 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:13022 ;lr:0.045 ;Bloss:1.413 ;acc:0.740 ;iou:0.870 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 0.809 ;Train accuracy: 0.855 ;IOU accuracy: 0.909 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 43\n",
      "batch: 0 ;B loss: 0.435 ;acc: 0.915 ;iou_acc: 0.950 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.610 ;acc: 0.885 ;iou_acc: 0.910 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.913 ;acc: 0.815 ;iou_acc: 0.875 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 0.935 ;acc: 0.810 ;iou_acc: 0.870 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.281 ;acc: 0.725 ;iou_acc: 0.840 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.787 ;acc: 0.600 ;iou_acc: 0.730 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.113 ;Test accuracy 0.759 ;IOU accuracy: 0.837 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 44 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:13069 ;lr:0.045 ;Bloss:0.553 ;acc:0.910 ;iou:0.920 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:13119 ;lr:0.045 ;Bloss:0.383 ;acc:0.950 ;iou:0.990 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:13169 ;lr:0.045 ;Bloss:0.403 ;acc:0.935 ;iou:0.960 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:13219 ;lr:0.045 ;Bloss:0.708 ;acc:0.880 ;iou:0.920 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:13269 ;lr:0.045 ;Bloss:1.348 ;acc:0.720 ;iou:0.815 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:13319 ;lr:0.045 ;Bloss:0.209 ;acc:0.980 ;iou:0.995 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 0.790 ;Train accuracy: 0.861 ;IOU accuracy: 0.913 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 44\n",
      "batch: 0 ;B loss: 0.432 ;acc: 0.905 ;iou_acc: 0.945 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.624 ;acc: 0.875 ;iou_acc: 0.900 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 0.928 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 0.929 ;acc: 0.785 ;iou_acc: 0.840 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.245 ;acc: 0.715 ;iou_acc: 0.835 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.813 ;acc: 0.590 ;iou_acc: 0.730 ;time: 0:01:17\n",
      "\n",
      "*BTrain: False ;Test loss: 1.118 ;Test accuracy 0.760 ;IOU accuracy: 0.838 ;Time: 0:01:27\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 45 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:13366 ;lr:0.045 ;Bloss:1.260 ;acc:0.770 ;iou:0.845 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:13416 ;lr:0.045 ;Bloss:0.595 ;acc:0.925 ;iou:0.960 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:13466 ;lr:0.045 ;Bloss:0.248 ;acc:0.970 ;iou:0.990 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:13516 ;lr:0.045 ;Bloss:0.757 ;acc:0.870 ;iou:0.940 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:13566 ;lr:0.045 ;Bloss:0.407 ;acc:0.950 ;iou:0.975 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:13616 ;lr:0.045 ;Bloss:1.793 ;acc:0.615 ;iou:0.790 ;time: 0:00:38\n",
      "\n",
      "*Training B: True ;B Train loss: 0.788 ;Train accuracy: 0.862 ;IOU accuracy: 0.913 ;Time: 0:00:46 \n",
      "\n",
      "Testing, ephoc: 45\n",
      "batch: 0 ;B loss: 0.417 ;acc: 0.915 ;iou_acc: 0.950 ;time: 0:00:46\n",
      "batch: 50 ;B loss: 0.615 ;acc: 0.880 ;iou_acc: 0.905 ;time: 0:00:50\n",
      "batch: 100 ;B loss: 0.884 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 0.906 ;acc: 0.800 ;iou_acc: 0.855 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.271 ;acc: 0.705 ;iou_acc: 0.820 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.818 ;acc: 0.575 ;iou_acc: 0.725 ;time: 0:01:17\n",
      "\n",
      "*BTrain: True ;Test loss: 1.116 ;Test accuracy 0.760 ;IOU accuracy: 0.837 ;Time: 0:01:27\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 46 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:13663 ;lr:0.045 ;Bloss:0.389 ;acc:0.950 ;iou:0.965 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:13713 ;lr:0.045 ;Bloss:1.178 ;acc:0.760 ;iou:0.880 ;time: 0:00:07\n",
      "Edit:False ;batch:100 ;gs:13763 ;lr:0.045 ;Bloss:0.294 ;acc:0.955 ;iou:0.985 ;time: 0:00:15\n",
      "Edit:False ;batch:150 ;gs:13813 ;lr:0.045 ;Bloss:1.585 ;acc:0.690 ;iou:0.805 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:13863 ;lr:0.045 ;Bloss:0.425 ;acc:0.950 ;iou:0.975 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:13913 ;lr:0.045 ;Bloss:0.809 ;acc:0.865 ;iou:0.935 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 0.776 ;Train accuracy: 0.864 ;IOU accuracy: 0.915 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 46\n",
      "batch: 0 ;B loss: 0.434 ;acc: 0.905 ;iou_acc: 0.950 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.630 ;acc: 0.880 ;iou_acc: 0.905 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.912 ;acc: 0.810 ;iou_acc: 0.870 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 0.901 ;acc: 0.800 ;iou_acc: 0.845 ;time: 0:01:04\n",
      "batch: 200 ;B loss: 1.281 ;acc: 0.700 ;iou_acc: 0.825 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 1.819 ;acc: 0.580 ;iou_acc: 0.730 ;time: 0:01:19\n",
      "\n",
      "*BTrain: False ;Test loss: 1.122 ;Test accuracy 0.759 ;IOU accuracy: 0.838 ;Time: 0:01:29\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 47 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:13960 ;lr:0.045 ;Bloss:0.512 ;acc:0.925 ;iou:0.950 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:14010 ;lr:0.045 ;Bloss:1.751 ;acc:0.635 ;iou:0.800 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:14060 ;lr:0.045 ;Bloss:1.692 ;acc:0.685 ;iou:0.775 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:14110 ;lr:0.045 ;Bloss:1.044 ;acc:0.795 ;iou:0.900 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:14160 ;lr:0.045 ;Bloss:0.342 ;acc:0.950 ;iou:0.980 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:14210 ;lr:0.045 ;Bloss:0.264 ;acc:0.965 ;iou:0.980 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 0.766 ;Train accuracy: 0.867 ;IOU accuracy: 0.917 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 47\n",
      "batch: 0 ;B loss: 0.440 ;acc: 0.915 ;iou_acc: 0.955 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.631 ;acc: 0.875 ;iou_acc: 0.900 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.932 ;acc: 0.810 ;iou_acc: 0.870 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.932 ;acc: 0.805 ;iou_acc: 0.865 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.269 ;acc: 0.715 ;iou_acc: 0.830 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.823 ;acc: 0.575 ;iou_acc: 0.715 ;time: 0:01:19\n",
      "\n",
      "*BTrain: False ;Test loss: 1.125 ;Test accuracy 0.759 ;IOU accuracy: 0.838 ;Time: 0:01:30\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 48 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:14257 ;lr:0.045 ;Bloss:0.185 ;acc:0.980 ;iou:0.990 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:14307 ;lr:0.045 ;Bloss:0.969 ;acc:0.815 ;iou:0.915 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:14357 ;lr:0.045 ;Bloss:0.176 ;acc:0.970 ;iou:0.980 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:14407 ;lr:0.045 ;Bloss:0.396 ;acc:0.945 ;iou:0.980 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:14457 ;lr:0.045 ;Bloss:1.167 ;acc:0.805 ;iou:0.885 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:14507 ;lr:0.045 ;Bloss:1.739 ;acc:0.655 ;iou:0.820 ;time: 0:00:41\n",
      "\n",
      "*Training B: True ;B Train loss: 0.760 ;Train accuracy: 0.867 ;IOU accuracy: 0.917 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 48\n",
      "batch: 0 ;B loss: 0.428 ;acc: 0.920 ;iou_acc: 0.955 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.603 ;acc: 0.875 ;iou_acc: 0.900 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.911 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 0.931 ;acc: 0.800 ;iou_acc: 0.850 ;time: 0:01:04\n",
      "batch: 200 ;B loss: 1.301 ;acc: 0.710 ;iou_acc: 0.835 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 1.797 ;acc: 0.590 ;iou_acc: 0.725 ;time: 0:01:19\n",
      "\n",
      "*BTrain: True ;Test loss: 1.110 ;Test accuracy 0.763 ;IOU accuracy: 0.840 ;Time: 0:01:29\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 49 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:14554 ;lr:0.045 ;Bloss:0.941 ;acc:0.835 ;iou:0.855 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:14604 ;lr:0.045 ;Bloss:0.200 ;acc:0.965 ;iou:0.990 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:14654 ;lr:0.045 ;Bloss:1.005 ;acc:0.815 ;iou:0.890 ;time: 0:00:15\n",
      "Edit:False ;batch:150 ;gs:14704 ;lr:0.045 ;Bloss:0.982 ;acc:0.835 ;iou:0.890 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:14754 ;lr:0.045 ;Bloss:0.695 ;acc:0.895 ;iou:0.940 ;time: 0:00:30\n",
      "Edit:False ;batch:250 ;gs:14804 ;lr:0.045 ;Bloss:1.850 ;acc:0.545 ;iou:0.720 ;time: 0:00:38\n",
      "\n",
      "*Training B: False ;B Train loss: 0.749 ;Train accuracy: 0.872 ;IOU accuracy: 0.921 ;Time: 0:00:45 \n",
      "\n",
      "Testing, ephoc: 49\n",
      "batch: 0 ;B loss: 0.425 ;acc: 0.920 ;iou_acc: 0.955 ;time: 0:00:46\n",
      "batch: 50 ;B loss: 0.615 ;acc: 0.875 ;iou_acc: 0.900 ;time: 0:00:50\n",
      "batch: 100 ;B loss: 0.926 ;acc: 0.820 ;iou_acc: 0.880 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 0.930 ;acc: 0.820 ;iou_acc: 0.870 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.305 ;acc: 0.715 ;iou_acc: 0.840 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.817 ;acc: 0.600 ;iou_acc: 0.745 ;time: 0:01:17\n",
      "\n",
      "*BTrain: False ;Test loss: 1.117 ;Test accuracy 0.763 ;IOU accuracy: 0.841 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 50 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:14851 ;lr:0.045 ;Bloss:0.727 ;acc:0.875 ;iou:0.945 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:14901 ;lr:0.045 ;Bloss:0.155 ;acc:0.995 ;iou:0.995 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:14951 ;lr:0.045 ;Bloss:0.782 ;acc:0.880 ;iou:0.915 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:15001 ;lr:0.045 ;Bloss:0.546 ;acc:0.920 ;iou:0.935 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:15051 ;lr:0.045 ;Bloss:0.212 ;acc:0.970 ;iou:0.985 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:15101 ;lr:0.045 ;Bloss:1.014 ;acc:0.820 ;iou:0.900 ;time: 0:00:42\n",
      "\n",
      "*Training B: False ;B Train loss: 0.744 ;Train accuracy: 0.872 ;IOU accuracy: 0.921 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 50\n",
      "batch: 0 ;B loss: 0.411 ;acc: 0.920 ;iou_acc: 0.960 ;time: 0:00:50\n",
      "batch: 50 ;B loss: 0.640 ;acc: 0.875 ;iou_acc: 0.900 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.909 ;acc: 0.805 ;iou_acc: 0.865 ;time: 0:01:00\n",
      "batch: 150 ;B loss: 0.926 ;acc: 0.795 ;iou_acc: 0.845 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.303 ;acc: 0.715 ;iou_acc: 0.835 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.839 ;acc: 0.585 ;iou_acc: 0.725 ;time: 0:01:23\n",
      "\n",
      "*BTrain: False ;Test loss: 1.120 ;Test accuracy 0.762 ;IOU accuracy: 0.839 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 51 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:15148 ;lr:0.045 ;Bloss:0.258 ;acc:0.985 ;iou:0.985 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:15198 ;lr:0.045 ;Bloss:0.247 ;acc:0.965 ;iou:0.975 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:15248 ;lr:0.045 ;Bloss:0.987 ;acc:0.820 ;iou:0.915 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:15298 ;lr:0.045 ;Bloss:0.288 ;acc:0.975 ;iou:0.980 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:15348 ;lr:0.045 ;Bloss:1.248 ;acc:0.775 ;iou:0.895 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:15398 ;lr:0.045 ;Bloss:0.884 ;acc:0.845 ;iou:0.925 ;time: 0:00:42\n",
      "\n",
      "*Training B: True ;B Train loss: 0.736 ;Train accuracy: 0.874 ;IOU accuracy: 0.922 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 51\n",
      "batch: 0 ;B loss: 0.422 ;acc: 0.920 ;iou_acc: 0.955 ;time: 0:00:50\n",
      "batch: 50 ;B loss: 0.618 ;acc: 0.885 ;iou_acc: 0.910 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.903 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:01:00\n",
      "batch: 150 ;B loss: 0.963 ;acc: 0.795 ;iou_acc: 0.850 ;time: 0:01:06\n",
      "batch: 200 ;B loss: 1.293 ;acc: 0.715 ;iou_acc: 0.830 ;time: 0:01:13\n",
      "batch: 250 ;B loss: 1.813 ;acc: 0.585 ;iou_acc: 0.725 ;time: 0:01:21\n",
      "\n",
      "*BTrain: True ;Test loss: 1.112 ;Test accuracy 0.764 ;IOU accuracy: 0.840 ;Time: 0:01:32\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 52 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:15445 ;lr:0.045 ;Bloss:1.680 ;acc:0.660 ;iou:0.810 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:15495 ;lr:0.045 ;Bloss:0.532 ;acc:0.920 ;iou:0.965 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:15545 ;lr:0.045 ;Bloss:0.563 ;acc:0.940 ;iou:0.945 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:15595 ;lr:0.045 ;Bloss:0.241 ;acc:0.980 ;iou:0.990 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:15645 ;lr:0.045 ;Bloss:0.885 ;acc:0.865 ;iou:0.915 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:15695 ;lr:0.045 ;Bloss:0.770 ;acc:0.875 ;iou:0.915 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 0.725 ;Train accuracy: 0.877 ;IOU accuracy: 0.924 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 52\n",
      "batch: 0 ;B loss: 0.447 ;acc: 0.915 ;iou_acc: 0.955 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.614 ;acc: 0.885 ;iou_acc: 0.905 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 0.936 ;acc: 0.810 ;iou_acc: 0.865 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.945 ;acc: 0.785 ;iou_acc: 0.830 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.303 ;acc: 0.700 ;iou_acc: 0.840 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 1.783 ;acc: 0.600 ;iou_acc: 0.740 ;time: 0:01:20\n",
      "\n",
      "*BTrain: False ;Test loss: 1.123 ;Test accuracy 0.763 ;IOU accuracy: 0.839 ;Time: 0:01:30\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 53 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:15742 ;lr:0.045 ;Bloss:1.267 ;acc:0.755 ;iou:0.860 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:15792 ;lr:0.045 ;Bloss:0.435 ;acc:0.950 ;iou:0.970 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:15842 ;lr:0.045 ;Bloss:0.165 ;acc:0.970 ;iou:0.990 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:15892 ;lr:0.045 ;Bloss:0.392 ;acc:0.950 ;iou:0.975 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:15942 ;lr:0.045 ;Bloss:0.842 ;acc:0.845 ;iou:0.920 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:15992 ;lr:0.045 ;Bloss:1.081 ;acc:0.810 ;iou:0.880 ;time: 0:00:43\n",
      "\n",
      "*Training B: False ;B Train loss: 0.714 ;Train accuracy: 0.880 ;IOU accuracy: 0.926 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 53\n",
      "batch: 0 ;B loss: 0.446 ;acc: 0.920 ;iou_acc: 0.950 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.639 ;acc: 0.875 ;iou_acc: 0.900 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.901 ;acc: 0.825 ;iou_acc: 0.880 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 0.949 ;acc: 0.795 ;iou_acc: 0.845 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.276 ;acc: 0.730 ;iou_acc: 0.845 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 1.892 ;acc: 0.590 ;iou_acc: 0.735 ;time: 0:01:23\n",
      "\n",
      "*BTrain: False ;Test loss: 1.125 ;Test accuracy 0.763 ;IOU accuracy: 0.840 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 54 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:16039 ;lr:0.045 ;Bloss:1.634 ;acc:0.675 ;iou:0.830 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:16089 ;lr:0.045 ;Bloss:1.265 ;acc:0.740 ;iou:0.845 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:16139 ;lr:0.045 ;Bloss:0.253 ;acc:0.975 ;iou:0.980 ;time: 0:00:15\n",
      "Edit:False ;batch:150 ;gs:16189 ;lr:0.045 ;Bloss:1.796 ;acc:0.625 ;iou:0.770 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:16239 ;lr:0.045 ;Bloss:0.638 ;acc:0.885 ;iou:0.940 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:16289 ;lr:0.045 ;Bloss:0.551 ;acc:0.935 ;iou:0.960 ;time: 0:00:39\n",
      "\n",
      "*Training B: True ;B Train loss: 0.710 ;Train accuracy: 0.881 ;IOU accuracy: 0.927 ;Time: 0:00:45 \n",
      "\n",
      "Testing, ephoc: 54\n",
      "batch: 0 ;B loss: 0.428 ;acc: 0.920 ;iou_acc: 0.950 ;time: 0:00:46\n",
      "batch: 50 ;B loss: 0.625 ;acc: 0.875 ;iou_acc: 0.900 ;time: 0:00:49\n",
      "batch: 100 ;B loss: 0.923 ;acc: 0.805 ;iou_acc: 0.860 ;time: 0:00:54\n",
      "batch: 150 ;B loss: 0.947 ;acc: 0.810 ;iou_acc: 0.860 ;time: 0:01:00\n",
      "batch: 200 ;B loss: 1.291 ;acc: 0.705 ;iou_acc: 0.835 ;time: 0:01:08\n",
      "batch: 250 ;B loss: 1.843 ;acc: 0.590 ;iou_acc: 0.730 ;time: 0:01:17\n",
      "\n",
      "*BTrain: True ;Test loss: 1.116 ;Test accuracy 0.764 ;IOU accuracy: 0.840 ;Time: 0:01:27\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 55 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:16336 ;lr:0.045 ;Bloss:0.236 ;acc:0.980 ;iou:0.985 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:16386 ;lr:0.045 ;Bloss:0.987 ;acc:0.855 ;iou:0.925 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:16436 ;lr:0.045 ;Bloss:0.940 ;acc:0.840 ;iou:0.880 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:16486 ;lr:0.045 ;Bloss:0.434 ;acc:0.920 ;iou:0.960 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:16536 ;lr:0.045 ;Bloss:0.429 ;acc:0.950 ;iou:0.970 ;time: 0:00:35\n",
      "Edit:False ;batch:250 ;gs:16586 ;lr:0.045 ;Bloss:0.645 ;acc:0.920 ;iou:0.950 ;time: 0:00:43\n",
      "\n",
      "*Training B: False ;B Train loss: 0.697 ;Train accuracy: 0.884 ;IOU accuracy: 0.929 ;Time: 0:00:51 \n",
      "\n",
      "Testing, ephoc: 55\n",
      "batch: 0 ;B loss: 0.433 ;acc: 0.915 ;iou_acc: 0.955 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.637 ;acc: 0.880 ;iou_acc: 0.910 ;time: 0:00:56\n",
      "batch: 100 ;B loss: 0.907 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 0.915 ;acc: 0.825 ;iou_acc: 0.880 ;time: 0:01:08\n",
      "batch: 200 ;B loss: 1.327 ;acc: 0.700 ;iou_acc: 0.830 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 1.827 ;acc: 0.605 ;iou_acc: 0.740 ;time: 0:01:24\n",
      "\n",
      "*BTrain: False ;Test loss: 1.119 ;Test accuracy 0.765 ;IOU accuracy: 0.842 ;Time: 0:01:35\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 56 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:16633 ;lr:0.045 ;Bloss:0.715 ;acc:0.880 ;iou:0.935 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:16683 ;lr:0.045 ;Bloss:1.236 ;acc:0.785 ;iou:0.840 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:16733 ;lr:0.045 ;Bloss:0.950 ;acc:0.825 ;iou:0.895 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:16783 ;lr:0.045 ;Bloss:1.169 ;acc:0.785 ;iou:0.865 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:16833 ;lr:0.045 ;Bloss:1.273 ;acc:0.725 ;iou:0.815 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:16883 ;lr:0.045 ;Bloss:0.595 ;acc:0.930 ;iou:0.965 ;time: 0:00:42\n",
      "\n",
      "*Training B: False ;B Train loss: 0.692 ;Train accuracy: 0.884 ;IOU accuracy: 0.929 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 56\n",
      "batch: 0 ;B loss: 0.453 ;acc: 0.915 ;iou_acc: 0.950 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.624 ;acc: 0.880 ;iou_acc: 0.905 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.952 ;acc: 0.820 ;iou_acc: 0.870 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 0.919 ;acc: 0.790 ;iou_acc: 0.835 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.367 ;acc: 0.710 ;iou_acc: 0.840 ;time: 0:01:14\n",
      "batch: 250 ;B loss: 1.818 ;acc: 0.575 ;iou_acc: 0.715 ;time: 0:01:22\n",
      "\n",
      "*BTrain: False ;Test loss: 1.137 ;Test accuracy 0.761 ;IOU accuracy: 0.838 ;Time: 0:01:33\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 57 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:16930 ;lr:0.045 ;Bloss:0.401 ;acc:0.950 ;iou:0.980 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:16980 ;lr:0.045 ;Bloss:0.325 ;acc:0.965 ;iou:0.980 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:17030 ;lr:0.045 ;Bloss:0.175 ;acc:0.985 ;iou:0.995 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:17080 ;lr:0.045 ;Bloss:0.247 ;acc:0.965 ;iou:0.970 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:17130 ;lr:0.045 ;Bloss:1.147 ;acc:0.775 ;iou:0.860 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:17180 ;lr:0.045 ;Bloss:0.649 ;acc:0.890 ;iou:0.925 ;time: 0:00:40\n",
      "\n",
      "*Training B: True ;B Train loss: 0.684 ;Train accuracy: 0.886 ;IOU accuracy: 0.930 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 57\n",
      "batch: 0 ;B loss: 0.445 ;acc: 0.915 ;iou_acc: 0.955 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.621 ;acc: 0.890 ;iou_acc: 0.910 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.946 ;acc: 0.820 ;iou_acc: 0.880 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.951 ;acc: 0.790 ;iou_acc: 0.835 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.315 ;acc: 0.740 ;iou_acc: 0.850 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 1.828 ;acc: 0.595 ;iou_acc: 0.730 ;time: 0:01:19\n",
      "\n",
      "*BTrain: True ;Test loss: 1.139 ;Test accuracy 0.762 ;IOU accuracy: 0.839 ;Time: 0:01:29\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 58 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:17227 ;lr:0.045 ;Bloss:1.108 ;acc:0.815 ;iou:0.880 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:17277 ;lr:0.045 ;Bloss:0.449 ;acc:0.955 ;iou:0.970 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:17327 ;lr:0.045 ;Bloss:0.709 ;acc:0.900 ;iou:0.940 ;time: 0:00:15\n",
      "Edit:False ;batch:150 ;gs:17377 ;lr:0.045 ;Bloss:0.377 ;acc:0.955 ;iou:0.975 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:17427 ;lr:0.045 ;Bloss:0.574 ;acc:0.900 ;iou:0.945 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:17477 ;lr:0.045 ;Bloss:0.287 ;acc:0.970 ;iou:0.985 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 0.677 ;Train accuracy: 0.889 ;IOU accuracy: 0.932 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 58\n",
      "batch: 0 ;B loss: 0.456 ;acc: 0.910 ;iou_acc: 0.955 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.637 ;acc: 0.885 ;iou_acc: 0.910 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.922 ;acc: 0.815 ;iou_acc: 0.875 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.934 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.301 ;acc: 0.730 ;iou_acc: 0.840 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 1.859 ;acc: 0.585 ;iou_acc: 0.710 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.125 ;Test accuracy 0.765 ;IOU accuracy: 0.841 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 59 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:17524 ;lr:0.045 ;Bloss:0.480 ;acc:0.925 ;iou:0.955 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:17574 ;lr:0.045 ;Bloss:0.496 ;acc:0.945 ;iou:0.980 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:17624 ;lr:0.045 ;Bloss:1.424 ;acc:0.815 ;iou:0.880 ;time: 0:00:15\n",
      "Edit:False ;batch:150 ;gs:17674 ;lr:0.045 ;Bloss:0.324 ;acc:0.955 ;iou:0.985 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:17724 ;lr:0.045 ;Bloss:0.791 ;acc:0.885 ;iou:0.930 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:17774 ;lr:0.045 ;Bloss:0.425 ;acc:0.950 ;iou:0.965 ;time: 0:00:38\n",
      "\n",
      "*Training B: False ;B Train loss: 0.675 ;Train accuracy: 0.889 ;IOU accuracy: 0.932 ;Time: 0:00:45 \n",
      "\n",
      "Testing, ephoc: 59\n",
      "batch: 0 ;B loss: 0.461 ;acc: 0.910 ;iou_acc: 0.945 ;time: 0:00:45\n",
      "batch: 50 ;B loss: 0.642 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:49\n",
      "batch: 100 ;B loss: 0.927 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:00:55\n",
      "batch: 150 ;B loss: 0.932 ;acc: 0.800 ;iou_acc: 0.855 ;time: 0:01:00\n",
      "batch: 200 ;B loss: 1.297 ;acc: 0.715 ;iou_acc: 0.840 ;time: 0:01:07\n",
      "batch: 250 ;B loss: 1.866 ;acc: 0.610 ;iou_acc: 0.735 ;time: 0:01:15\n",
      "\n",
      "*BTrain: False ;Test loss: 1.126 ;Test accuracy 0.768 ;IOU accuracy: 0.843 ;Time: 0:01:26\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 60 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:17821 ;lr:0.045 ;Bloss:0.296 ;acc:0.975 ;iou:0.985 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:17871 ;lr:0.045 ;Bloss:0.740 ;acc:0.885 ;iou:0.935 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:17921 ;lr:0.045 ;Bloss:1.249 ;acc:0.795 ;iou:0.885 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:17971 ;lr:0.045 ;Bloss:0.441 ;acc:0.955 ;iou:0.965 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:18021 ;lr:0.045 ;Bloss:0.638 ;acc:0.880 ;iou:0.940 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:18071 ;lr:0.045 ;Bloss:0.890 ;acc:0.835 ;iou:0.905 ;time: 0:00:42\n",
      "\n",
      "*Training B: True ;B Train loss: 0.666 ;Train accuracy: 0.892 ;IOU accuracy: 0.934 ;Time: 0:00:49 \n",
      "\n",
      "Testing, ephoc: 60\n",
      "batch: 0 ;B loss: 0.442 ;acc: 0.920 ;iou_acc: 0.955 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.625 ;acc: 0.875 ;iou_acc: 0.905 ;time: 0:00:54\n",
      "batch: 100 ;B loss: 0.892 ;acc: 0.810 ;iou_acc: 0.870 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 0.926 ;acc: 0.810 ;iou_acc: 0.860 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.376 ;acc: 0.705 ;iou_acc: 0.825 ;time: 0:01:12\n",
      "batch: 250 ;B loss: 1.836 ;acc: 0.595 ;iou_acc: 0.725 ;time: 0:01:21\n",
      "\n",
      "*BTrain: True ;Test loss: 1.133 ;Test accuracy 0.764 ;IOU accuracy: 0.840 ;Time: 0:01:31\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 61 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:18118 ;lr:0.045 ;Bloss:1.130 ;acc:0.780 ;iou:0.865 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:18168 ;lr:0.045 ;Bloss:0.594 ;acc:0.895 ;iou:0.950 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:18218 ;lr:0.045 ;Bloss:1.200 ;acc:0.815 ;iou:0.855 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:18268 ;lr:0.045 ;Bloss:0.684 ;acc:0.890 ;iou:0.940 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:18318 ;lr:0.045 ;Bloss:1.088 ;acc:0.815 ;iou:0.880 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:18368 ;lr:0.045 ;Bloss:0.459 ;acc:0.950 ;iou:0.965 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 0.657 ;Train accuracy: 0.893 ;IOU accuracy: 0.934 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 61\n",
      "batch: 0 ;B loss: 0.455 ;acc: 0.910 ;iou_acc: 0.945 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.637 ;acc: 0.885 ;iou_acc: 0.910 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.907 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 0.933 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.316 ;acc: 0.735 ;iou_acc: 0.840 ;time: 0:01:12\n",
      "batch: 250 ;B loss: 1.854 ;acc: 0.605 ;iou_acc: 0.735 ;time: 0:01:21\n",
      "\n",
      "*BTrain: False ;Test loss: 1.121 ;Test accuracy 0.766 ;IOU accuracy: 0.843 ;Time: 0:01:31\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 62 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:18415 ;lr:0.045 ;Bloss:0.132 ;acc:0.985 ;iou:1.000 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:18465 ;lr:0.045 ;Bloss:0.607 ;acc:0.910 ;iou:0.955 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:18515 ;lr:0.045 ;Bloss:0.464 ;acc:0.940 ;iou:0.970 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:18565 ;lr:0.045 ;Bloss:0.266 ;acc:0.980 ;iou:0.990 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:18615 ;lr:0.045 ;Bloss:1.024 ;acc:0.805 ;iou:0.890 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:18665 ;lr:0.045 ;Bloss:0.747 ;acc:0.885 ;iou:0.920 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 0.652 ;Train accuracy: 0.895 ;IOU accuracy: 0.936 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 62\n",
      "batch: 0 ;B loss: 0.453 ;acc: 0.915 ;iou_acc: 0.950 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.627 ;acc: 0.880 ;iou_acc: 0.910 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.918 ;acc: 0.815 ;iou_acc: 0.875 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 0.923 ;acc: 0.805 ;iou_acc: 0.860 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.329 ;acc: 0.720 ;iou_acc: 0.835 ;time: 0:01:12\n",
      "batch: 250 ;B loss: 1.814 ;acc: 0.605 ;iou_acc: 0.735 ;time: 0:01:21\n",
      "\n",
      "*BTrain: False ;Test loss: 1.123 ;Test accuracy 0.768 ;IOU accuracy: 0.844 ;Time: 0:01:31\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 63 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:18712 ;lr:0.045 ;Bloss:0.415 ;acc:0.955 ;iou:0.970 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:18762 ;lr:0.045 ;Bloss:0.180 ;acc:0.980 ;iou:0.990 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:18812 ;lr:0.045 ;Bloss:0.754 ;acc:0.905 ;iou:0.940 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:18862 ;lr:0.045 ;Bloss:1.697 ;acc:0.660 ;iou:0.785 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:18912 ;lr:0.045 ;Bloss:0.708 ;acc:0.885 ;iou:0.930 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:18962 ;lr:0.045 ;Bloss:0.322 ;acc:0.970 ;iou:0.995 ;time: 0:00:40\n",
      "\n",
      "*Training B: True ;B Train loss: 0.640 ;Train accuracy: 0.898 ;IOU accuracy: 0.937 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 63\n",
      "batch: 0 ;B loss: 0.452 ;acc: 0.910 ;iou_acc: 0.950 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.640 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.914 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 0.958 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.295 ;acc: 0.740 ;iou_acc: 0.850 ;time: 0:01:12\n",
      "batch: 250 ;B loss: 1.844 ;acc: 0.615 ;iou_acc: 0.725 ;time: 0:01:20\n",
      "\n",
      "*BTrain: True ;Test loss: 1.126 ;Test accuracy 0.767 ;IOU accuracy: 0.843 ;Time: 0:01:30\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 64 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:19009 ;lr:0.045 ;Bloss:0.279 ;acc:0.960 ;iou:0.975 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:19059 ;lr:0.045 ;Bloss:0.572 ;acc:0.915 ;iou:0.955 ;time: 0:00:10\n",
      "Edit:False ;batch:100 ;gs:19109 ;lr:0.045 ;Bloss:1.134 ;acc:0.805 ;iou:0.910 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:19159 ;lr:0.045 ;Bloss:0.177 ;acc:0.975 ;iou:0.985 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:19209 ;lr:0.045 ;Bloss:0.463 ;acc:0.960 ;iou:0.990 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:19259 ;lr:0.045 ;Bloss:1.139 ;acc:0.815 ;iou:0.920 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 0.638 ;Train accuracy: 0.898 ;IOU accuracy: 0.938 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 64\n",
      "batch: 0 ;B loss: 0.432 ;acc: 0.925 ;iou_acc: 0.955 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.607 ;acc: 0.880 ;iou_acc: 0.905 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.945 ;acc: 0.830 ;iou_acc: 0.885 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 0.927 ;acc: 0.810 ;iou_acc: 0.855 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.356 ;acc: 0.705 ;iou_acc: 0.830 ;time: 0:01:13\n",
      "batch: 250 ;B loss: 1.801 ;acc: 0.610 ;iou_acc: 0.725 ;time: 0:01:21\n",
      "\n",
      "*BTrain: False ;Test loss: 1.139 ;Test accuracy 0.766 ;IOU accuracy: 0.842 ;Time: 0:01:32\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 65 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:19306 ;lr:0.045 ;Bloss:0.300 ;acc:0.970 ;iou:0.990 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:19356 ;lr:0.045 ;Bloss:0.223 ;acc:0.980 ;iou:0.985 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:19406 ;lr:0.045 ;Bloss:0.763 ;acc:0.885 ;iou:0.935 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:19456 ;lr:0.045 ;Bloss:0.528 ;acc:0.930 ;iou:0.965 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:19506 ;lr:0.045 ;Bloss:0.250 ;acc:0.970 ;iou:0.980 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:19556 ;lr:0.045 ;Bloss:0.383 ;acc:0.960 ;iou:0.970 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 0.628 ;Train accuracy: 0.900 ;IOU accuracy: 0.939 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 65\n",
      "batch: 0 ;B loss: 0.442 ;acc: 0.915 ;iou_acc: 0.945 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.597 ;acc: 0.885 ;iou_acc: 0.910 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.926 ;acc: 0.810 ;iou_acc: 0.870 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 0.925 ;acc: 0.815 ;iou_acc: 0.865 ;time: 0:01:04\n",
      "batch: 200 ;B loss: 1.337 ;acc: 0.705 ;iou_acc: 0.835 ;time: 0:01:12\n",
      "batch: 250 ;B loss: 1.802 ;acc: 0.605 ;iou_acc: 0.725 ;time: 0:01:20\n",
      "\n",
      "*BTrain: False ;Test loss: 1.124 ;Test accuracy 0.768 ;IOU accuracy: 0.843 ;Time: 0:01:31\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 66 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:19603 ;lr:0.045 ;Bloss:0.940 ;acc:0.865 ;iou:0.925 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:19653 ;lr:0.045 ;Bloss:1.099 ;acc:0.815 ;iou:0.870 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:19703 ;lr:0.045 ;Bloss:0.168 ;acc:0.985 ;iou:1.000 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:19753 ;lr:0.045 ;Bloss:0.150 ;acc:0.995 ;iou:0.995 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:19803 ;lr:0.045 ;Bloss:0.976 ;acc:0.870 ;iou:0.905 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:19853 ;lr:0.045 ;Bloss:1.052 ;acc:0.770 ;iou:0.865 ;time: 0:00:41\n",
      "\n",
      "*Training B: True ;B Train loss: 0.622 ;Train accuracy: 0.902 ;IOU accuracy: 0.941 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 66\n",
      "batch: 0 ;B loss: 0.455 ;acc: 0.915 ;iou_acc: 0.950 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.624 ;acc: 0.875 ;iou_acc: 0.905 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.927 ;acc: 0.805 ;iou_acc: 0.865 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.935 ;acc: 0.805 ;iou_acc: 0.855 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.390 ;acc: 0.710 ;iou_acc: 0.835 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.853 ;acc: 0.605 ;iou_acc: 0.735 ;time: 0:01:17\n",
      "\n",
      "*BTrain: True ;Test loss: 1.149 ;Test accuracy 0.765 ;IOU accuracy: 0.841 ;Time: 0:01:27\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 67 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:19900 ;lr:0.045 ;Bloss:0.690 ;acc:0.900 ;iou:0.935 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:19950 ;lr:0.045 ;Bloss:0.851 ;acc:0.870 ;iou:0.920 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:20000 ;lr:0.045 ;Bloss:0.179 ;acc:0.985 ;iou:0.995 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:20050 ;lr:0.040 ;Bloss:0.389 ;acc:0.940 ;iou:0.960 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:20100 ;lr:0.040 ;Bloss:0.375 ;acc:0.955 ;iou:0.965 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:20150 ;lr:0.040 ;Bloss:1.064 ;acc:0.845 ;iou:0.945 ;time: 0:00:39\n",
      "\n",
      "*Training B: False ;B Train loss: 0.609 ;Train accuracy: 0.904 ;IOU accuracy: 0.942 ;Time: 0:00:46 \n",
      "\n",
      "Testing, ephoc: 67\n",
      "batch: 0 ;B loss: 0.434 ;acc: 0.920 ;iou_acc: 0.950 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.637 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 0.928 ;acc: 0.795 ;iou_acc: 0.855 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 0.970 ;acc: 0.800 ;iou_acc: 0.860 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.332 ;acc: 0.730 ;iou_acc: 0.850 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.842 ;acc: 0.600 ;iou_acc: 0.715 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.130 ;Test accuracy 0.768 ;IOU accuracy: 0.844 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 68 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:20197 ;lr:0.040 ;Bloss:0.286 ;acc:0.975 ;iou:0.975 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:20247 ;lr:0.040 ;Bloss:1.065 ;acc:0.815 ;iou:0.885 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:20297 ;lr:0.040 ;Bloss:0.474 ;acc:0.925 ;iou:0.975 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:20347 ;lr:0.040 ;Bloss:0.525 ;acc:0.925 ;iou:0.955 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:20397 ;lr:0.040 ;Bloss:0.944 ;acc:0.850 ;iou:0.910 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:20447 ;lr:0.040 ;Bloss:0.822 ;acc:0.870 ;iou:0.915 ;time: 0:00:39\n",
      "\n",
      "*Training B: False ;B Train loss: 0.596 ;Train accuracy: 0.907 ;IOU accuracy: 0.944 ;Time: 0:00:46 \n",
      "\n",
      "Testing, ephoc: 68\n",
      "batch: 0 ;B loss: 0.436 ;acc: 0.920 ;iou_acc: 0.950 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.621 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 0.912 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 0.938 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.305 ;acc: 0.710 ;iou_acc: 0.830 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.825 ;acc: 0.625 ;iou_acc: 0.725 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.128 ;Test accuracy 0.770 ;IOU accuracy: 0.845 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 69 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:20494 ;lr:0.040 ;Bloss:0.507 ;acc:0.950 ;iou:0.970 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:20544 ;lr:0.040 ;Bloss:0.681 ;acc:0.895 ;iou:0.950 ;time: 0:00:07\n",
      "Edit:False ;batch:100 ;gs:20594 ;lr:0.040 ;Bloss:0.142 ;acc:0.995 ;iou:1.000 ;time: 0:00:15\n",
      "Edit:False ;batch:150 ;gs:20644 ;lr:0.040 ;Bloss:0.903 ;acc:0.820 ;iou:0.910 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:20694 ;lr:0.040 ;Bloss:1.022 ;acc:0.820 ;iou:0.885 ;time: 0:00:30\n",
      "Edit:False ;batch:250 ;gs:20744 ;lr:0.040 ;Bloss:0.222 ;acc:0.975 ;iou:0.985 ;time: 0:00:38\n",
      "\n",
      "*Training B: True ;B Train loss: 0.591 ;Train accuracy: 0.908 ;IOU accuracy: 0.945 ;Time: 0:00:45 \n",
      "\n",
      "Testing, ephoc: 69\n",
      "batch: 0 ;B loss: 0.462 ;acc: 0.905 ;iou_acc: 0.940 ;time: 0:00:45\n",
      "batch: 50 ;B loss: 0.604 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:49\n",
      "batch: 100 ;B loss: 0.895 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:00:54\n",
      "batch: 150 ;B loss: 0.935 ;acc: 0.805 ;iou_acc: 0.860 ;time: 0:01:00\n",
      "batch: 200 ;B loss: 1.351 ;acc: 0.710 ;iou_acc: 0.845 ;time: 0:01:07\n",
      "batch: 250 ;B loss: 1.834 ;acc: 0.600 ;iou_acc: 0.715 ;time: 0:01:15\n",
      "\n",
      "*BTrain: True ;Test loss: 1.127 ;Test accuracy 0.770 ;IOU accuracy: 0.845 ;Time: 0:01:25\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 70 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:20791 ;lr:0.040 ;Bloss:0.550 ;acc:0.925 ;iou:0.960 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:20841 ;lr:0.040 ;Bloss:0.653 ;acc:0.915 ;iou:0.955 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:20891 ;lr:0.040 ;Bloss:0.805 ;acc:0.885 ;iou:0.935 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:20941 ;lr:0.040 ;Bloss:0.371 ;acc:0.960 ;iou:0.970 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:20991 ;lr:0.040 ;Bloss:0.410 ;acc:0.960 ;iou:0.975 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:21041 ;lr:0.040 ;Bloss:0.189 ;acc:0.985 ;iou:0.990 ;time: 0:00:38\n",
      "\n",
      "*Training B: False ;B Train loss: 0.587 ;Train accuracy: 0.909 ;IOU accuracy: 0.946 ;Time: 0:00:45 \n",
      "\n",
      "Testing, ephoc: 70\n",
      "batch: 0 ;B loss: 0.451 ;acc: 0.920 ;iou_acc: 0.955 ;time: 0:00:46\n",
      "batch: 50 ;B loss: 0.616 ;acc: 0.890 ;iou_acc: 0.910 ;time: 0:00:50\n",
      "batch: 100 ;B loss: 0.948 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:00:55\n",
      "batch: 150 ;B loss: 0.924 ;acc: 0.810 ;iou_acc: 0.865 ;time: 0:01:01\n",
      "batch: 200 ;B loss: 1.375 ;acc: 0.695 ;iou_acc: 0.815 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.846 ;acc: 0.615 ;iou_acc: 0.730 ;time: 0:01:17\n",
      "\n",
      "*BTrain: False ;Test loss: 1.144 ;Test accuracy 0.768 ;IOU accuracy: 0.844 ;Time: 0:01:27\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 71 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:21088 ;lr:0.040 ;Bloss:0.689 ;acc:0.900 ;iou:0.940 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:21138 ;lr:0.040 ;Bloss:0.667 ;acc:0.895 ;iou:0.915 ;time: 0:00:07\n",
      "Edit:False ;batch:100 ;gs:21188 ;lr:0.040 ;Bloss:0.585 ;acc:0.925 ;iou:0.970 ;time: 0:00:14\n",
      "Edit:False ;batch:150 ;gs:21238 ;lr:0.040 ;Bloss:0.354 ;acc:0.950 ;iou:0.975 ;time: 0:00:21\n",
      "Edit:False ;batch:200 ;gs:21288 ;lr:0.040 ;Bloss:1.059 ;acc:0.795 ;iou:0.885 ;time: 0:00:28\n",
      "Edit:False ;batch:250 ;gs:21338 ;lr:0.040 ;Bloss:0.926 ;acc:0.880 ;iou:0.910 ;time: 0:00:36\n",
      "\n",
      "*Training B: False ;B Train loss: 0.582 ;Train accuracy: 0.909 ;IOU accuracy: 0.946 ;Time: 0:00:43 \n",
      "\n",
      "Testing, ephoc: 71\n",
      "batch: 0 ;B loss: 0.432 ;acc: 0.910 ;iou_acc: 0.945 ;time: 0:00:43\n",
      "batch: 50 ;B loss: 0.647 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:48\n",
      "batch: 100 ;B loss: 0.917 ;acc: 0.825 ;iou_acc: 0.885 ;time: 0:00:53\n",
      "batch: 150 ;B loss: 0.946 ;acc: 0.790 ;iou_acc: 0.845 ;time: 0:01:00\n",
      "batch: 200 ;B loss: 1.347 ;acc: 0.725 ;iou_acc: 0.835 ;time: 0:01:07\n",
      "batch: 250 ;B loss: 1.875 ;acc: 0.605 ;iou_acc: 0.720 ;time: 0:01:16\n",
      "\n",
      "*BTrain: False ;Test loss: 1.150 ;Test accuracy 0.767 ;IOU accuracy: 0.843 ;Time: 0:01:26\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 72 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:21385 ;lr:0.040 ;Bloss:0.627 ;acc:0.920 ;iou:0.955 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:21435 ;lr:0.040 ;Bloss:0.472 ;acc:0.940 ;iou:0.970 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:21485 ;lr:0.040 ;Bloss:0.123 ;acc:0.990 ;iou:1.000 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:21535 ;lr:0.040 ;Bloss:0.105 ;acc:0.990 ;iou:0.990 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:21585 ;lr:0.040 ;Bloss:0.321 ;acc:0.965 ;iou:0.975 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:21635 ;lr:0.040 ;Bloss:1.433 ;acc:0.735 ;iou:0.850 ;time: 0:00:40\n",
      "\n",
      "*Training B: True ;B Train loss: 0.577 ;Train accuracy: 0.910 ;IOU accuracy: 0.946 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 72\n",
      "batch: 0 ;B loss: 0.438 ;acc: 0.915 ;iou_acc: 0.955 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.673 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.941 ;acc: 0.795 ;iou_acc: 0.855 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 0.963 ;acc: 0.815 ;iou_acc: 0.860 ;time: 0:01:04\n",
      "batch: 200 ;B loss: 1.419 ;acc: 0.695 ;iou_acc: 0.815 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 1.942 ;acc: 0.610 ;iou_acc: 0.715 ;time: 0:01:20\n",
      "\n",
      "*BTrain: True ;Test loss: 1.195 ;Test accuracy 0.761 ;IOU accuracy: 0.837 ;Time: 0:01:30\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 73 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:21682 ;lr:0.040 ;Bloss:0.408 ;acc:0.955 ;iou:0.980 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:21732 ;lr:0.040 ;Bloss:0.157 ;acc:0.990 ;iou:0.995 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:21782 ;lr:0.040 ;Bloss:0.293 ;acc:0.965 ;iou:0.990 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:21832 ;lr:0.040 ;Bloss:0.307 ;acc:0.975 ;iou:0.990 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:21882 ;lr:0.040 ;Bloss:1.079 ;acc:0.835 ;iou:0.890 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:21932 ;lr:0.040 ;Bloss:1.030 ;acc:0.815 ;iou:0.880 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 0.570 ;Train accuracy: 0.912 ;IOU accuracy: 0.948 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 73\n",
      "batch: 0 ;B loss: 0.461 ;acc: 0.900 ;iou_acc: 0.945 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.622 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.900 ;acc: 0.810 ;iou_acc: 0.865 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.929 ;acc: 0.810 ;iou_acc: 0.870 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.384 ;acc: 0.685 ;iou_acc: 0.830 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.893 ;acc: 0.635 ;iou_acc: 0.745 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.149 ;Test accuracy 0.769 ;IOU accuracy: 0.843 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 74 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:21979 ;lr:0.040 ;Bloss:0.662 ;acc:0.900 ;iou:0.955 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:22029 ;lr:0.040 ;Bloss:0.196 ;acc:0.985 ;iou:0.990 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:22079 ;lr:0.040 ;Bloss:0.885 ;acc:0.860 ;iou:0.890 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:22129 ;lr:0.040 ;Bloss:0.207 ;acc:0.985 ;iou:0.990 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:22179 ;lr:0.040 ;Bloss:0.464 ;acc:0.915 ;iou:0.945 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:22229 ;lr:0.040 ;Bloss:0.365 ;acc:0.970 ;iou:0.975 ;time: 0:00:39\n",
      "\n",
      "*Training B: False ;B Train loss: 0.563 ;Train accuracy: 0.915 ;IOU accuracy: 0.949 ;Time: 0:00:45 \n",
      "\n",
      "Testing, ephoc: 74\n",
      "batch: 0 ;B loss: 0.479 ;acc: 0.915 ;iou_acc: 0.950 ;time: 0:00:46\n",
      "batch: 50 ;B loss: 0.616 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:50\n",
      "batch: 100 ;B loss: 0.942 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:00:54\n",
      "batch: 150 ;B loss: 0.920 ;acc: 0.815 ;iou_acc: 0.865 ;time: 0:01:00\n",
      "batch: 200 ;B loss: 1.358 ;acc: 0.700 ;iou_acc: 0.825 ;time: 0:01:07\n",
      "batch: 250 ;B loss: 1.771 ;acc: 0.635 ;iou_acc: 0.740 ;time: 0:01:16\n",
      "\n",
      "*BTrain: False ;Test loss: 1.139 ;Test accuracy 0.769 ;IOU accuracy: 0.845 ;Time: 0:01:26\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 75 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:22276 ;lr:0.040 ;Bloss:0.494 ;acc:0.945 ;iou:0.970 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:22326 ;lr:0.040 ;Bloss:0.576 ;acc:0.925 ;iou:0.955 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:22376 ;lr:0.040 ;Bloss:0.243 ;acc:0.980 ;iou:0.990 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:22426 ;lr:0.040 ;Bloss:0.309 ;acc:0.955 ;iou:0.985 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:22476 ;lr:0.040 ;Bloss:0.888 ;acc:0.830 ;iou:0.900 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:22526 ;lr:0.040 ;Bloss:0.957 ;acc:0.855 ;iou:0.895 ;time: 0:00:40\n",
      "\n",
      "*Training B: True ;B Train loss: 0.557 ;Train accuracy: 0.916 ;IOU accuracy: 0.950 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 75\n",
      "batch: 0 ;B loss: 0.433 ;acc: 0.915 ;iou_acc: 0.955 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.679 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.943 ;acc: 0.820 ;iou_acc: 0.870 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.975 ;acc: 0.795 ;iou_acc: 0.845 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.396 ;acc: 0.710 ;iou_acc: 0.835 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.934 ;acc: 0.630 ;iou_acc: 0.715 ;time: 0:01:18\n",
      "\n",
      "*BTrain: True ;Test loss: 1.179 ;Test accuracy 0.763 ;IOU accuracy: 0.839 ;Time: 0:01:27\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 76 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:22573 ;lr:0.040 ;Bloss:0.361 ;acc:0.955 ;iou:0.970 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:22623 ;lr:0.040 ;Bloss:0.788 ;acc:0.885 ;iou:0.915 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:22673 ;lr:0.040 ;Bloss:1.036 ;acc:0.840 ;iou:0.890 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:22723 ;lr:0.040 ;Bloss:0.566 ;acc:0.925 ;iou:0.945 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:22773 ;lr:0.040 ;Bloss:0.340 ;acc:0.965 ;iou:0.980 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:22823 ;lr:0.040 ;Bloss:0.224 ;acc:0.990 ;iou:0.990 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 0.553 ;Train accuracy: 0.916 ;IOU accuracy: 0.950 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 76\n",
      "batch: 0 ;B loss: 0.435 ;acc: 0.900 ;iou_acc: 0.940 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.648 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.915 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.963 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.416 ;acc: 0.700 ;iou_acc: 0.830 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.901 ;acc: 0.620 ;iou_acc: 0.730 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.161 ;Test accuracy 0.769 ;IOU accuracy: 0.845 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 77 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:22870 ;lr:0.040 ;Bloss:0.196 ;acc:0.990 ;iou:0.995 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:22920 ;lr:0.040 ;Bloss:1.027 ;acc:0.830 ;iou:0.890 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:22970 ;lr:0.040 ;Bloss:1.487 ;acc:0.735 ;iou:0.830 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:23020 ;lr:0.040 ;Bloss:0.819 ;acc:0.880 ;iou:0.920 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:23070 ;lr:0.040 ;Bloss:0.247 ;acc:0.975 ;iou:0.985 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:23120 ;lr:0.040 ;Bloss:0.185 ;acc:0.990 ;iou:0.995 ;time: 0:00:39\n",
      "\n",
      "*Training B: False ;B Train loss: 0.551 ;Train accuracy: 0.917 ;IOU accuracy: 0.950 ;Time: 0:00:46 \n",
      "\n",
      "Testing, ephoc: 77\n",
      "batch: 0 ;B loss: 0.497 ;acc: 0.915 ;iou_acc: 0.950 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.613 ;acc: 0.890 ;iou_acc: 0.910 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 0.899 ;acc: 0.820 ;iou_acc: 0.880 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 0.983 ;acc: 0.815 ;iou_acc: 0.865 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.421 ;acc: 0.705 ;iou_acc: 0.835 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.790 ;acc: 0.615 ;iou_acc: 0.745 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.164 ;Test accuracy 0.767 ;IOU accuracy: 0.843 ;Time: 0:01:27\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 78 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:23167 ;lr:0.040 ;Bloss:0.576 ;acc:0.935 ;iou:0.965 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:23217 ;lr:0.040 ;Bloss:0.275 ;acc:0.960 ;iou:0.985 ;time: 0:00:07\n",
      "Edit:False ;batch:100 ;gs:23267 ;lr:0.040 ;Bloss:0.224 ;acc:0.975 ;iou:0.985 ;time: 0:00:15\n",
      "Edit:False ;batch:150 ;gs:23317 ;lr:0.040 ;Bloss:0.480 ;acc:0.935 ;iou:0.955 ;time: 0:00:22\n",
      "Edit:False ;batch:200 ;gs:23367 ;lr:0.040 ;Bloss:0.209 ;acc:0.980 ;iou:0.985 ;time: 0:00:30\n",
      "Edit:False ;batch:250 ;gs:23417 ;lr:0.040 ;Bloss:0.324 ;acc:0.980 ;iou:0.990 ;time: 0:00:38\n",
      "\n",
      "*Training B: True ;B Train loss: 0.543 ;Train accuracy: 0.919 ;IOU accuracy: 0.952 ;Time: 0:00:45 \n",
      "\n",
      "Testing, ephoc: 78\n",
      "batch: 0 ;B loss: 0.468 ;acc: 0.910 ;iou_acc: 0.950 ;time: 0:00:46\n",
      "batch: 50 ;B loss: 0.609 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:50\n",
      "batch: 100 ;B loss: 0.917 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:00:54\n",
      "batch: 150 ;B loss: 0.930 ;acc: 0.825 ;iou_acc: 0.880 ;time: 0:01:01\n",
      "batch: 200 ;B loss: 1.337 ;acc: 0.725 ;iou_acc: 0.840 ;time: 0:01:08\n",
      "batch: 250 ;B loss: 1.878 ;acc: 0.620 ;iou_acc: 0.735 ;time: 0:01:16\n",
      "\n",
      "*BTrain: True ;Test loss: 1.144 ;Test accuracy 0.772 ;IOU accuracy: 0.847 ;Time: 0:01:27\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 79 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:23464 ;lr:0.040 ;Bloss:0.174 ;acc:0.985 ;iou:0.990 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:23514 ;lr:0.040 ;Bloss:1.658 ;acc:0.690 ;iou:0.790 ;time: 0:00:07\n",
      "Edit:False ;batch:100 ;gs:23564 ;lr:0.040 ;Bloss:0.472 ;acc:0.925 ;iou:0.965 ;time: 0:00:15\n",
      "Edit:False ;batch:150 ;gs:23614 ;lr:0.040 ;Bloss:1.327 ;acc:0.760 ;iou:0.820 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:23664 ;lr:0.040 ;Bloss:1.040 ;acc:0.830 ;iou:0.865 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:23714 ;lr:0.040 ;Bloss:0.231 ;acc:0.960 ;iou:0.995 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 0.533 ;Train accuracy: 0.921 ;IOU accuracy: 0.953 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 79\n",
      "batch: 0 ;B loss: 0.473 ;acc: 0.915 ;iou_acc: 0.950 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.624 ;acc: 0.890 ;iou_acc: 0.910 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.931 ;acc: 0.810 ;iou_acc: 0.870 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 0.923 ;acc: 0.810 ;iou_acc: 0.865 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.436 ;acc: 0.710 ;iou_acc: 0.830 ;time: 0:01:12\n",
      "batch: 250 ;B loss: 1.826 ;acc: 0.640 ;iou_acc: 0.740 ;time: 0:01:20\n",
      "\n",
      "*BTrain: False ;Test loss: 1.162 ;Test accuracy 0.768 ;IOU accuracy: 0.844 ;Time: 0:01:30\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 80 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:23761 ;lr:0.040 ;Bloss:0.104 ;acc:0.995 ;iou:0.995 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:23811 ;lr:0.040 ;Bloss:0.222 ;acc:0.995 ;iou:0.995 ;time: 0:00:07\n",
      "Edit:False ;batch:100 ;gs:23861 ;lr:0.040 ;Bloss:0.817 ;acc:0.875 ;iou:0.930 ;time: 0:00:14\n",
      "Edit:False ;batch:150 ;gs:23911 ;lr:0.040 ;Bloss:0.172 ;acc:0.995 ;iou:0.995 ;time: 0:00:21\n",
      "Edit:False ;batch:200 ;gs:23961 ;lr:0.040 ;Bloss:0.720 ;acc:0.895 ;iou:0.935 ;time: 0:00:29\n",
      "Edit:False ;batch:250 ;gs:24011 ;lr:0.040 ;Bloss:0.162 ;acc:0.990 ;iou:0.995 ;time: 0:00:36\n",
      "\n",
      "*Training B: False ;B Train loss: 0.533 ;Train accuracy: 0.921 ;IOU accuracy: 0.952 ;Time: 0:00:43 \n",
      "\n",
      "Testing, ephoc: 80\n",
      "batch: 0 ;B loss: 0.455 ;acc: 0.910 ;iou_acc: 0.945 ;time: 0:00:44\n",
      "batch: 50 ;B loss: 0.605 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:48\n",
      "batch: 100 ;B loss: 0.929 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:00:53\n",
      "batch: 150 ;B loss: 0.918 ;acc: 0.820 ;iou_acc: 0.870 ;time: 0:00:59\n",
      "batch: 200 ;B loss: 1.381 ;acc: 0.720 ;iou_acc: 0.830 ;time: 0:01:05\n",
      "batch: 250 ;B loss: 1.810 ;acc: 0.620 ;iou_acc: 0.720 ;time: 0:01:14\n",
      "\n",
      "*BTrain: False ;Test loss: 1.152 ;Test accuracy 0.771 ;IOU accuracy: 0.845 ;Time: 0:01:23\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 81 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:24058 ;lr:0.040 ;Bloss:0.974 ;acc:0.840 ;iou:0.865 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:24108 ;lr:0.040 ;Bloss:0.432 ;acc:0.940 ;iou:0.975 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:24158 ;lr:0.040 ;Bloss:0.102 ;acc:1.000 ;iou:1.000 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:24208 ;lr:0.040 ;Bloss:0.568 ;acc:0.905 ;iou:0.945 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:24258 ;lr:0.040 ;Bloss:0.487 ;acc:0.930 ;iou:0.975 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:24308 ;lr:0.040 ;Bloss:0.472 ;acc:0.960 ;iou:0.980 ;time: 0:00:39\n",
      "\n",
      "*Training B: True ;B Train loss: 0.526 ;Train accuracy: 0.923 ;IOU accuracy: 0.954 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 81\n",
      "batch: 0 ;B loss: 0.444 ;acc: 0.910 ;iou_acc: 0.955 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.647 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 0.890 ;acc: 0.825 ;iou_acc: 0.875 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 0.922 ;acc: 0.810 ;iou_acc: 0.865 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.376 ;acc: 0.700 ;iou_acc: 0.820 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.837 ;acc: 0.615 ;iou_acc: 0.730 ;time: 0:01:18\n",
      "\n",
      "*BTrain: True ;Test loss: 1.157 ;Test accuracy 0.770 ;IOU accuracy: 0.845 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 82 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:24355 ;lr:0.040 ;Bloss:0.929 ;acc:0.845 ;iou:0.890 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:24405 ;lr:0.040 ;Bloss:1.631 ;acc:0.675 ;iou:0.765 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:24455 ;lr:0.040 ;Bloss:0.338 ;acc:0.960 ;iou:0.975 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:24505 ;lr:0.040 ;Bloss:0.631 ;acc:0.920 ;iou:0.965 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:24555 ;lr:0.040 ;Bloss:0.180 ;acc:0.990 ;iou:1.000 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:24605 ;lr:0.040 ;Bloss:0.925 ;acc:0.845 ;iou:0.895 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 0.523 ;Train accuracy: 0.922 ;IOU accuracy: 0.953 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 82\n",
      "batch: 0 ;B loss: 0.465 ;acc: 0.905 ;iou_acc: 0.950 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.627 ;acc: 0.885 ;iou_acc: 0.910 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.930 ;acc: 0.815 ;iou_acc: 0.865 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.942 ;acc: 0.825 ;iou_acc: 0.875 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.443 ;acc: 0.695 ;iou_acc: 0.820 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.849 ;acc: 0.610 ;iou_acc: 0.720 ;time: 0:01:19\n",
      "\n",
      "*BTrain: False ;Test loss: 1.162 ;Test accuracy 0.769 ;IOU accuracy: 0.844 ;Time: 0:01:29\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 83 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:24652 ;lr:0.040 ;Bloss:0.979 ;acc:0.850 ;iou:0.875 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:24702 ;lr:0.040 ;Bloss:0.132 ;acc:1.000 ;iou:1.000 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:24752 ;lr:0.040 ;Bloss:1.037 ;acc:0.815 ;iou:0.870 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:24802 ;lr:0.040 ;Bloss:0.264 ;acc:0.980 ;iou:0.990 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:24852 ;lr:0.040 ;Bloss:0.410 ;acc:0.945 ;iou:0.955 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:24902 ;lr:0.040 ;Bloss:0.126 ;acc:0.980 ;iou:0.995 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 0.512 ;Train accuracy: 0.926 ;IOU accuracy: 0.956 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 83\n",
      "batch: 0 ;B loss: 0.472 ;acc: 0.895 ;iou_acc: 0.930 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.699 ;acc: 0.885 ;iou_acc: 0.910 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.942 ;acc: 0.835 ;iou_acc: 0.890 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 0.981 ;acc: 0.815 ;iou_acc: 0.865 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.430 ;acc: 0.710 ;iou_acc: 0.830 ;time: 0:01:12\n",
      "batch: 250 ;B loss: 1.948 ;acc: 0.620 ;iou_acc: 0.720 ;time: 0:01:21\n",
      "\n",
      "*BTrain: False ;Test loss: 1.192 ;Test accuracy 0.767 ;IOU accuracy: 0.841 ;Time: 0:01:31\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 84 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:24949 ;lr:0.040 ;Bloss:1.374 ;acc:0.690 ;iou:0.770 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:24999 ;lr:0.040 ;Bloss:0.261 ;acc:0.980 ;iou:0.990 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:25049 ;lr:0.040 ;Bloss:0.441 ;acc:0.955 ;iou:0.975 ;time: 0:00:15\n",
      "Edit:False ;batch:150 ;gs:25099 ;lr:0.040 ;Bloss:0.160 ;acc:0.995 ;iou:1.000 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:25149 ;lr:0.040 ;Bloss:0.151 ;acc:0.990 ;iou:1.000 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:25199 ;lr:0.040 ;Bloss:0.668 ;acc:0.920 ;iou:0.945 ;time: 0:00:40\n",
      "\n",
      "*Training B: True ;B Train loss: 0.515 ;Train accuracy: 0.926 ;IOU accuracy: 0.955 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 84\n",
      "batch: 0 ;B loss: 0.462 ;acc: 0.910 ;iou_acc: 0.950 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.625 ;acc: 0.895 ;iou_acc: 0.920 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.925 ;acc: 0.825 ;iou_acc: 0.880 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.945 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.418 ;acc: 0.700 ;iou_acc: 0.835 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.831 ;acc: 0.635 ;iou_acc: 0.735 ;time: 0:01:18\n",
      "\n",
      "*BTrain: True ;Test loss: 1.169 ;Test accuracy 0.771 ;IOU accuracy: 0.846 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 85 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:25246 ;lr:0.040 ;Bloss:0.992 ;acc:0.840 ;iou:0.905 ;time: 0:00:01\n",
      "Edit:False ;batch:50 ;gs:25296 ;lr:0.040 ;Bloss:0.164 ;acc:0.990 ;iou:1.000 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:25346 ;lr:0.040 ;Bloss:0.499 ;acc:0.930 ;iou:0.965 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:25396 ;lr:0.040 ;Bloss:0.540 ;acc:0.935 ;iou:0.965 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:25446 ;lr:0.040 ;Bloss:1.228 ;acc:0.795 ;iou:0.890 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:25496 ;lr:0.040 ;Bloss:0.197 ;acc:0.980 ;iou:0.985 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 0.506 ;Train accuracy: 0.927 ;IOU accuracy: 0.957 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 85\n",
      "batch: 0 ;B loss: 0.484 ;acc: 0.910 ;iou_acc: 0.940 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.632 ;acc: 0.890 ;iou_acc: 0.910 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.926 ;acc: 0.825 ;iou_acc: 0.880 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 0.940 ;acc: 0.815 ;iou_acc: 0.860 ;time: 0:01:04\n",
      "batch: 200 ;B loss: 1.358 ;acc: 0.730 ;iou_acc: 0.830 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 1.897 ;acc: 0.620 ;iou_acc: 0.735 ;time: 0:01:18\n",
      "\n",
      "*BTrain: False ;Test loss: 1.172 ;Test accuracy 0.769 ;IOU accuracy: 0.845 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 86 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:25543 ;lr:0.040 ;Bloss:0.270 ;acc:0.965 ;iou:0.980 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:25593 ;lr:0.040 ;Bloss:0.117 ;acc:0.980 ;iou:0.995 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:25643 ;lr:0.040 ;Bloss:0.175 ;acc:0.990 ;iou:0.995 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:25693 ;lr:0.040 ;Bloss:0.454 ;acc:0.930 ;iou:0.965 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:25743 ;lr:0.040 ;Bloss:1.433 ;acc:0.735 ;iou:0.845 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:25793 ;lr:0.040 ;Bloss:0.560 ;acc:0.910 ;iou:0.955 ;time: 0:00:39\n",
      "\n",
      "*Training B: False ;B Train loss: 0.504 ;Train accuracy: 0.927 ;IOU accuracy: 0.957 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 86\n",
      "batch: 0 ;B loss: 0.488 ;acc: 0.905 ;iou_acc: 0.945 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.619 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.900 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.957 ;acc: 0.810 ;iou_acc: 0.865 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.401 ;acc: 0.720 ;iou_acc: 0.835 ;time: 0:01:10\n",
      "batch: 250 ;B loss: 1.885 ;acc: 0.620 ;iou_acc: 0.730 ;time: 0:01:19\n",
      "\n",
      "*BTrain: False ;Test loss: 1.166 ;Test accuracy 0.771 ;IOU accuracy: 0.846 ;Time: 0:01:29\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 87 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:25840 ;lr:0.040 ;Bloss:0.232 ;acc:0.990 ;iou:0.995 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:25890 ;lr:0.040 ;Bloss:0.492 ;acc:0.945 ;iou:0.970 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:25940 ;lr:0.040 ;Bloss:0.783 ;acc:0.835 ;iou:0.880 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:25990 ;lr:0.040 ;Bloss:0.218 ;acc:0.965 ;iou:0.990 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:26040 ;lr:0.040 ;Bloss:0.312 ;acc:0.970 ;iou:0.990 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:26090 ;lr:0.040 ;Bloss:0.528 ;acc:0.950 ;iou:0.975 ;time: 0:00:42\n",
      "\n",
      "*Training B: True ;B Train loss: 0.497 ;Train accuracy: 0.929 ;IOU accuracy: 0.958 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 87\n",
      "batch: 0 ;B loss: 0.465 ;acc: 0.905 ;iou_acc: 0.945 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.632 ;acc: 0.890 ;iou_acc: 0.915 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.891 ;acc: 0.830 ;iou_acc: 0.880 ;time: 0:01:00\n",
      "batch: 150 ;B loss: 0.939 ;acc: 0.820 ;iou_acc: 0.865 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.422 ;acc: 0.710 ;iou_acc: 0.830 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 1.863 ;acc: 0.635 ;iou_acc: 0.740 ;time: 0:01:24\n",
      "\n",
      "*BTrain: True ;Test loss: 1.170 ;Test accuracy 0.769 ;IOU accuracy: 0.844 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 88 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:26137 ;lr:0.040 ;Bloss:0.573 ;acc:0.930 ;iou:0.955 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:26187 ;lr:0.040 ;Bloss:0.345 ;acc:0.980 ;iou:0.980 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:26237 ;lr:0.040 ;Bloss:0.261 ;acc:0.980 ;iou:0.990 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:26287 ;lr:0.040 ;Bloss:0.226 ;acc:0.985 ;iou:0.990 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:26337 ;lr:0.040 ;Bloss:0.633 ;acc:0.900 ;iou:0.925 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:26387 ;lr:0.040 ;Bloss:0.104 ;acc:1.000 ;iou:1.000 ;time: 0:00:42\n",
      "\n",
      "*Training B: False ;B Train loss: 0.486 ;Train accuracy: 0.931 ;IOU accuracy: 0.960 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 88\n",
      "batch: 0 ;B loss: 0.508 ;acc: 0.910 ;iou_acc: 0.940 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.637 ;acc: 0.900 ;iou_acc: 0.920 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.984 ;acc: 0.815 ;iou_acc: 0.870 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 0.988 ;acc: 0.795 ;iou_acc: 0.840 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.480 ;acc: 0.705 ;iou_acc: 0.830 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 1.874 ;acc: 0.620 ;iou_acc: 0.725 ;time: 0:01:23\n",
      "\n",
      "*BTrain: False ;Test loss: 1.231 ;Test accuracy 0.765 ;IOU accuracy: 0.839 ;Time: 0:01:34\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 89 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:26434 ;lr:0.040 ;Bloss:0.409 ;acc:0.950 ;iou:0.980 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:26484 ;lr:0.040 ;Bloss:1.020 ;acc:0.825 ;iou:0.885 ;time: 0:00:07\n",
      "Edit:False ;batch:100 ;gs:26534 ;lr:0.040 ;Bloss:0.723 ;acc:0.870 ;iou:0.920 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:26584 ;lr:0.040 ;Bloss:0.474 ;acc:0.930 ;iou:0.960 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:26634 ;lr:0.040 ;Bloss:0.303 ;acc:0.985 ;iou:0.985 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:26684 ;lr:0.040 ;Bloss:1.242 ;acc:0.785 ;iou:0.850 ;time: 0:00:40\n",
      "\n",
      "*Training B: False ;B Train loss: 0.483 ;Train accuracy: 0.931 ;IOU accuracy: 0.959 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 89\n",
      "batch: 0 ;B loss: 0.465 ;acc: 0.910 ;iou_acc: 0.950 ;time: 0:00:48\n",
      "batch: 50 ;B loss: 0.615 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.871 ;acc: 0.820 ;iou_acc: 0.870 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.947 ;acc: 0.810 ;iou_acc: 0.850 ;time: 0:01:04\n",
      "batch: 200 ;B loss: 1.468 ;acc: 0.680 ;iou_acc: 0.810 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 1.837 ;acc: 0.600 ;iou_acc: 0.715 ;time: 0:01:19\n",
      "\n",
      "*BTrain: False ;Test loss: 1.172 ;Test accuracy 0.770 ;IOU accuracy: 0.844 ;Time: 0:01:30\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 90 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:26731 ;lr:0.040 ;Bloss:0.094 ;acc:0.995 ;iou:1.000 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:26781 ;lr:0.040 ;Bloss:1.128 ;acc:0.815 ;iou:0.910 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:26831 ;lr:0.040 ;Bloss:0.346 ;acc:0.955 ;iou:0.985 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:26881 ;lr:0.040 ;Bloss:0.416 ;acc:0.955 ;iou:0.965 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:26931 ;lr:0.040 ;Bloss:0.388 ;acc:0.950 ;iou:0.980 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:26981 ;lr:0.040 ;Bloss:0.509 ;acc:0.945 ;iou:0.965 ;time: 0:00:41\n",
      "\n",
      "*Training B: True ;B Train loss: 0.481 ;Train accuracy: 0.933 ;IOU accuracy: 0.961 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 90\n",
      "batch: 0 ;B loss: 0.506 ;acc: 0.915 ;iou_acc: 0.950 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.654 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.920 ;acc: 0.800 ;iou_acc: 0.860 ;time: 0:00:58\n",
      "batch: 150 ;B loss: 0.982 ;acc: 0.820 ;iou_acc: 0.870 ;time: 0:01:04\n",
      "batch: 200 ;B loss: 1.408 ;acc: 0.705 ;iou_acc: 0.825 ;time: 0:01:12\n",
      "batch: 250 ;B loss: 1.843 ;acc: 0.630 ;iou_acc: 0.760 ;time: 0:01:21\n",
      "\n",
      "*BTrain: True ;Test loss: 1.179 ;Test accuracy 0.773 ;IOU accuracy: 0.847 ;Time: 0:01:31\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 91 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:27028 ;lr:0.040 ;Bloss:0.196 ;acc:0.970 ;iou:0.980 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:27078 ;lr:0.040 ;Bloss:0.194 ;acc:0.995 ;iou:0.995 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:27128 ;lr:0.040 ;Bloss:0.355 ;acc:0.955 ;iou:0.985 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:27178 ;lr:0.040 ;Bloss:0.822 ;acc:0.870 ;iou:0.925 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:27228 ;lr:0.040 ;Bloss:0.423 ;acc:0.955 ;iou:0.980 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:27278 ;lr:0.040 ;Bloss:0.107 ;acc:0.990 ;iou:0.995 ;time: 0:00:42\n",
      "\n",
      "*Training B: False ;B Train loss: 0.470 ;Train accuracy: 0.934 ;IOU accuracy: 0.961 ;Time: 0:00:49 \n",
      "\n",
      "Testing, ephoc: 91\n",
      "batch: 0 ;B loss: 0.517 ;acc: 0.915 ;iou_acc: 0.950 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.638 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:54\n",
      "batch: 100 ;B loss: 0.942 ;acc: 0.820 ;iou_acc: 0.880 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 1.007 ;acc: 0.815 ;iou_acc: 0.860 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.426 ;acc: 0.715 ;iou_acc: 0.820 ;time: 0:01:12\n",
      "batch: 250 ;B loss: 1.875 ;acc: 0.640 ;iou_acc: 0.750 ;time: 0:01:20\n",
      "\n",
      "*BTrain: False ;Test loss: 1.204 ;Test accuracy 0.768 ;IOU accuracy: 0.842 ;Time: 0:01:30\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 92 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:27325 ;lr:0.040 ;Bloss:0.203 ;acc:0.985 ;iou:0.995 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:27375 ;lr:0.040 ;Bloss:0.709 ;acc:0.885 ;iou:0.925 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:27425 ;lr:0.040 ;Bloss:0.659 ;acc:0.910 ;iou:0.960 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:27475 ;lr:0.040 ;Bloss:0.094 ;acc:1.000 ;iou:1.000 ;time: 0:00:23\n",
      "Edit:False ;batch:200 ;gs:27525 ;lr:0.040 ;Bloss:0.479 ;acc:0.960 ;iou:0.980 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:27575 ;lr:0.040 ;Bloss:0.520 ;acc:0.930 ;iou:0.950 ;time: 0:00:38\n",
      "\n",
      "*Training B: False ;B Train loss: 0.471 ;Train accuracy: 0.934 ;IOU accuracy: 0.961 ;Time: 0:00:44 \n",
      "\n",
      "Testing, ephoc: 92\n",
      "batch: 0 ;B loss: 0.450 ;acc: 0.920 ;iou_acc: 0.955 ;time: 0:00:45\n",
      "batch: 50 ;B loss: 0.672 ;acc: 0.880 ;iou_acc: 0.910 ;time: 0:00:49\n",
      "batch: 100 ;B loss: 0.912 ;acc: 0.815 ;iou_acc: 0.875 ;time: 0:00:54\n",
      "batch: 150 ;B loss: 0.989 ;acc: 0.805 ;iou_acc: 0.865 ;time: 0:01:00\n",
      "batch: 200 ;B loss: 1.472 ;acc: 0.700 ;iou_acc: 0.835 ;time: 0:01:06\n",
      "batch: 250 ;B loss: 1.907 ;acc: 0.625 ;iou_acc: 0.730 ;time: 0:01:14\n",
      "\n",
      "*BTrain: False ;Test loss: 1.206 ;Test accuracy 0.767 ;IOU accuracy: 0.842 ;Time: 0:01:24\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 93 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:27622 ;lr:0.040 ;Bloss:0.452 ;acc:0.965 ;iou:0.980 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:27672 ;lr:0.040 ;Bloss:0.625 ;acc:0.925 ;iou:0.930 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:27722 ;lr:0.040 ;Bloss:0.151 ;acc:0.990 ;iou:0.995 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:27772 ;lr:0.040 ;Bloss:0.758 ;acc:0.880 ;iou:0.925 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:27822 ;lr:0.040 ;Bloss:0.305 ;acc:0.975 ;iou:0.985 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:27872 ;lr:0.040 ;Bloss:1.132 ;acc:0.835 ;iou:0.890 ;time: 0:00:40\n",
      "\n",
      "*Training B: True ;B Train loss: 0.465 ;Train accuracy: 0.935 ;IOU accuracy: 0.961 ;Time: 0:00:46 \n",
      "\n",
      "Testing, ephoc: 93\n",
      "batch: 0 ;B loss: 0.477 ;acc: 0.915 ;iou_acc: 0.945 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.655 ;acc: 0.900 ;iou_acc: 0.920 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 0.910 ;acc: 0.830 ;iou_acc: 0.885 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 0.935 ;acc: 0.820 ;iou_acc: 0.865 ;time: 0:01:01\n",
      "batch: 200 ;B loss: 1.467 ;acc: 0.680 ;iou_acc: 0.810 ;time: 0:01:08\n",
      "batch: 250 ;B loss: 1.901 ;acc: 0.620 ;iou_acc: 0.730 ;time: 0:01:15\n",
      "\n",
      "*BTrain: True ;Test loss: 1.195 ;Test accuracy 0.770 ;IOU accuracy: 0.845 ;Time: 0:01:26\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 94 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:27919 ;lr:0.040 ;Bloss:0.390 ;acc:0.950 ;iou:0.975 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:27969 ;lr:0.040 ;Bloss:1.158 ;acc:0.810 ;iou:0.885 ;time: 0:00:07\n",
      "Edit:False ;batch:100 ;gs:28019 ;lr:0.040 ;Bloss:0.422 ;acc:0.950 ;iou:0.975 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:28069 ;lr:0.040 ;Bloss:0.799 ;acc:0.855 ;iou:0.910 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:28119 ;lr:0.040 ;Bloss:0.852 ;acc:0.855 ;iou:0.925 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:28169 ;lr:0.040 ;Bloss:0.162 ;acc:0.990 ;iou:1.000 ;time: 0:00:39\n",
      "\n",
      "*Training B: False ;B Train loss: 0.462 ;Train accuracy: 0.936 ;IOU accuracy: 0.963 ;Time: 0:00:46 \n",
      "\n",
      "Testing, ephoc: 94\n",
      "batch: 0 ;B loss: 0.487 ;acc: 0.910 ;iou_acc: 0.945 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.613 ;acc: 0.905 ;iou_acc: 0.925 ;time: 0:00:51\n",
      "batch: 100 ;B loss: 0.939 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:00:56\n",
      "batch: 150 ;B loss: 0.897 ;acc: 0.820 ;iou_acc: 0.870 ;time: 0:01:02\n",
      "batch: 200 ;B loss: 1.482 ;acc: 0.675 ;iou_acc: 0.815 ;time: 0:01:09\n",
      "batch: 250 ;B loss: 1.840 ;acc: 0.600 ;iou_acc: 0.700 ;time: 0:01:17\n",
      "\n",
      "*BTrain: False ;Test loss: 1.206 ;Test accuracy 0.769 ;IOU accuracy: 0.844 ;Time: 0:01:28\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 95 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:28216 ;lr:0.040 ;Bloss:1.227 ;acc:0.745 ;iou:0.845 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:28266 ;lr:0.040 ;Bloss:0.259 ;acc:0.975 ;iou:0.990 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:28316 ;lr:0.040 ;Bloss:0.726 ;acc:0.905 ;iou:0.935 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:28366 ;lr:0.040 ;Bloss:0.256 ;acc:0.985 ;iou:0.985 ;time: 0:00:25\n",
      "Edit:False ;batch:200 ;gs:28416 ;lr:0.040 ;Bloss:0.447 ;acc:0.940 ;iou:0.975 ;time: 0:00:32\n",
      "Edit:False ;batch:250 ;gs:28466 ;lr:0.040 ;Bloss:2.234 ;acc:0.520 ;iou:0.665 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 0.454 ;Train accuracy: 0.938 ;IOU accuracy: 0.964 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 95\n",
      "batch: 0 ;B loss: 0.488 ;acc: 0.905 ;iou_acc: 0.945 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.641 ;acc: 0.890 ;iou_acc: 0.910 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.937 ;acc: 0.830 ;iou_acc: 0.880 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 0.943 ;acc: 0.800 ;iou_acc: 0.850 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.454 ;acc: 0.715 ;iou_acc: 0.825 ;time: 0:01:13\n",
      "batch: 250 ;B loss: 1.819 ;acc: 0.630 ;iou_acc: 0.745 ;time: 0:01:21\n",
      "\n",
      "*BTrain: False ;Test loss: 1.194 ;Test accuracy 0.772 ;IOU accuracy: 0.846 ;Time: 0:01:32\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 96 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:28513 ;lr:0.040 ;Bloss:0.394 ;acc:0.945 ;iou:0.980 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:28563 ;lr:0.040 ;Bloss:0.686 ;acc:0.915 ;iou:0.945 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:28613 ;lr:0.040 ;Bloss:0.272 ;acc:0.980 ;iou:0.990 ;time: 0:00:18\n",
      "Edit:False ;batch:150 ;gs:28663 ;lr:0.040 ;Bloss:0.231 ;acc:0.985 ;iou:0.995 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:28713 ;lr:0.040 ;Bloss:0.722 ;acc:0.890 ;iou:0.945 ;time: 0:00:35\n",
      "Edit:False ;batch:250 ;gs:28763 ;lr:0.040 ;Bloss:0.186 ;acc:0.990 ;iou:0.995 ;time: 0:00:43\n",
      "\n",
      "*Training B: True ;B Train loss: 0.445 ;Train accuracy: 0.941 ;IOU accuracy: 0.966 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 96\n",
      "batch: 0 ;B loss: 0.520 ;acc: 0.910 ;iou_acc: 0.940 ;time: 0:00:51\n",
      "batch: 50 ;B loss: 0.668 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.960 ;acc: 0.825 ;iou_acc: 0.875 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 0.984 ;acc: 0.820 ;iou_acc: 0.860 ;time: 0:01:08\n",
      "batch: 200 ;B loss: 1.488 ;acc: 0.705 ;iou_acc: 0.825 ;time: 0:01:15\n",
      "batch: 250 ;B loss: 1.870 ;acc: 0.620 ;iou_acc: 0.730 ;time: 0:01:24\n",
      "\n",
      "*BTrain: True ;Test loss: 1.232 ;Test accuracy 0.767 ;IOU accuracy: 0.842 ;Time: 0:01:35\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 97 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:28810 ;lr:0.040 ;Bloss:0.175 ;acc:0.985 ;iou:0.995 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:28860 ;lr:0.040 ;Bloss:0.215 ;acc:0.975 ;iou:0.990 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:28910 ;lr:0.040 ;Bloss:1.154 ;acc:0.820 ;iou:0.895 ;time: 0:00:17\n",
      "Edit:False ;batch:150 ;gs:28960 ;lr:0.040 ;Bloss:0.113 ;acc:0.995 ;iou:1.000 ;time: 0:00:26\n",
      "Edit:False ;batch:200 ;gs:29010 ;lr:0.040 ;Bloss:0.250 ;acc:0.980 ;iou:0.995 ;time: 0:00:34\n",
      "Edit:False ;batch:250 ;gs:29060 ;lr:0.040 ;Bloss:0.222 ;acc:0.990 ;iou:0.995 ;time: 0:00:42\n",
      "\n",
      "*Training B: False ;B Train loss: 0.445 ;Train accuracy: 0.941 ;IOU accuracy: 0.966 ;Time: 0:00:50 \n",
      "\n",
      "Testing, ephoc: 97\n",
      "batch: 0 ;B loss: 0.494 ;acc: 0.900 ;iou_acc: 0.945 ;time: 0:00:50\n",
      "batch: 50 ;B loss: 0.644 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:55\n",
      "batch: 100 ;B loss: 0.971 ;acc: 0.820 ;iou_acc: 0.870 ;time: 0:01:01\n",
      "batch: 150 ;B loss: 0.970 ;acc: 0.805 ;iou_acc: 0.855 ;time: 0:01:07\n",
      "batch: 200 ;B loss: 1.470 ;acc: 0.715 ;iou_acc: 0.840 ;time: 0:01:13\n",
      "batch: 250 ;B loss: 1.885 ;acc: 0.620 ;iou_acc: 0.720 ;time: 0:01:21\n",
      "\n",
      "*BTrain: False ;Test loss: 1.218 ;Test accuracy 0.770 ;IOU accuracy: 0.844 ;Time: 0:01:32\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 98 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:29107 ;lr:0.040 ;Bloss:0.598 ;acc:0.910 ;iou:0.950 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:29157 ;lr:0.040 ;Bloss:0.383 ;acc:0.960 ;iou:0.980 ;time: 0:00:08\n",
      "Edit:False ;batch:100 ;gs:29207 ;lr:0.040 ;Bloss:0.636 ;acc:0.895 ;iou:0.935 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:29257 ;lr:0.040 ;Bloss:0.956 ;acc:0.855 ;iou:0.900 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:29307 ;lr:0.040 ;Bloss:1.322 ;acc:0.785 ;iou:0.865 ;time: 0:00:33\n",
      "Edit:False ;batch:250 ;gs:29357 ;lr:0.040 ;Bloss:0.212 ;acc:0.985 ;iou:0.990 ;time: 0:00:41\n",
      "\n",
      "*Training B: False ;B Train loss: 0.441 ;Train accuracy: 0.941 ;IOU accuracy: 0.965 ;Time: 0:00:48 \n",
      "\n",
      "Testing, ephoc: 98\n",
      "batch: 0 ;B loss: 0.520 ;acc: 0.905 ;iou_acc: 0.945 ;time: 0:00:49\n",
      "batch: 50 ;B loss: 0.647 ;acc: 0.905 ;iou_acc: 0.920 ;time: 0:00:53\n",
      "batch: 100 ;B loss: 0.908 ;acc: 0.825 ;iou_acc: 0.875 ;time: 0:00:59\n",
      "batch: 150 ;B loss: 0.968 ;acc: 0.800 ;iou_acc: 0.850 ;time: 0:01:05\n",
      "batch: 200 ;B loss: 1.475 ;acc: 0.690 ;iou_acc: 0.820 ;time: 0:01:13\n",
      "batch: 250 ;B loss: 1.905 ;acc: 0.635 ;iou_acc: 0.740 ;time: 0:01:22\n",
      "\n",
      "*BTrain: False ;Test loss: 1.219 ;Test accuracy 0.770 ;IOU accuracy: 0.845 ;Time: 0:01:32\n",
      "================================================== \n",
      "\n",
      "================================================== \n",
      "Train, ephoc: 99 ; Training B: True\n",
      "Edit:False ;batch:0 ;gs:29404 ;lr:0.040 ;Bloss:0.372 ;acc:0.960 ;iou:0.980 ;time: 0:00:00\n",
      "Edit:False ;batch:50 ;gs:29454 ;lr:0.040 ;Bloss:0.115 ;acc:0.995 ;iou:1.000 ;time: 0:00:09\n",
      "Edit:False ;batch:100 ;gs:29504 ;lr:0.040 ;Bloss:1.267 ;acc:0.745 ;iou:0.835 ;time: 0:00:16\n",
      "Edit:False ;batch:150 ;gs:29554 ;lr:0.040 ;Bloss:0.271 ;acc:0.975 ;iou:0.985 ;time: 0:00:24\n",
      "Edit:False ;batch:200 ;gs:29604 ;lr:0.040 ;Bloss:0.141 ;acc:0.995 ;iou:1.000 ;time: 0:00:31\n",
      "Edit:False ;batch:250 ;gs:29654 ;lr:0.040 ;Bloss:0.226 ;acc:0.985 ;iou:0.990 ;time: 0:00:39\n",
      "\n",
      "*Training B: True ;B Train loss: 0.439 ;Train accuracy: 0.941 ;IOU accuracy: 0.965 ;Time: 0:00:47 \n",
      "\n",
      "Testing, ephoc: 99\n",
      "batch: 0 ;B loss: 0.475 ;acc: 0.910 ;iou_acc: 0.945 ;time: 0:00:47\n",
      "batch: 50 ;B loss: 0.720 ;acc: 0.895 ;iou_acc: 0.915 ;time: 0:00:52\n",
      "batch: 100 ;B loss: 0.976 ;acc: 0.820 ;iou_acc: 0.875 ;time: 0:00:57\n",
      "batch: 150 ;B loss: 0.991 ;acc: 0.820 ;iou_acc: 0.870 ;time: 0:01:03\n",
      "batch: 200 ;B loss: 1.465 ;acc: 0.700 ;iou_acc: 0.840 ;time: 0:01:11\n",
      "batch: 250 ;B loss: 1.947 ;acc: 0.630 ;iou_acc: 0.745 ;time: 0:01:19\n",
      "\n",
      "*BTrain: True ;Test loss: 1.234 ;Test accuracy 0.767 ;IOU accuracy: 0.843 ;Time: 0:01:30\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_hidden=200\n",
    "edit_rewards_tst, edit_rewards_trn = [], []\n",
    "params_dir = params_dir_tmp+'RL/EXP/hidden:'+str(num_hidden)\n",
    "tf.reset_default_graph()\n",
    "m = Model(\n",
    "    batch_size=200, \n",
    "    num_hidden=num_hidden,\n",
    "    embed_size=embed_vecs.shape[1],\n",
    "    img_dims=trainset[0][0][1][0].shape[1], \n",
    "    bbox_dims=trainset[0][0][1][1].shape[1], \n",
    "    lr=.05,\n",
    "    vocab=vocab, \n",
    "    decay_steps=10000, \n",
    "    decay_rate=0.9, \n",
    "    coefAlr=1,\n",
    "    bnorm=False,\n",
    "    toQscale=True\n",
    ")\n",
    "\n",
    "\n",
    "print('edit_reward:', edit_reward)\n",
    "print('params_dir:', params_dir)\n",
    "print('num_hidden:', m.num_hidden)\n",
    "print('learning rate:', m.lr)\n",
    "tst, trn = m.train(trainset, testset,\n",
    "        ephocs_num=100,\n",
    "        start_ephoc=0,\n",
    "        startA=101,\n",
    "        activation_ephoc=101,\n",
    "        muteB=2, \n",
    "        editProb=0.8,\n",
    "        edit_reward=edit_reward,\n",
    "        imScale=50,\n",
    "        rnn_editProb=0.2,\n",
    "        onlyB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
