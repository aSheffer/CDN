{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset_file = '../data/training/w2v_train_data.bin'\n",
    "testset_file = '../data/training/w2v_test_data.bin'\n",
    "vocab_file =  '../data/metadata/vocab.json'\n",
    "# params_dir = '../data/training/models/GroundR/params:hs,50:lr,0.05'\n",
    "params_dir = '../data/training/models/RGAB5/w2v'\n",
    "embed_path =  '../data/metadata/w2v.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset length: 59449\n",
      "test length: 59507\n"
     ]
    }
   ],
   "source": [
    "trainset = np.load(open(trainset_file, 'rb'))\n",
    "trainset = [item for item in trainset if len(item)>2 and len(item[0])>0]\n",
    "print('trainset length:', len(trainset))\n",
    "\n",
    "testset = np.load(open(testset_file, 'rb'))\n",
    "testset = [item for item in testset if len(item)>2 and len(item[0])>0]\n",
    "print('test length:', len(testset))\n",
    "\n",
    "with open(vocab_file, 'r') as f:\n",
    "    vocab = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def accuracy(data, batch_size, model):\n",
    "    np.random.shuffle(data)\n",
    "    np.random.shuffle(data)\n",
    "    nbatch = len(data)//batch_size\n",
    "    print('Number of batches:', nbatch, '\\n')\n",
    "    acc = 0\n",
    "    stime, btime = datetime.now(), datetime.now()\n",
    "    for b in range(nbatch):\n",
    "        b_acc=model.accuracy(data, b*batch_size, (b+1)*batch_size)\n",
    "        acc+=b_acc/nbatch\n",
    "        if b%100==0:\n",
    "            print('-batch number:', b, ' '*(len(str(nbatch))-len(str(b))), '-- accuracy:', b_acc, '-- time interval:', datetime.now()-btime, '-')\n",
    "            btime = datetime.now()\n",
    "    print('\\nAccuracy:', acc, '\\nTime i+nterval:', datetime.now()-stime)\n",
    "    return acc\n",
    "\n",
    "def iou_accuracy(data, batch_size, model, threshold=0.5):\n",
    "    np.random.shuffle(data)\n",
    "    np.random.shuffle(data)\n",
    "    nbatch = len(data)//batch_size\n",
    "    print('Number of batches:', nbatch, '\\n')\n",
    "    acc = 0\n",
    "    stime, btime = datetime.now(), datetime.now()\n",
    "    for b in range(nbatch):\n",
    "        b_acc=model.iou_accuracy(data, b*batch_size, (b+1)*batch_size)\n",
    "        acc+=b_acc/nbatch\n",
    "        if b%100==0:\n",
    "            print('-batch number:', b,  ' '*(len(str(nbatch))-len(str(b))), '-- IOU accuracy:', b_acc, '-- time interval:', datetime.now()-btime, '-')\n",
    "            btime = datetime.now()\n",
    "    print('\\nIOU accuracy:', acc, '\\nTime interval:', datetime.now()-stime)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_vecs = np.load(open(embed_path, 'rb')).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self,\n",
    "                 batch_size, \n",
    "                 embed_size,\n",
    "                 \n",
    "                 #Image's vector size.\n",
    "                 img_dims, \n",
    "                 \n",
    "                 #Spaital features length.\n",
    "                 bbox_dims, \n",
    "                 vocab, \n",
    "                 lr, #  learning rate.\n",
    "                 decay_steps, \n",
    "                 decay_rate,\n",
    "                 embed_vecs=embed_vecs):\n",
    "        \n",
    "        with tf.device(':cpu:0'):\n",
    "            self.batch_size = batch_size\n",
    "            self.img_dims = img_dims\n",
    "            self.bbox_dims = bbox_dims \n",
    "            self.embed_size = embed_size\n",
    "            self.vocab = vocab\n",
    "            self.lr=lr\n",
    "\n",
    "            self.queries = tf.placeholder(tf.int32, [None, None], name='queries_holder')\n",
    "            self.img  = tf.placeholder(tf.float32, [None, None, self.img_dims], name='img_holder')# VGG output vectors\n",
    "            self.bboxes = tf.placeholder(tf.float32, [None, None, self.bbox_dims], name='bboxes_holder')# spatial bbox's features.\n",
    "\n",
    "            # attn_idx: inicates whether attention box is a dummy (0) or not (1).\n",
    "            self.attn_idx = tf.placeholder(tf.float32, [None, None], name='attn_idx')\n",
    "\n",
    "            self.labels = tf.placeholder(tf.float32, [None, None], name='labels_holder')\n",
    "\n",
    "            with tf.variable_scope('embed_scope'):\n",
    "                embed = tf.get_variable(name='embed', initializer=embed_vecs, dtype=tf.float32)\n",
    "                embed_queries = tf.nn.embedding_lookup(embed, self.queries, name='embed_queries')\n",
    "                \n",
    "            # Each query is represent as the normed average of its word vectors\n",
    "            # shape: batch_size x 1 x embed_size\n",
    "            avgQ = tf.nn.l2_normalize(tf.reduce_sum(embed_queries, axis=1, keep_dims=True), dim=-2)\n",
    "            \n",
    "            # Concatinate images vectors and their spaital features\n",
    "            img_vecs = tf.concat([self.img, self.bboxes], 2)\n",
    "             \n",
    "            # Trandorm the images vectors to have embed_size\n",
    "            # and normelize them.\n",
    "            img_newVec = tf.nn.l2_normalize(\n",
    "                tf.reshape(\n",
    "                    self.linear(tf.reshape(img_vecs, [-1, img_dims+bbox_dims]), self.embed_size), \n",
    "                    shape=[self.batch_size, -1, int(self.embed_size)]),\n",
    "                dim=-1)\n",
    "            \n",
    "            # Calculate cosine distance between\n",
    "            # each query and all of its bboxes\n",
    "            dist = tf.reduce_sum(avgQ*img_newVec, axis = -1)\n",
    "            \n",
    "            # Calculate the distances masked softmax (we use self.attn_idx to mas\n",
    "            max_logits = tf.reduce_max(dist, axis=-1)\n",
    "            masked_logits = tf.exp(dist-tf.expand_dims(max_logits, axis=1))*self.attn_idx\n",
    "            self.scores = self.attn_idx*masked_logits/tf.reduce_sum(masked_logits, axis=-1, keep_dims=True)\n",
    "            \n",
    "            \n",
    "            # Cross entophy loss.\n",
    "            self.loss = tf.reduce_mean(\n",
    "                -tf.reduce_sum(\n",
    "                    self.labels*tf.log(self.scores+0.00000001)+\n",
    "                        (1-self.labels)*tf.log((1-self.scores)+0.00000001), \n",
    "                    axis=-1)\n",
    "            )\n",
    "\n",
    "\n",
    "            ##############\n",
    "            # Optimizers #\n",
    "            ##############\n",
    "\n",
    "            starter_learning_rate = self.lr\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.learning_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step,\n",
    "                                                           decay_steps=decay_steps, decay_rate=decay_rate, staircase=True)\n",
    "\n",
    "        \n",
    "            self.optimizer =  tf.train.GradientDescentOptimizer(\n",
    "                        learning_rate=self.learning_rate).minimize(self.loss, global_step=self.global_step)  \n",
    "\n",
    "            if not os.path.exists(params_dir):\n",
    "                    os.makedirs(params_dir)\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "        \n",
    "    def length(self, seq):\n",
    "        '''\n",
    "        Retruns real lengths (before addings) of all queries in seq  .\n",
    "        '''\n",
    "        return tf.cast(tf.reduce_sum(tf.sign(tf.abs(seq)), reduction_indices=1), tf.int32)\n",
    "       \n",
    "\n",
    "    def linear(self, inputs, output_dim, scope='linear', bias=True, reuse=False):\n",
    "\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            W = tf.get_variable('W', initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                shape=(inputs.get_shape()[-1], output_dim))\n",
    "            if bias:\n",
    "                b = tf.get_variable('b', initializer=tf.constant_initializer(0.1),\n",
    "                               shape=[1, output_dim])\n",
    "                return tf.matmul(inputs, W) + b\n",
    "            \n",
    "            return tf.matmul(inputs, W)\n",
    "\n",
    "        \n",
    "    def q_padding(self, seq, max_length):\n",
    "        '''\n",
    "        Pad  seq with vocab['<pad>'] (0) to max_length length.\n",
    "        '''                  \n",
    "        return seq + [self.vocab['<pad>']]*(max_length-len(seq))\n",
    "\n",
    "    \n",
    "    def build_data(self, data, start, end):\n",
    "        '''\n",
    "        Build batch.\n",
    "        ------------\n",
    "        \n",
    "        Params:\n",
    "            data: each entry in this list has the following structure:\n",
    "                  [query indexes, [bounding box vector, bounding box spaital features], ..., [bounding box vector, bounding box spaital features], index of the true label]\n",
    "            start/end: batch data is built from data[start:end]\n",
    "            \n",
    "        Returns:\n",
    "            attn_idx: attn_idx[i, j]=1 ifthe j'th bbox in the i'th query is not padding, else equals to 0. \n",
    "            \n",
    "            \n",
    "            padded_queries: list of queries, padded to the length of the longest query in the batch.\n",
    "                            Note: vocab['pad']=0\n",
    "                            \n",
    "            padded_im: list of bounding boxes vectors, padded to the maximum number of bbox per query.\n",
    "                            Note: padded vector is vector of zeros. \n",
    "                            \n",
    "            padded_bbox: list of bounding boxes spatial features, padded to the maximum number of bbox per query.\n",
    "                            Note: padded vector is vector of zeros.  \n",
    "        \n",
    "            onehot_labels: onehot_labels[i][j]=1 if j is the true bbox for query i, else  onehot_labels[i][j]=0\n",
    "            \n",
    "            addNoise: Boolean. Whether to add normal noise to the images.\n",
    "                        \n",
    "        '''\n",
    "                          \n",
    "        qlen = max([len(data[i][0]) for i in range(start, end)]) # Length fo the longest query\n",
    "        imlen = max([len(data[i]) for i in range(start, end)])-2 # Maximum number of bbox per query.\n",
    "        padded_queries, padded_im, padded_bbox, attn_idx = [], [], [], []\n",
    "        \n",
    "        # Build one hot labels from the labels index, given in the data.                  \n",
    "        labels = [item[-1] for item in data[start:end]] #data[i][-1]=index of the true bbox of query i\n",
    "        onehot_labels = np.zeros((end-start, imlen))\n",
    "        onehot_labels[np.arange(end-start), labels]=1\n",
    "                          \n",
    "        im_dim, bbox_dim = data[0][1][0].shape[1], data[0][1][1].shape[1]\n",
    "        for i in range(start, end):\n",
    "            padded_queries.append(self.q_padding(data[i][0], qlen))\n",
    "            \n",
    "            attn_idx.append([1 for _ in range(len(data[i])-2)]+[0 for _ in range(imlen-(len(data[i])-2))])\n",
    "            \n",
    "            padded_im.append(np.concatenate([data[i][j][0] for j in range(1, len(data[i])-1)] + \n",
    "                                       [np.full((imlen-(len(data[i])-2), im_dim), vocab['<pad>'], dtype=np.float32)], axis=0))\n",
    "            \n",
    "            padded_bbox.append(np.concatenate([data[i][j][1] for j in range(1, len(data[i])-1)] + \n",
    "                                       [np.full((imlen-(len(data[i])-2),bbox_dim), vocab['<pad>'], dtype=np.float32)], axis=0))\n",
    "           \n",
    "            \n",
    "        return np.array(attn_idx), np.array(padded_queries, dtype=np.int32), np.array(padded_im), np.array(padded_bbox), np.array(onehot_labels)\n",
    "            \n",
    "   \n",
    "    def ground(self, data=None, start=None, end=None, sess=None, feed_dict = None, isEdit=True):\n",
    "        '''\n",
    "        Given a query and a list of bboxes, the function returns the index of the referred bbox.\n",
    "        '''\n",
    "        isSess = (sess==None)\n",
    "        if isSess:\n",
    "            sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            if isSess:\n",
    "                tf.global_variables_initializer().run()\n",
    "                ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "                else:\n",
    "                    print('Initializing variables')\n",
    "            if feed_dict is None:\n",
    "                attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(data, start, end)\n",
    "                feed_dict = {\n",
    "                        self.queries:padded_queries,\n",
    "                        self.img:padded_im,\n",
    "                        self.bboxes:padded_bbox,\n",
    "                        self.labels:labels,\n",
    "                        self.attn_idx:attn_idx\n",
    "                    }\n",
    "            scores = sess.run(self.scores, feed_dict=feed_dict) # get score for each bbox\n",
    "\n",
    "        return np.argmax(scores, axis=1), np.argmax(feed_dict[self.labels], axis=1)\n",
    "        \n",
    "        \n",
    "    def iou_accuracy(self, data, start, end, sess=None, feed_dict = None, threshold=0.5, test=False, isEdit=True):\n",
    "        '''\n",
    "        Calculate the IOU score between the Model bbox and the true bbox.\n",
    "        ''' \n",
    "                          \n",
    "        # Get score for each bbox (labels) and th true bbox index (gt_idx)                  \n",
    "        if feed_dict is None:\n",
    "            labels, gt_idx = self.ground(data, start, end, sess=sess, feed_dict=feed_dict, isEdit=isEdit)\n",
    "        else: labels, gt_idx = self.ground(sess=sess, feed_dict=feed_dict, isEdit=isEdit)\n",
    "        acc = 0\n",
    "        \n",
    "        for i in range(start, end):\n",
    "            gt = data[i][gt_idx[i-start]+1][1][0] # ground truth bbox\n",
    "            crops = np.expand_dims(data[i][labels[i-start]+1][1][0], axis=0) #Model chosen bbox\n",
    "            acc += (retriever.compute_iou(crops, gt)[0]>threshold) #IOU for the i sample.\n",
    "            \n",
    "        return acc/(end-start)\n",
    "        \n",
    "    def accuracy(self, data=None, start=None, end=None, sess=None, feed_dict = None, isEdit=True):\n",
    "        isSess = (sess==None)\n",
    "        if isSess:\n",
    "            print('Building sess')\n",
    "            sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            if isSess:\n",
    "                print('Building sess used')\n",
    "                tf.global_variables_initializer().run()\n",
    "                ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "                if ckpt and ckpt.model_checkpoint_path:\n",
    "                    print('3')\n",
    "                    self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "                else:\n",
    "                    print('Initializing variables')\n",
    "            if feed_dict is None:\n",
    "                print('Building feed_dict')\n",
    "                attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(data, start, end)\n",
    "                feed_dict = {\n",
    "                        self.queries:padded_queries,\n",
    "                        self.img:padded_im,\n",
    "                        self.bboxes:padded_bbox,\n",
    "                        self.attn_idx:attn_idx,\n",
    "                        self.labels:labels,\n",
    "                    }\n",
    "            scores = sess.run(self.scores, feed_dict=feed_dict)\n",
    "            acc = sum(np.equal(np.argmax(scores, axis=1), np.argmax(feed_dict[self.labels], axis=1))/len(feed_dict[self.labels]))\n",
    "\n",
    "                    \n",
    "        return acc\n",
    "    \n",
    "        \n",
    "    def train(self, trn_data, tst_data, ephocs_num):\n",
    "                          \n",
    "        '''\n",
    "        Params:\n",
    "             trn_data: list, train set. \n",
    "             \n",
    "             tst_data: list, test set. \n",
    "             \n",
    "             ephocs_num: number of ephocs\n",
    "             \n",
    "             start_ephoc: number of first ephoc.\n",
    "             \n",
    "             edit_reward: int, coefficient to multiply the reward by when editing a word.\n",
    "             \n",
    "             startA: int, Start competition only at ephoc # startA.\n",
    "             \n",
    "             activation_ephoc: at ephoc numer \"activation_ephoc\", A will be activate.\n",
    "                               That is, for (activation_ephoc-startA) number of ephocs, \n",
    "                               A will chooce an action randomly.\n",
    "             \n",
    "            muteB: After A starts, for each ephoc which A & B trains, \n",
    "                   only A will be trained for this amount of ephocs.\n",
    "                   \n",
    "            editProb: robabilty for editing a query.\n",
    "            \n",
    "            activateAProb: when running A, we can choos an action randomly or taknig A decision. \n",
    "                            This is the starting probabilty for NOT choocing an action ranomdly.\n",
    "            \n",
    "            max_activateAProb: Final probabilty for NOT choocing an action ranomdly.\n",
    "            \n",
    "            dropout_in: dropout ratio of B's rnn inputs.\n",
    "            \n",
    "            dropout_output: dropout ratio of B's rnn output.\n",
    "            \n",
    "            dropout_img: dropout ratio of images vectors before the last attention layer .\n",
    "            \n",
    "            addNoise: Boolean. Whether to add normal noise to the images (see build_data).\n",
    "                               \n",
    "        '''                  \n",
    "        \n",
    "        trn_nbatch = len(trn_data)//self.batch_size\n",
    "        tst_nbatch = len(tst_data)//self.batch_size\n",
    "        print('# Train set size:', len(trn_data))\n",
    "        print('# Training batches:', trn_nbatch)\n",
    "        print('# Test set size:', len(tst_data))\n",
    "        print('# Testing batches:', tst_nbatch)\n",
    "        self.test_res, self.train_res = [], [] #list to hold accuracy of test set\n",
    "        sess = tf.Session()\n",
    "        with sess.as_default():\n",
    "            tf.global_variables_initializer().run()\n",
    "            ckpt = tf.train.get_checkpoint_state(params_dir)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                print('Loading parameters from', ckpt.model_checkpoint_path)\n",
    "                self.saver.restore(sess, ckpt.model_checkpoint_path) # restore all variables\n",
    "            else:\n",
    "                print('Initializing variables')\n",
    "                \n",
    "            for ephoc in range(ephocs_num):\n",
    "                startTime = datetime.now().replace(microsecond=0)\n",
    "                    \n",
    "                print('='*50,'\\nTrain, ephoc:',ephoc)\n",
    "                np.random.shuffle(trn_data)\n",
    "                trn_loss, trn_acc, trn_iou = 0, 0, 0\n",
    "              \n",
    "                for b in range(trn_nbatch):\n",
    "                    attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(trn_data, \n",
    "                                                                                        b*self.batch_size, (b+1)*self.batch_size)\n",
    "\n",
    "                    feed_dict = {\n",
    "                        self.queries:padded_queries,\n",
    "                        self.img:padded_im,\n",
    "                        self.bboxes:padded_bbox,\n",
    "                        self.attn_idx:attn_idx,\n",
    "                        self.labels: labels\n",
    "                    }\n",
    "\n",
    "                  \n",
    "                    loss, lr, gs, _ = sess.run([self.loss, self.learning_rate, self.global_step, self.optimizer], feed_dict=feed_dict)\n",
    "\n",
    "                    acc = self.accuracy(sess=sess, feed_dict=feed_dict)  \n",
    "                    iou_acc = self.iou_accuracy(\n",
    "                        trn_data, b*self.batch_size, (b+1)*self.batch_size, \n",
    "                        sess=sess, feed_dict=feed_dict)\n",
    "                    \n",
    "                    \n",
    "                    trn_loss += loss/trn_nbatch\n",
    "                    trn_acc += acc/trn_nbatch\n",
    "                    trn_iou += iou_acc/trn_nbatch\n",
    "\n",
    "                    if b%100==0:\n",
    "                        print('Ephoc:',ephoc, ';batch:', b, \n",
    "                              ';gs:', gs, ';lr: %.4f'%(lr), ';loss: %.2f'%(loss), \n",
    "                              ';acc: %.3f'%(acc), ';iou: %.3f'%(iou_acc), ';time:', datetime.now().replace(microsecond=0)-startTime)    \n",
    "                  \n",
    "                print('\\n*Train loss: %.3f'%(trn_loss),                                                                                            \n",
    "                          ';Train accuracy: %.3f'%(trn_acc),  ';IOU accuracy: %.3f'%(trn_iou), \n",
    "                          ';Time:', datetime.now().replace(microsecond=0)-startTime, '\\n')\n",
    "                self.train_res.append([trn_acc, trn_iou, trn_loss])\n",
    "                    \n",
    "                self.saver.save(sess, params_dir + \"/model.ckpt\", global_step=ephoc)    \n",
    "                \n",
    "                \n",
    "                print('Testing, ephoc:',ephoc)\n",
    "                tstTime = datetime.now().replace(microsecond=0)\n",
    "                tst_loss, tst_acc, tst_iou = 0, 0, 0\n",
    "                for b in range(tst_nbatch):\n",
    "                    attn_idx, padded_queries, padded_im, padded_bbox, labels = self.build_data(tst_data,\n",
    "                                                                                b*self.batch_size, (b+1)*self.batch_size)\n",
    "                    \n",
    "                    feed_dict = {\n",
    "                        self.queries:padded_queries,\n",
    "                        self.img:padded_im,\n",
    "                        self.bboxes:padded_bbox,\n",
    "                        self.attn_idx:attn_idx,\n",
    "                        self.labels: labels\n",
    "                    }\n",
    "                    \n",
    "                    loss = sess.run(self.loss, feed_dict=feed_dict)\n",
    "\n",
    "                    acc = self.accuracy(sess=sess, feed_dict=feed_dict)\n",
    "                    iou_acc = self.iou_accuracy(\n",
    "                        tst_data, b*self.batch_size, (b+1)*self.batch_size, sess=sess, feed_dict=feed_dict)\n",
    "                    \n",
    "                    tst_acc += acc/tst_nbatch\n",
    "                    tst_loss += loss/tst_nbatch\n",
    "                    tst_iou += iou_acc/tst_nbatch\n",
    "                    if b%100==0:\n",
    "                        print('Batch:', b, ';loss: %.3f'%(loss), ';acc: %.3f'%(acc), \n",
    "                               ';iou_acc: %.3f'%(iou_acc), ';time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "                        \n",
    "                print('\\n*Test loss: %.3f'%(tst_loss), ';Test accuracy %.3f'%(tst_acc), \n",
    "                      ';IOU accuracy: %.3f'%(tst_iou), ';Time:', datetime.now().replace(microsecond=0)-startTime)\n",
    "                self.test_res.append([tst_acc, tst_iou, tst_loss])\n",
    "                \n",
    "            print('='*50,'\\n')\n",
    "        return self.test_res, self.train_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "m = Model(\n",
    "    batch_size=100, \n",
    "    embed_size=embed_vecs.shape[1],\n",
    "    img_dims=trainset[0][1][0].shape[1], \n",
    "    bbox_dims=trainset[0][1][1].shape[1], \n",
    "    lr=.05,\n",
    "    vocab=vocab, \n",
    "    decay_steps=10000, \n",
    "    decay_rate=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params_dir: ../data/training/models/RGAB5/w2v\n",
      "Embed size: 100\n",
      "learning rate: 0.05\n",
      "# Train set size: 59449\n",
      "# Training batches: 594\n",
      "# Test set size: 59507\n",
      "# Testing batches: 595\n",
      "Initializing variables\n",
      "================================================== \n",
      "Train, ephoc: 0\n",
      "Ephoc: 0 ;batch: 0 ;gs: 1 ;lr: 0.0500 ;loss: 2.87 ;acc: 0.520 ;iou: 0.640 ;time: 0:00:00\n",
      "Ephoc: 0 ;batch: 100 ;gs: 101 ;lr: 0.0500 ;loss: 1.91 ;acc: 0.710 ;iou: 0.770 ;time: 0:00:13\n",
      "Ephoc: 0 ;batch: 200 ;gs: 201 ;lr: 0.0500 ;loss: 2.05 ;acc: 0.630 ;iou: 0.690 ;time: 0:00:26\n",
      "Ephoc: 0 ;batch: 300 ;gs: 301 ;lr: 0.0500 ;loss: 1.89 ;acc: 0.660 ;iou: 0.710 ;time: 0:00:40\n",
      "Ephoc: 0 ;batch: 400 ;gs: 401 ;lr: 0.0500 ;loss: 1.95 ;acc: 0.620 ;iou: 0.690 ;time: 0:00:53\n",
      "Ephoc: 0 ;batch: 500 ;gs: 501 ;lr: 0.0500 ;loss: 2.05 ;acc: 0.550 ;iou: 0.650 ;time: 0:01:07\n",
      "\n",
      "*Train loss: 2.029 ;Train accuracy: 0.628 ;IOU accuracy: 0.707 ;Time: 0:01:20 \n",
      "\n",
      "Testing, ephoc: 0\n",
      "Batch: 0 ;loss: 1.891 ;acc: 0.570 ;iou_acc: 0.670 ;time: 0:01:20\n",
      "Batch: 100 ;loss: 1.713 ;acc: 0.590 ;iou_acc: 0.670 ;time: 0:01:30\n",
      "Batch: 200 ;loss: 1.942 ;acc: 0.570 ;iou_acc: 0.640 ;time: 0:01:41\n",
      "Batch: 300 ;loss: 1.937 ;acc: 0.540 ;iou_acc: 0.650 ;time: 0:01:51\n",
      "Batch: 400 ;loss: 2.191 ;acc: 0.470 ;iou_acc: 0.560 ;time: 0:02:01\n",
      "Batch: 500 ;loss: 2.094 ;acc: 0.450 ;iou_acc: 0.610 ;time: 0:02:12\n",
      "\n",
      "*Test loss: 1.946 ;Test accuracy 0.523 ;IOU accuracy: 0.621 ;Time: 0:02:22\n",
      "================================================== \n",
      "Train, ephoc: 1\n",
      "Ephoc: 1 ;batch: 0 ;gs: 595 ;lr: 0.0500 ;loss: 1.92 ;acc: 0.620 ;iou: 0.700 ;time: 0:00:00\n",
      "Ephoc: 1 ;batch: 100 ;gs: 695 ;lr: 0.0500 ;loss: 1.99 ;acc: 0.580 ;iou: 0.710 ;time: 0:00:13\n",
      "Ephoc: 1 ;batch: 200 ;gs: 795 ;lr: 0.0500 ;loss: 1.86 ;acc: 0.660 ;iou: 0.700 ;time: 0:00:26\n",
      "Ephoc: 1 ;batch: 300 ;gs: 895 ;lr: 0.0500 ;loss: 1.79 ;acc: 0.730 ;iou: 0.800 ;time: 0:01:04\n",
      "Ephoc: 1 ;batch: 400 ;gs: 995 ;lr: 0.0500 ;loss: 1.69 ;acc: 0.730 ;iou: 0.770 ;time: 0:01:40\n",
      "Ephoc: 1 ;batch: 500 ;gs: 1095 ;lr: 0.0500 ;loss: 1.91 ;acc: 0.590 ;iou: 0.680 ;time: 0:02:14\n",
      "\n",
      "*Train loss: 1.868 ;Train accuracy: 0.672 ;IOU accuracy: 0.741 ;Time: 0:02:44 \n",
      "\n",
      "Testing, ephoc: 1\n",
      "Batch: 0 ;loss: 1.858 ;acc: 0.530 ;iou_acc: 0.620 ;time: 0:02:44\n",
      "Batch: 100 ;loss: 1.793 ;acc: 0.600 ;iou_acc: 0.660 ;time: 0:03:14\n",
      "Batch: 200 ;loss: 1.911 ;acc: 0.570 ;iou_acc: 0.670 ;time: 0:03:48\n",
      "Batch: 300 ;loss: 1.950 ;acc: 0.520 ;iou_acc: 0.600 ;time: 0:04:21\n",
      "Batch: 400 ;loss: 2.185 ;acc: 0.500 ;iou_acc: 0.610 ;time: 0:04:43\n",
      "Batch: 500 ;loss: 2.071 ;acc: 0.450 ;iou_acc: 0.570 ;time: 0:05:21\n",
      "\n",
      "*Test loss: 1.944 ;Test accuracy 0.525 ;IOU accuracy: 0.621 ;Time: 0:05:57\n",
      "================================================== \n",
      "Train, ephoc: 2\n",
      "Ephoc: 2 ;batch: 0 ;gs: 1189 ;lr: 0.0500 ;loss: 1.72 ;acc: 0.750 ;iou: 0.810 ;time: 0:00:00\n",
      "Ephoc: 2 ;batch: 100 ;gs: 1289 ;lr: 0.0500 ;loss: 1.72 ;acc: 0.750 ;iou: 0.790 ;time: 0:00:42\n",
      "Ephoc: 2 ;batch: 200 ;gs: 1389 ;lr: 0.0500 ;loss: 1.88 ;acc: 0.710 ;iou: 0.830 ;time: 0:01:31\n",
      "Ephoc: 2 ;batch: 300 ;gs: 1489 ;lr: 0.0500 ;loss: 1.87 ;acc: 0.690 ;iou: 0.740 ;time: 0:02:09\n",
      "Ephoc: 2 ;batch: 400 ;gs: 1589 ;lr: 0.0500 ;loss: 2.03 ;acc: 0.640 ;iou: 0.730 ;time: 0:02:43\n",
      "Ephoc: 2 ;batch: 500 ;gs: 1689 ;lr: 0.0500 ;loss: 1.90 ;acc: 0.650 ;iou: 0.690 ;time: 0:03:18\n",
      "\n",
      "*Train loss: 1.828 ;Train accuracy: 0.690 ;IOU accuracy: 0.756 ;Time: 0:04:00 \n",
      "\n",
      "Testing, ephoc: 2\n",
      "Batch: 0 ;loss: 1.911 ;acc: 0.490 ;iou_acc: 0.620 ;time: 0:04:00\n",
      "Batch: 100 ;loss: 1.752 ;acc: 0.600 ;iou_acc: 0.690 ;time: 0:04:42\n",
      "Batch: 200 ;loss: 2.012 ;acc: 0.510 ;iou_acc: 0.580 ;time: 0:05:21\n",
      "Batch: 300 ;loss: 1.926 ;acc: 0.550 ;iou_acc: 0.640 ;time: 0:06:01\n",
      "Batch: 400 ;loss: 2.220 ;acc: 0.430 ;iou_acc: 0.520 ;time: 0:06:35\n",
      "Batch: 500 ;loss: 2.099 ;acc: 0.450 ;iou_acc: 0.560 ;time: 0:07:05\n",
      "\n",
      "*Test loss: 1.981 ;Test accuracy 0.513 ;IOU accuracy: 0.606 ;Time: 0:07:43\n",
      "================================================== \n",
      "Train, ephoc: 3\n",
      "Ephoc: 3 ;batch: 0 ;gs: 1783 ;lr: 0.0500 ;loss: 1.93 ;acc: 0.720 ;iou: 0.750 ;time: 0:00:01\n",
      "Ephoc: 3 ;batch: 100 ;gs: 1883 ;lr: 0.0500 ;loss: 1.68 ;acc: 0.700 ;iou: 0.750 ;time: 0:00:38\n",
      "Ephoc: 3 ;batch: 200 ;gs: 1983 ;lr: 0.0500 ;loss: 1.81 ;acc: 0.710 ;iou: 0.760 ;time: 0:01:15\n",
      "Ephoc: 3 ;batch: 300 ;gs: 2083 ;lr: 0.0500 ;loss: 1.97 ;acc: 0.710 ;iou: 0.760 ;time: 0:01:51\n",
      "Ephoc: 3 ;batch: 400 ;gs: 2183 ;lr: 0.0500 ;loss: 1.79 ;acc: 0.700 ;iou: 0.760 ;time: 0:02:08\n",
      "Ephoc: 3 ;batch: 500 ;gs: 2283 ;lr: 0.0500 ;loss: 1.90 ;acc: 0.660 ;iou: 0.680 ;time: 0:02:23\n",
      "\n",
      "*Train loss: 1.838 ;Train accuracy: 0.692 ;IOU accuracy: 0.755 ;Time: 0:02:36 \n",
      "\n",
      "Testing, ephoc: 3\n",
      "Batch: 0 ;loss: 1.816 ;acc: 0.600 ;iou_acc: 0.670 ;time: 0:02:36\n",
      "Batch: 100 ;loss: 1.844 ;acc: 0.580 ;iou_acc: 0.640 ;time: 0:02:47\n",
      "Batch: 200 ;loss: 2.035 ;acc: 0.490 ;iou_acc: 0.530 ;time: 0:02:58\n",
      "Batch: 300 ;loss: 2.012 ;acc: 0.470 ;iou_acc: 0.560 ;time: 0:03:10\n",
      "Batch: 400 ;loss: 2.142 ;acc: 0.490 ;iou_acc: 0.560 ;time: 0:03:20\n",
      "Batch: 500 ;loss: 2.100 ;acc: 0.450 ;iou_acc: 0.520 ;time: 0:03:31\n",
      "\n",
      "*Test loss: 2.010 ;Test accuracy 0.504 ;IOU accuracy: 0.599 ;Time: 0:03:42\n",
      "================================================== \n",
      "Train, ephoc: 4\n",
      "Ephoc: 4 ;batch: 0 ;gs: 2377 ;lr: 0.0500 ;loss: 1.76 ;acc: 0.730 ;iou: 0.820 ;time: 0:00:00\n",
      "Ephoc: 4 ;batch: 100 ;gs: 2477 ;lr: 0.0500 ;loss: 1.79 ;acc: 0.700 ;iou: 0.780 ;time: 0:00:14\n",
      "Ephoc: 4 ;batch: 200 ;gs: 2577 ;lr: 0.0500 ;loss: 1.93 ;acc: 0.720 ;iou: 0.790 ;time: 0:00:28\n",
      "Ephoc: 4 ;batch: 300 ;gs: 2677 ;lr: 0.0500 ;loss: 1.91 ;acc: 0.670 ;iou: 0.730 ;time: 0:00:43\n",
      "Ephoc: 4 ;batch: 400 ;gs: 2777 ;lr: 0.0500 ;loss: 1.78 ;acc: 0.740 ;iou: 0.770 ;time: 0:00:57\n",
      "Ephoc: 4 ;batch: 500 ;gs: 2877 ;lr: 0.0500 ;loss: 1.83 ;acc: 0.650 ;iou: 0.720 ;time: 0:01:11\n",
      "\n",
      "*Train loss: 1.843 ;Train accuracy: 0.690 ;IOU accuracy: 0.754 ;Time: 0:01:24 \n",
      "\n",
      "Testing, ephoc: 4\n",
      "Batch: 0 ;loss: 1.868 ;acc: 0.580 ;iou_acc: 0.670 ;time: 0:01:24\n",
      "Batch: 100 ;loss: 1.779 ;acc: 0.610 ;iou_acc: 0.680 ;time: 0:01:35\n",
      "Batch: 200 ;loss: 1.924 ;acc: 0.530 ;iou_acc: 0.620 ;time: 0:02:05\n",
      "Batch: 300 ;loss: 1.961 ;acc: 0.480 ;iou_acc: 0.580 ;time: 0:02:38\n",
      "Batch: 400 ;loss: 2.188 ;acc: 0.480 ;iou_acc: 0.570 ;time: 0:03:05\n",
      "Batch: 500 ;loss: 2.062 ;acc: 0.470 ;iou_acc: 0.580 ;time: 0:03:42\n",
      "\n",
      "*Test loss: 1.984 ;Test accuracy 0.512 ;IOU accuracy: 0.606 ;Time: 0:04:14\n",
      "================================================== \n",
      "Train, ephoc: 5\n",
      "Ephoc: 5 ;batch: 0 ;gs: 2971 ;lr: 0.0500 ;loss: 1.73 ;acc: 0.690 ;iou: 0.780 ;time: 0:00:00\n",
      "Ephoc: 5 ;batch: 100 ;gs: 3071 ;lr: 0.0500 ;loss: 1.72 ;acc: 0.700 ;iou: 0.760 ;time: 0:00:38\n",
      "Ephoc: 5 ;batch: 200 ;gs: 3171 ;lr: 0.0500 ;loss: 1.78 ;acc: 0.700 ;iou: 0.720 ;time: 0:01:03\n",
      "Ephoc: 5 ;batch: 300 ;gs: 3271 ;lr: 0.0500 ;loss: 1.90 ;acc: 0.650 ;iou: 0.730 ;time: 0:01:37\n",
      "Ephoc: 5 ;batch: 400 ;gs: 3371 ;lr: 0.0500 ;loss: 1.78 ;acc: 0.710 ;iou: 0.760 ;time: 0:02:15\n",
      "Ephoc: 5 ;batch: 500 ;gs: 3471 ;lr: 0.0500 ;loss: 1.70 ;acc: 0.710 ;iou: 0.760 ;time: 0:02:50\n",
      "\n",
      "*Train loss: 1.824 ;Train accuracy: 0.701 ;IOU accuracy: 0.762 ;Time: 0:03:26 \n",
      "\n",
      "Testing, ephoc: 5\n",
      "Batch: 0 ;loss: 1.881 ;acc: 0.540 ;iou_acc: 0.620 ;time: 0:03:27\n",
      "Batch: 100 ;loss: 1.819 ;acc: 0.630 ;iou_acc: 0.680 ;time: 0:04:05\n",
      "Batch: 200 ;loss: 1.999 ;acc: 0.490 ;iou_acc: 0.580 ;time: 0:04:41\n",
      "Batch: 300 ;loss: 1.995 ;acc: 0.480 ;iou_acc: 0.560 ;time: 0:05:07\n",
      "Batch: 400 ;loss: 2.187 ;acc: 0.510 ;iou_acc: 0.600 ;time: 0:05:17\n",
      "Batch: 500 ;loss: 2.079 ;acc: 0.480 ;iou_acc: 0.590 ;time: 0:05:27\n",
      "\n",
      "*Test loss: 2.005 ;Test accuracy 0.505 ;IOU accuracy: 0.603 ;Time: 0:05:37\n",
      "================================================== \n",
      "Train, ephoc: 6\n",
      "Ephoc: 6 ;batch: 0 ;gs: 3565 ;lr: 0.0500 ;loss: 1.98 ;acc: 0.660 ;iou: 0.730 ;time: 0:00:00\n",
      "Ephoc: 6 ;batch: 100 ;gs: 3665 ;lr: 0.0500 ;loss: 2.06 ;acc: 0.640 ;iou: 0.710 ;time: 0:00:14\n",
      "Ephoc: 6 ;batch: 200 ;gs: 3765 ;lr: 0.0500 ;loss: 1.86 ;acc: 0.720 ;iou: 0.800 ;time: 0:00:27\n",
      "Ephoc: 6 ;batch: 300 ;gs: 3865 ;lr: 0.0500 ;loss: 1.67 ;acc: 0.770 ;iou: 0.840 ;time: 0:00:41\n",
      "Ephoc: 6 ;batch: 400 ;gs: 3965 ;lr: 0.0500 ;loss: 1.94 ;acc: 0.670 ;iou: 0.700 ;time: 0:00:54\n",
      "Ephoc: 6 ;batch: 500 ;gs: 4065 ;lr: 0.0500 ;loss: 1.76 ;acc: 0.740 ;iou: 0.770 ;time: 0:01:08\n",
      "\n",
      "*Train loss: 1.831 ;Train accuracy: 0.704 ;IOU accuracy: 0.765 ;Time: 0:01:20 \n",
      "\n",
      "Testing, ephoc: 6\n",
      "Batch: 0 ;loss: 1.913 ;acc: 0.540 ;iou_acc: 0.620 ;time: 0:01:20\n",
      "Batch: 100 ;loss: 1.832 ;acc: 0.600 ;iou_acc: 0.700 ;time: 0:01:31\n",
      "Batch: 200 ;loss: 1.972 ;acc: 0.610 ;iou_acc: 0.680 ;time: 0:01:42\n",
      "Batch: 300 ;loss: 1.925 ;acc: 0.520 ;iou_acc: 0.610 ;time: 0:01:53\n",
      "Batch: 400 ;loss: 2.221 ;acc: 0.440 ;iou_acc: 0.520 ;time: 0:02:03\n",
      "Batch: 500 ;loss: 2.090 ;acc: 0.460 ;iou_acc: 0.580 ;time: 0:02:14\n",
      "\n",
      "*Test loss: 2.024 ;Test accuracy 0.502 ;IOU accuracy: 0.598 ;Time: 0:02:23\n",
      "================================================== \n",
      "Train, ephoc: 7\n",
      "Ephoc: 7 ;batch: 0 ;gs: 4159 ;lr: 0.0500 ;loss: 1.66 ;acc: 0.780 ;iou: 0.830 ;time: 0:00:01\n",
      "Ephoc: 7 ;batch: 100 ;gs: 4259 ;lr: 0.0500 ;loss: 1.87 ;acc: 0.690 ;iou: 0.730 ;time: 0:00:13\n",
      "Ephoc: 7 ;batch: 200 ;gs: 4359 ;lr: 0.0500 ;loss: 1.74 ;acc: 0.720 ;iou: 0.770 ;time: 0:00:31\n",
      "Ephoc: 7 ;batch: 300 ;gs: 4459 ;lr: 0.0500 ;loss: 1.83 ;acc: 0.720 ;iou: 0.770 ;time: 0:00:47\n",
      "Ephoc: 7 ;batch: 400 ;gs: 4559 ;lr: 0.0500 ;loss: 1.91 ;acc: 0.740 ;iou: 0.800 ;time: 0:01:06\n",
      "Ephoc: 7 ;batch: 500 ;gs: 4659 ;lr: 0.0500 ;loss: 1.92 ;acc: 0.620 ;iou: 0.730 ;time: 0:01:24\n",
      "\n",
      "*Train loss: 1.864 ;Train accuracy: 0.695 ;IOU accuracy: 0.756 ;Time: 0:01:42 \n",
      "\n",
      "Testing, ephoc: 7\n",
      "Batch: 0 ;loss: 1.991 ;acc: 0.530 ;iou_acc: 0.620 ;time: 0:01:42\n",
      "Batch: 100 ;loss: 1.877 ;acc: 0.620 ;iou_acc: 0.660 ;time: 0:01:57\n",
      "Batch: 200 ;loss: 1.936 ;acc: 0.520 ;iou_acc: 0.600 ;time: 0:02:12\n",
      "Batch: 300 ;loss: 1.954 ;acc: 0.480 ;iou_acc: 0.560 ;time: 0:02:27\n",
      "Batch: 400 ;loss: 2.211 ;acc: 0.490 ;iou_acc: 0.610 ;time: 0:02:41\n",
      "Batch: 500 ;loss: 2.134 ;acc: 0.490 ;iou_acc: 0.630 ;time: 0:02:57\n",
      "\n",
      "*Test loss: 2.053 ;Test accuracy 0.491 ;IOU accuracy: 0.587 ;Time: 0:03:12\n",
      "================================================== \n",
      "Train, ephoc: 8\n",
      "Ephoc: 8 ;batch: 0 ;gs: 4753 ;lr: 0.0500 ;loss: 1.77 ;acc: 0.690 ;iou: 0.750 ;time: 0:00:00\n",
      "Ephoc: 8 ;batch: 100 ;gs: 4853 ;lr: 0.0500 ;loss: 1.84 ;acc: 0.700 ;iou: 0.760 ;time: 0:00:18\n",
      "Ephoc: 8 ;batch: 200 ;gs: 4953 ;lr: 0.0500 ;loss: 1.84 ;acc: 0.720 ;iou: 0.770 ;time: 0:00:36\n",
      "Ephoc: 8 ;batch: 300 ;gs: 5053 ;lr: 0.0500 ;loss: 1.85 ;acc: 0.690 ;iou: 0.760 ;time: 0:00:54\n",
      "Ephoc: 8 ;batch: 400 ;gs: 5153 ;lr: 0.0500 ;loss: 1.64 ;acc: 0.730 ;iou: 0.790 ;time: 0:01:12\n",
      "Ephoc: 8 ;batch: 500 ;gs: 5253 ;lr: 0.0500 ;loss: 1.78 ;acc: 0.740 ;iou: 0.790 ;time: 0:01:29\n",
      "\n",
      "*Train loss: 1.872 ;Train accuracy: 0.694 ;IOU accuracy: 0.755 ;Time: 0:01:45 \n",
      "\n",
      "Testing, ephoc: 8\n",
      "Batch: 0 ;loss: 1.968 ;acc: 0.560 ;iou_acc: 0.630 ;time: 0:01:46\n",
      "Batch: 100 ;loss: 1.895 ;acc: 0.570 ;iou_acc: 0.650 ;time: 0:02:01\n",
      "Batch: 200 ;loss: 1.984 ;acc: 0.530 ;iou_acc: 0.580 ;time: 0:02:16\n",
      "Batch: 300 ;loss: 1.993 ;acc: 0.500 ;iou_acc: 0.610 ;time: 0:02:32\n",
      "Batch: 400 ;loss: 2.237 ;acc: 0.470 ;iou_acc: 0.530 ;time: 0:02:45\n",
      "Batch: 500 ;loss: 2.113 ;acc: 0.460 ;iou_acc: 0.530 ;time: 0:03:01\n",
      "\n",
      "*Test loss: 2.060 ;Test accuracy 0.488 ;IOU accuracy: 0.585 ;Time: 0:03:15\n",
      "================================================== \n",
      "Train, ephoc: 9\n",
      "Ephoc: 9 ;batch: 0 ;gs: 5347 ;lr: 0.0500 ;loss: 1.90 ;acc: 0.670 ;iou: 0.750 ;time: 0:00:00\n",
      "Ephoc: 9 ;batch: 100 ;gs: 5447 ;lr: 0.0500 ;loss: 2.06 ;acc: 0.670 ;iou: 0.740 ;time: 0:00:16\n",
      "Ephoc: 9 ;batch: 200 ;gs: 5547 ;lr: 0.0500 ;loss: 1.96 ;acc: 0.710 ;iou: 0.800 ;time: 0:00:34\n",
      "Ephoc: 9 ;batch: 300 ;gs: 5647 ;lr: 0.0500 ;loss: 1.92 ;acc: 0.720 ;iou: 0.780 ;time: 0:00:52\n",
      "Ephoc: 9 ;batch: 400 ;gs: 5747 ;lr: 0.0500 ;loss: 1.87 ;acc: 0.710 ;iou: 0.750 ;time: 0:01:10\n",
      "Ephoc: 9 ;batch: 500 ;gs: 5847 ;lr: 0.0500 ;loss: 2.01 ;acc: 0.640 ;iou: 0.700 ;time: 0:01:28\n",
      "\n",
      "*Train loss: 1.884 ;Train accuracy: 0.689 ;IOU accuracy: 0.752 ;Time: 0:01:43 \n",
      "\n",
      "Testing, ephoc: 9\n",
      "Batch: 0 ;loss: 2.055 ;acc: 0.540 ;iou_acc: 0.620 ;time: 0:01:43\n",
      "Batch: 100 ;loss: 1.965 ;acc: 0.540 ;iou_acc: 0.600 ;time: 0:01:58\n",
      "Batch: 200 ;loss: 2.038 ;acc: 0.500 ;iou_acc: 0.530 ;time: 0:02:12\n",
      "Batch: 300 ;loss: 2.042 ;acc: 0.500 ;iou_acc: 0.590 ;time: 0:02:28\n",
      "Batch: 400 ;loss: 2.239 ;acc: 0.450 ;iou_acc: 0.530 ;time: 0:02:42\n",
      "Batch: 500 ;loss: 2.177 ;acc: 0.390 ;iou_acc: 0.500 ;time: 0:02:58\n",
      "\n",
      "*Test loss: 2.084 ;Test accuracy 0.482 ;IOU accuracy: 0.578 ;Time: 0:03:12\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_res, train_res = [], []\n",
    "\n",
    "print('params_dir:', params_dir)\n",
    "print('Embed size:', m.embed_size)\n",
    "print('learning rate:', m.lr)\n",
    "tst, trn = m.train(trainset, testset,\n",
    "        ephocs_num=10)  \n",
    "\n",
    "test_res.append(tst)\n",
    "train_res.append(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
